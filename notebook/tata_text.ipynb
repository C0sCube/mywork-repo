{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import fitz\n",
    "import camelot\n",
    "import warnings , math, collections , os, re\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\Kaustubh.keny\\OneDrive - Cogencis Information Services Ltd\\Documents\\mywork-repo\"\n",
    "#path = r\"C:\\Users\\rando\\OneDrive\\Documents\\mywork-repo\"\n",
    "\n",
    "tata_path = path + r\"\\files\\TataFactSheet2024.pdf\"\n",
    "\n",
    "#dry run paths\n",
    "dry_run_path = path + r\"\\output\\DryRun.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TATA PATHS\n",
    "#tata output path\n",
    "no_image_path = path +r\"\\output\\tata\\TatanoImgPdf.pdf\"\n",
    "textual_pdf_path = path + r\"\\output\\tata\\TatatextalPdf.pdf\"\n",
    "tabular_pdf_path = path + r\"\\output\\tata\\TatatabularPdf.pdf\"\n",
    "\n",
    "#pickkle data paths tata\n",
    "pickle_text = r\"\\output\\pkl\\tata\\textual_data.pkl\"\n",
    "pickle_tab = r\"\\output\\pkl\\tata\\tabular_data.pkl\"\n",
    "pickle_nonimg = r\"\\output\\pkl\\tata\\nonimg_data.pkl\"\n",
    "pickle_all = r'\\output\\pkl\\tata\\all_data.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + pickle_all , 'rb') as file:\n",
    "    document_data = pickle.load(file)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_data['total_pages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 Tata Large Cap Fund\n",
      "17 Tata Flexi Cap Fund\n",
      "18 Tata Large & Mid Cap Fund\n",
      "19 Tata Equity P/E Fund\n",
      "20 Tata Mid Cap Growth Fund\n",
      "21 Tata ELSS Tax Saver Fund\n",
      "22 Tata Small Cap Fund\n",
      "23 Tata Focused Equity Fund\n",
      "24 Tata Nifty 50 Index Fund\n",
      "25 Tata Nifty Auto Index Fund\n",
      "26 Tata Nifty India Tourism Index Fund\n",
      "27 Tata Nifty Financial Services Index Fund\n",
      "28 Tata Nifty MidSmall Healthcare Index Fund\n",
      "29 Tata Nifty Realty Index Fund\n",
      "30 TATA Nifty500 Multicap India Manufacturing 50:30:20 Index Fund\n",
      "31 Tata Nifty500 Multicap Infrastructure 50:30:20 Index Fund\n",
      "32 Tata BSE Sensex Index Fund\n",
      "33 Tata Nifty 50 Exchange Traded Fund\n",
      "34 Tata Nifty Private Bank Exchange Traded Fund\n",
      "35 Tata Dividend Yield Fund\n",
      "36 Tata Nifty India Digital ETF Fund of Fund\n",
      "37 Tata Nifty India Digital Exchange Traded Fund\n",
      "38 Tata Silver Exchange Traded Fund\n",
      "39 Tata Silver ETF Fund of Fund\n",
      "40 Tata Gold Exchange Traded Fund\n",
      "41 Tata Gold ETF Fund of Fund\n",
      "42 Tata Nifty Midcap 150 Momentum 50 Index Fund\n",
      "43 Tata Nifty200 Alpha 30 Index Fund\n",
      "44 Tata Nifty Capital Markets Index Fund\n",
      "45 Tata Multicap Fund\n",
      "46 Tata Business Cycle Fund\n",
      "47 Tata Ethical Fund\n",
      "48 Tata Banking & Financial Services Fund\n",
      "49 Tata Digital India Fund\n",
      "50 Tata India Consumer Fund\n",
      "51 Tata India Pharma & Healthcare Fund\n",
      "52 Tata Resources & Energy Fund\n",
      "53 Tata Infrastructure Fund\n",
      "54 Tata Quant Fund\n",
      "55 Tata Housing Opportunities Fund\n",
      "56 Tata India Innovation Fund\n",
      "59 Tata Balanced Advantage Fund\n",
      "60 Tata Balanced Advantage Fund\n",
      "61 Tata Arbitrage Fund\n",
      "62 Tata Arbitrage Fund\n",
      "63 Tata Multi Asset Opportunities Fund\n",
      "64 Tata Multi Asset Opportunities Fund\n",
      "65 Tata Short Term Bond Fund\n",
      "66 Tata Treasury Advantage Fund\n",
      "67 Tata Money Market Fund\n",
      "68 Tata Ultra Short Term Fund\n",
      "69 Tata Corporate Bond Fund\n",
      "70 Tata Floating Rate Fund\n",
      "71 Tata Nifty SDL Plus AAA PSU Bond Dec 2027 60:40 Index Fund\n",
      "72 Tata CRISIL-IBX Gilt Index  April 2026 Index Fund\n",
      "73 Tata Nifty G-Sec Dec 2026 Index Fund\n",
      "74 Tata Nifty G-Sec Dec 2029 Index Fund\n",
      "75 Tata Liquid Fund\n",
      "76 Tata Overnight Fund\n",
      "77 Tata Gilt Securities Fund\n",
      "81 Tata Young Citizensâ€™ Fund\n"
     ]
    }
   ],
   "source": [
    "pdf = fitz.open(tata_path)\n",
    "\n",
    "for pgn in range(pdf.page_count):\n",
    "    page = pdf[pgn]\n",
    "    \n",
    "    text_data = page.get_text('dict')['blocks']\n",
    "    sorted_text_data = sorted(text_data, key=lambda x :(x['bbox'][1],x['bbox'][0]))\n",
    "    \n",
    "    first_block = sorted_text_data[0]\n",
    "    if \"lines\" in first_block:\n",
    "        for line in first_block['lines']:\n",
    "            for span in line['spans']:\n",
    "                if re.match(r'^(tata|samco|icici).*fund$', span['text'], re.IGNORECASE):\n",
    "                    print(pgn, span['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "title_data = dict()\n",
    "for pgn,page in enumerate(document_data['blocks_data']):\n",
    "    sortedPage = sorted(page['blocks'], key= lambda k: (round(k['bbox'][1]), round(k['bbox'][0]))) #sort t to b, l to r\n",
    "    firstBlock = sortedPage[0] #get first block of sorted data\n",
    "    \n",
    "    if 'lines' in firstBlock:\n",
    "        for line in firstBlock['lines']:\n",
    "            for span in line['spans']:\n",
    "                text = span['text'].lower()\n",
    "                \n",
    "                #regex condition\n",
    "                cond1 = re.findall('^samco|^tata|fund$', text)\n",
    "                if cond1:\n",
    "                    \n",
    "                    title_data[pgn] = text\n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = list(title_data.keys())\n",
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox = (15,55,170,615)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_clipped(input:str, pageSelect:list, bbox:set):   \n",
    "    document = fitz.open(input)\n",
    "    finalData = []\n",
    "    \n",
    "    for pgn, title in pageSelect.items():\n",
    "        page = document[pgn]\n",
    "        \n",
    "        blocks = page.get_text('dict', clip = bbox)['blocks']\n",
    "        sorted_blocks = sorted(blocks, key= lambda x: (x['bbox'][1], x['bbox'][0]))\n",
    "        \n",
    "        finalData.append(\n",
    "            {\n",
    "                'page':pgn,\n",
    "                'title': title,\n",
    "                \"block\": sorted_blocks\n",
    "            }\n",
    "        )          \n",
    "    return finalData\n",
    "\n",
    "def remove_keys(data:list, keys_to_remove:set):\n",
    "    if isinstance(data, list):\n",
    "        # Process each element in the list\n",
    "        return [remove_keys(item, keys_to_remove) for item in data]\n",
    "    elif isinstance(data, dict):\n",
    "        # Process each key-value pair in the dictionary\n",
    "        return {key: remove_keys(value, keys_to_remove) for key, value in data.items() if key not in keys_to_remove}\n",
    "    else:\n",
    "        # Return data as is if it's neither a dict nor a list\n",
    "        return data\n",
    "    \n",
    "def clean_text(input_str):\n",
    "    # Step 1: Remove extra spaces between words\n",
    "    input_str = re.sub(r'\\s+', ' ', input_str.strip())\n",
    "    # Step 2: Fix split words\n",
    "    #input_str = re.sub(r'(\\b[A-Z]+)\\s+([A-Z]+\\b)', r'\\1\\2', input_str)\n",
    "    # Step 3: Remove trailing special characters like '/', '**', '^'\n",
    "    input_str = re.sub(r'[/*^]+$', '', input_str)\n",
    "    return input_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data_clipped(tata_path, title_data, bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]['block']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_headers = set()\n",
    "sizes = set()\n",
    "remove_text = [':', \"  \"]\n",
    "patterns_text = [r'^MULTI|(MENT|TORS)$']\n",
    "for page in data:\n",
    "    for blocks in page['block']:\n",
    "        if 'lines' in blocks:\n",
    "            \n",
    "            for line in blocks['lines']:\n",
    "                \n",
    "                line['spans'] = [\n",
    "                    span for span in line['spans']\n",
    "                    if not any(re.match(pattern, span['text'].strip()) for pattern in patterns_text)\n",
    "                ]\n",
    "                \n",
    "                # Filter out spans with text that is whitespace or empty\n",
    "                line['spans'] = [\n",
    "                    span for span in line['spans']\n",
    "                    if span['text'].strip()  # Keep spans with non-whitespace text\n",
    "                ]\n",
    "            \n",
    "\n",
    "                for span in line['spans']:\n",
    "                    text = clean_text(span['text'].strip())\n",
    "                    \n",
    "                    #imp conditions for text\n",
    "                    conditions = [span['size'] == 6 ,\n",
    "                                span['font'] == 'Swiss721BT-BoldCondensed',\n",
    "                                span['color']== -15570765]\n",
    "                    if all(conditions):\n",
    "        \n",
    "                        sub_headers.add(text)\n",
    "                        span['text'] = text\n",
    "                        span['size'] = 30.0\n",
    "                    \n",
    "                    sizes.add(span['size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cleaned_data in data:\n",
    "    for block in cleaned_data.get(\"block\", []):\n",
    "        for line in block.get(\"lines\", []):\n",
    "            combined_spans = []\n",
    "            for span in line.get(\"spans\", []):\n",
    "                if combined_spans and all(\n",
    "                    combined_spans[-1].get(key) == span.get(key)\n",
    "                    for key in [\"flags\", \"size\", \"color\"]\n",
    "                ):\n",
    "                    # Combine text if spans are similar\n",
    "                    combined_spans[-1][\"text\"] += \" \" + span[\"text\"]\n",
    "                else:\n",
    "                    combined_spans.append(span)\n",
    "            line[\"spans\"] = combined_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]['block']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_to_remove = {} #{'ascender', 'descender', 'bbox','wmode','dir','number'}\n",
    "cleaned_data = remove_keys(data, keys_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[1]['block']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_matrix_structure(data: list, subheader_font: float, content_font: float):\n",
    "    # extract unique coordinates\n",
    "    coordinates = []\n",
    "    fonts = set()\n",
    "\n",
    "    for block in data['block']:\n",
    "        for line in block['lines']:\n",
    "            for span in line['spans']:\n",
    "                origin = tuple(span['origin'])  # Extract (x, y) coordinates from origin\n",
    "                coordinates.append(origin)\n",
    "                fonts.add(span['size'])\n",
    "\n",
    "    coordinates = sorted(set(coordinates), key=lambda c: (c[1], c[0]))  # Sort by y, then x\n",
    "    fonts = sorted(fonts)\n",
    "\n",
    "    # Step 2: Initialize the matrix\n",
    "    coord_to_index = {coord: idx for idx, coord in enumerate(coordinates)}\n",
    "    font_to_index = {font: idx for idx, font in enumerate(fonts)}\n",
    "    matrix = np.zeros((len(coordinates), len(fonts)), dtype=object)\n",
    "\n",
    "    # Step 3: Fill the matrix\n",
    "    for block in data['block']:\n",
    "        for line in block['lines']:\n",
    "            for span in line['spans']:\n",
    "                origin = tuple(span['origin'])  # Get (x, y) coordinates from origin\n",
    "                font = span['size']\n",
    "                if origin in coord_to_index and font in font_to_index:\n",
    "                    row = coord_to_index[origin]\n",
    "                    col = font_to_index[font]\n",
    "                    if matrix[row, col] == 0:\n",
    "                        matrix[row, col] = []\n",
    "                    matrix[row, col].append(span)  # Append the entire span dictionary\n",
    "\n",
    "    # Step 4: Generate the nested dictionary\n",
    "    nested_dict = {}\n",
    "    current_subheader = None\n",
    "\n",
    "    for row_idx, coord in enumerate(coordinates):\n",
    "        for col_idx, font in enumerate(fonts):\n",
    "            if matrix[row_idx, col_idx] != 0:\n",
    "                spans = matrix[row_idx, col_idx]\n",
    "\n",
    "                for span in spans:\n",
    "                    if font == subheader_font:\n",
    "                        current_subheader = span\n",
    "                        nested_dict[current_subheader['text']] = []\n",
    "                    elif font <= content_font and current_subheader:\n",
    "                        nested_dict[current_subheader['text']].append(span)\n",
    "\n",
    "    return nested_dict\n",
    "def generate_pdf_from_data(data:list, output_path:str):\n",
    "    \"\"\"\n",
    "    Generates a PDF from the nested dictionary data structure.\n",
    "\n",
    "    Parameters:\n",
    "        data (dict): The nested dictionary containing sections and fitz spans.\n",
    "        output_path (str): The file path where the PDF will be saved.\n",
    "    \"\"\"\n",
    "    # Create a new PDF document\n",
    "    pdf_document = fitz.open()\n",
    "    \n",
    "    for section, spans in data.items():\n",
    "        # Add a new page for each section\n",
    "        page = pdf_document.new_page()\n",
    "        text_position = 72  # Initial vertical position (used only for section titles)\n",
    "\n",
    "        # Add section title\n",
    "        title_font_size = 14\n",
    "        page.insert_text(\n",
    "            (72, text_position),\n",
    "            section,\n",
    "            fontsize=title_font_size,\n",
    "            fontname=\"helv\",\n",
    "            color=(0, 0, 1),\n",
    "        )\n",
    "\n",
    "        # Iterate through each span in the section\n",
    "        for span in spans:\n",
    "            bbox = span.get(\"bbox\", [0, 0, 0, 0])  # Use bbox for exact placement\n",
    "\n",
    "            # Error handling for font issues\n",
    "            try:\n",
    "                page.insert_text(\n",
    "                    (bbox[0], bbox[1]),  # Use bbox coordinates for exact placement\n",
    "                    span[\"text\"],\n",
    "                    fontsize=span[\"size\"],\n",
    "                    fontname=\"helv\",\n",
    "                    color=tuple(int(span[\"color\"] & 0xFFFFFF) for _ in range(3)),  # Convert span color\n",
    "                )\n",
    "            except Exception:\n",
    "                page.insert_text(\n",
    "                    (bbox[0], bbox[1]),  # Use bbox coordinates for exact placement\n",
    "                    span[\"text\"],\n",
    "                    fontsize=span[\"size\"],\n",
    "                    fontname=\"helv\",\n",
    "                    color=(1, 0, 0),  # Fallback color for errors\n",
    "                )\n",
    "\n",
    "    # Save the created PDF\n",
    "    pdf_document.save(output_path)\n",
    "    pdf_document.close()\n",
    "    print(f\"PDF successfully generated and saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_data = create_matrix_structure(data[1],30,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF successfully generated and saved to: C:\\Users\\Kaustubh.keny\\OneDrive - Cogencis Information Services Ltd\\Documents\\mywork-repo\\output\\samcoDryRun.pdf\n"
     ]
    }
   ],
   "source": [
    "generate_pdf_from_data(matrix_data, path + r\"\\output\\samcoDryRun.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pdfplumber.open(path +r\"\\output\\samcoDryRun.pdf\") as pdf:\n",
    "    final_data = []\n",
    "    final_data_generated = {}\n",
    "    \n",
    "    for page in pdf.pages:\n",
    "        # extract text from the page\n",
    "        text = page.extract_text()\n",
    "        final_data.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['INVESTMENT STYLE\\nPrimarily focuses on investing in equity and equity related\\ninstruments of well researched companies across market\\ncapitalization.',\n",
       " 'INVESTMENT OBJECTIVE\\nThe investment objective of the Scheme is to generate capital\\nappreciation over medium to long term. However, there is no\\nassurance or guarantee that the investment objective of the Scheme\\nwill be achieved. The scheme does not assure or guarantee any\\nreturns.',\n",
       " 'DATE OF ALLOTMENT\\nSeptember 06,2018',\n",
       " 'FUND MANAGER\\nAmey Sathe (Managing Since 05-Apr-23 and overall\\nexperience of 16 years)',\n",
       " 'ASSISTANT FUND MANAGER\\nAditya Bagul (Managing Since 03-Oct-23 and overall\\nexperience of 11 years)',\n",
       " 'BENCHMARK\\nStd. Dev (Annualised) 12.06 13.50\\nSharpe Ratio 0.54 0.73\\nPortfolio Beta 0.82 NA\\nR Squared 0.90 NA\\nTreynor 0.67 NA\\nJenson -0.13 NA\\n^Risk-free rate based on the FBIL Overnight MIBOR rate of 6.76% as\\non Nov 29, 2024\\nFor calculation methodology please refer to Pg 100.',\n",
       " 'NAV (in Rs.)\\nDirect - IDCW : 25.5363\\nDirect - Growth : 25.5363\\nRegular - IDCW : 23.1880\\nRegular - Growth : 23.1880',\n",
       " 'FUND SIZE\\nRs. 2976.41 (Rs. in Cr.)',\n",
       " 'MONTHLY AVERAGE AUM\\nRs. 2956.69 (Rs. in Cr.)',\n",
       " 'TURN OVER\\nPortfolio Turnover (Equity component only)42.54%',\n",
       " 'EXPENSE RATIO\\nDirect 0.66\\nRegular 1.93\\n**Note: The rates specified are actual month end expenses charged\\nas on Nov 30, 2024. The above ratio includes the Service tax on\\nInvestment Management Fees. The above ratio excludes, borrowing\\ncost, wherever applicable.',\n",
       " 'VOLATILITY MEASURES',\n",
       " 'FUND',\n",
       " 'MINIMUM INVESTMENT\\nRs. 5,000/- and in multiples of Re. 1/- thereafter.',\n",
       " 'ADDITIONAL INVESTMENT\\nRs. 1,000/- and in multiples of Re. 1/- thereafter.',\n",
       " 'LOAD STRUCTURE\\nEntry Load : Not Applicable\\nExit Load :\\n1) Redemption/Switch-out/SWP/STP on or before expiry of 12 Months\\nfrom the date of allotment: If the withdrawal amount or switched out\\namount is not more than 12% of the original cost of investment - NIL']"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
