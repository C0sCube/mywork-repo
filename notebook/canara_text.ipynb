{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import fitz\n",
    "import warnings , math, collections , os, re, pprint, json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) \n",
    "\n",
    "\n",
    "path = r\"C:\\Users\\Kaustubh.keny\\OneDrive - Cogencis Information Services Ltd\\Documents\\mywork-repo\"\n",
    "#path = r\"C:\\Users\\rando\\OneDrive\\Documents\\mywork-repo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "canara_path = path + r\"\\files\\factsheet-march-2022.pdf\"\n",
    "dry_run_path  = path + r\"\\output\\DryRun.pdf\"\n",
    "indice_path = path + r\"\\output\\pkl\\indices_var.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_financial_indices(path:str):\n",
    "    final_indices = set()\n",
    "    with open(path , 'rb') as file:\n",
    "        indices = pickle.load(file)  \n",
    "        for k,v in indices.items():\n",
    "            temp = [k] + v\n",
    "            for t in temp:\n",
    "                final_indices.add(t)\n",
    "    \n",
    "    return final_indices\n",
    "\n",
    "\n",
    "\"\"\" Highlights important financial indices in the pdf, does other pre\n",
    "analysis of data.\n",
    "Args: list of indices, string of pdf path\n",
    "Returns: dict of pages highlighted, string of output pdf, dict of pages contaiting FUND NAMES\n",
    "\"\"\"\n",
    "def check_indice_highlight(path:str, indices_variations:list, fund_pattern:str, fund_size:int):\n",
    "    doc = fitz.open(path)\n",
    "    page_count = doc.page_count #No of pages\n",
    "    \n",
    "    pages = [i for i in range(page_count)]\n",
    "    important_pages = dict.fromkeys(pages, 0)\n",
    "    fund_titles = dict.fromkeys(pages, \"\")\n",
    "\n",
    "\n",
    "    for page_num, page in enumerate(doc):\n",
    "        \n",
    "        text_instances = page.get_text('dict')[\"blocks\"]\n",
    "        \n",
    "        #sort for all data in pdf document \n",
    "        sorted_text_instances = sorted(text_instances, key=lambda x: (x['bbox'][1], x['bbox'][0]))\n",
    "        \n",
    "        # rect = fitz.Rect((35,120,250,765))\n",
    "        # page.add_highlight_annot(rect)\n",
    "\n",
    "        for pgn,block in enumerate(sorted_text_instances):     \n",
    "            if \"lines\" not in block:\n",
    "                continue\n",
    "            \n",
    "            for line in block[\"lines\"]: \n",
    "                for span in line[\"spans\"]:\n",
    "                    if span['flags'] in [20,25, 16,0 ,4]:  # learn flag logic , rn set for all flags value\n",
    "                        span_text = span['text'].strip().lower()\n",
    "                        \n",
    "                        \n",
    "                        #FUND PAGE CHECK\n",
    "                        conditions = [\n",
    "                            pgn in range(0,15),\n",
    "                            re.match(fund_pattern, span_text, re.IGNORECASE),\n",
    "                            span['size'] >fund_size\n",
    "                        ]\n",
    "                        if all(conditions):\n",
    "        \n",
    "                            fund_titles[page_num] = span_text\n",
    "                            rect = fitz.Rect(span['bbox']) \n",
    "                            page.add_rect_annot(rect)\n",
    "                            \n",
    "                            rect = fitz.Rect((30,150,220,760))\n",
    "                            page.add_rect_annot(rect)  \n",
    "                        \n",
    "                        #CHECK IMP FINANCE INDICES  \n",
    "                        for term in indices_variations:  \n",
    "                            pattern = r'\\b' + re.escape(term.lower()) + r'\\b'\n",
    "                            if re.search(pattern, span_text):\n",
    "\n",
    "                                #count highlights\n",
    "                                important_pages[page_num] +=1\n",
    "                                #mark content\n",
    "                                rect = fitz.Rect(span['bbox']) \n",
    "                                page.add_highlight_annot(rect)\n",
    "                                break  #optional , one highlight\n",
    "\n",
    "    \n",
    "    output_path = None\n",
    "    if any(important_pages.values()):\n",
    "        output_path = path.replace('.pdf', '_highlighted.pdf')\n",
    "        doc.save(output_path)\n",
    "\n",
    "    doc.close()\n",
    "    return important_pages, output_path, fund_titles\n",
    "\n",
    "\n",
    "\"\"\" Get the clipped data in the bbox provided and store in nested dict\n",
    "Args: input path, dryrun path, important pages, bbox coords\n",
    "Returns: dict { 'page' : int 'block': dict}\"\"\"\n",
    "def get_clipped_data(input:str, output:str, pageSelect:list, bbox:list[set], fund_names:dict):\n",
    "    \n",
    "    document = fitz.open(input)\n",
    "    finalData = []\n",
    "    \n",
    "    for pgn in pageSelect:\n",
    "        #get the page\n",
    "        page = document[pgn]\n",
    "        fundName = fund_names[pgn]\n",
    "    \n",
    "        blocks = page.get_text('dict', clip = bbox[0])['blocks'] #get all blocks\n",
    "        \n",
    "        filtered_blocks = [block for block in blocks if block['type']==0 and 'lines' in block]\n",
    "        sorted_blocks = sorted(filtered_blocks, key= lambda x: (x['bbox'][1], x['bbox'][0]))\n",
    "        \n",
    "        finalData.append({\n",
    "            \"page\": pgn,\n",
    "            \"fundname\": fundName,\n",
    "            \"block\": sorted_blocks,\n",
    "        })\n",
    "            \n",
    "    return finalData\n",
    "\n",
    "def get_pdf_data(input:str, output:str, pageSelect:list, fund_names:dict):\n",
    "    \n",
    "    document = fitz.open(input)\n",
    "    finalData = []\n",
    "    \n",
    "    for pgn in pageSelect:\n",
    "        #get the page\n",
    "        page = document[pgn]\n",
    "        fundName = fund_names[pgn]\n",
    "    \n",
    "        blocks = page.get_text('dict')['blocks'] #get all blocks\n",
    "        \n",
    "        filtered_blocks = [block for block in blocks if block['type']==0 and 'lines' in block]\n",
    "        sorted_blocks = sorted(filtered_blocks, key= lambda x: (x['bbox'][1], x['bbox'][0]))\n",
    "        \n",
    "        finalData.append({\n",
    "            \"page\": pgn,\n",
    "            \"fundname\": fundName,\n",
    "            \"block\": sorted_blocks,\n",
    "        })\n",
    "            \n",
    "    return finalData\n",
    "\n",
    "def extract_span_data(data:list, name:list): #all\n",
    "    final_data = dict()\n",
    "    for pgn,page in enumerate(data):\n",
    "        pgn_content = []\n",
    "        for blocks in page['block']:\n",
    "            for line in blocks['lines']:\n",
    "                spans = line.get('spans',[])\n",
    "                for span in spans:\n",
    "                    \n",
    "                    text = span['text'].strip()\n",
    "                    size = span['size']\n",
    "                    color = span['color']\n",
    "                    origin = span['origin']\n",
    "                    bbox = span['bbox']\n",
    "                \n",
    "                    pgn_content.append([size,text,color,origin,bbox])\n",
    "                    \n",
    "        final_data[page['fundname']] = pgn_content\n",
    "    \n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = canara_path\n",
    "final_indices = get_financial_indices(indice_path)\n",
    "fund_pattern = r\"^(samco|tata|canara)\"\n",
    "fund_size = 10 #greater than logic\n",
    "highlight_pages, saved_path, fund_pages =  check_indice_highlight(file_path, final_indices, fund_pattern, fund_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             title  highlight_count\n",
      "0                                                                 0\n",
      "1                                                                 2\n",
      "2                                                                 1\n",
      "3                                                                 0\n",
      "4                                                                 2\n",
      "5                                                                19\n",
      "6              canara robeco flexicap fund (crfcf)               12\n",
      "7     canara robeco blue chip equity fund (crbcef)               13\n",
      "8           canara robeco emerging equities (cree)               12\n",
      "9             canara robeco small cap fund (crscf)               14\n",
      "10              canara robeco infrastructure (cri)               11\n",
      "11      canara robeco consumer trends fund (crctf)               11\n",
      "12    canara robeco equity tax saver fund (cretsf)               12\n",
      "13       canara robeco focused equity fund (crfef)               11\n",
      "14                 canara robeco value fund (crvf)               11\n",
      "15             canara robeco overnight fund (crof)               14\n",
      "16                 canara robeco liquid fund (crl)               12\n",
      "17    canara robeco ultra short term fund (crustf)               15\n",
      "18               canara robeco savings fund (crsf)               15\n",
      "19       canara robeco short duration fund (crsdf)               14\n",
      "20               canara robeco income fund (crinc)               16\n",
      "21         canara robeco dynamic bond fund (crdbf)               13\n",
      "22       canara robeco corporate bond fund (crcbf)               13\n",
      "23                canara robeco gilt fund (crgilt)               13\n",
      "24  canara robeco conservative hybrid fund (crchf)               16\n",
      "25        canara robeco equity hybrid fund (crehf)               16\n",
      "26                                                               14\n",
      "27                                                               13\n",
      "28                                                               13\n",
      "29                                                               12\n",
      "30                                                               12\n",
      "31                                                               27\n",
      "32                                                               23\n",
      "33                                                               12\n",
      "34                                                                8\n",
      "35                                                               13\n",
      "36                                                                9\n",
      "37                                                                1\n",
      "38                                                                1\n",
      "39                                                               19\n",
      "40                                                               10\n",
      "41                                                                1\n",
      "42                                                                1\n",
      "43                                                                1\n"
     ]
    }
   ],
   "source": [
    "pagedf = pd.DataFrame({'title': fund_pages.values(),'highlight_count': highlight_pages.values()})\n",
    "\n",
    "\"\"\"_summary_ fund is located only on certain pages, based on no. of \n",
    "highlights we know which pages are imp. automate this content later\n",
    "\"\"\"\n",
    "pagedf.to_excel(path + r'\\output\\example.xlsx')\n",
    "print(pagedf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_matrix_structure(data: list, header_font: float, content_font: float):\n",
    "    # Step 1: collect font sizes and coordinates\n",
    "    coordinates = []\n",
    "    fonts = set()\n",
    "\n",
    "    for block in data['block']:\n",
    "        for line in block['lines']:\n",
    "            for span in line['spans']:\n",
    "                origin = tuple(span['origin'])  # Top-left coordinates\n",
    "                coordinates.append(origin)\n",
    "                fonts.add(span['size'])\n",
    "\n",
    "    coordinates = sorted(set(coordinates), key=lambda c: (c[1], c[0]))  # Sort by y, then x\n",
    "    fonts = sorted(fonts, reverse=True)  # Descending order of font size\n",
    "\n",
    "    # Step 2: create the matrix\n",
    "    coord_to_index = {coord: idx for idx, coord in enumerate(coordinates)}  # (x,y) at pos 0 etc. ROWS\n",
    "    font_to_index = {font: idx for idx, font in enumerate(fonts)}  # COLUMNS\n",
    "    matrix = np.zeros((len(coordinates), len(fonts)), dtype=object)  # Set all to zeros initially\n",
    "\n",
    "    # Step 3: matrix populate and nested dict add\n",
    "    nested_dict = {}\n",
    "    current_subheader = None\n",
    "\n",
    "    for block in data['block']:\n",
    "        for line in block['lines']:\n",
    "            for span in line['spans']:\n",
    "                origin = tuple(span['origin'])  # Top-left x,y\n",
    "                font = span['size']\n",
    "                text_preview = span['text']  # Get the first two words of the text\n",
    "\n",
    "                # Populate the matrix with text preview\n",
    "                if origin in coord_to_index and font in font_to_index:\n",
    "                    row = coord_to_index[origin]\n",
    "                    col = font_to_index[font]\n",
    "                    if matrix[row, col] == 0:\n",
    "                        matrix[row, col] = \"na\"\n",
    "                    matrix[row, col] = text_preview\n",
    "\n",
    "                # Build the nested dictionary\n",
    "                if font == header_font:\n",
    "                    current_subheader = span\n",
    "                    nested_dict[current_subheader['text']] = []\n",
    "                elif font <= content_font and current_subheader:\n",
    "                    nested_dict[current_subheader['text']].append(span)\n",
    "\n",
    "    matrix_df = pd.DataFrame(matrix, index=coordinates, columns=fonts)\n",
    "\n",
    "    return nested_dict, matrix_df\n",
    "def generate_pdf_from_data(data:list, output_path:str):\n",
    "    pdf_doc = fitz.open()\n",
    "    \n",
    "    for section, spans in data.items():\n",
    "    \n",
    "        page = pdf_doc.new_page()\n",
    "        text_position = 72  # for title initalize something\n",
    "\n",
    "        #section title\n",
    "        title_font_size = 14 \n",
    "        try:\n",
    "            page.insert_text(\n",
    "                (72, text_position), #initalizor\n",
    "                section,\n",
    "                fontsize=title_font_size,\n",
    "                fontname=\"helv\",\n",
    "                color=(0, 0, 1),\n",
    "            )        \n",
    "        except Exception as e:\n",
    "            print(f\"The error is {e}\")\n",
    "            \n",
    "        #content title\n",
    "        for span in spans:\n",
    "            bbox = span.get(\"bbox\", [0, 0, 0, 0])  # default \n",
    "\n",
    "            #Errror in fitz font \n",
    "            try:\n",
    "                page.insert_text(\n",
    "                    (bbox[0], bbox[1]),\n",
    "                    span[\"text\"],\n",
    "                    fontsize=span[\"size\"],\n",
    "                    fontname=\"helv\",\n",
    "                    color=tuple(int(span[\"color\"] & 0xFFFFFF) for _ in range(3)))#unsigned int value so (0,0,0)\n",
    "                \n",
    "            except Exception:\n",
    "                page.insert_text(\n",
    "                    (bbox[0], bbox[1]),\n",
    "                    span[\"text\"],\n",
    "                    fontsize=span[\"size\"],\n",
    "                    fontname=\"helv\",\n",
    "                    color=(1, 0, 0),\n",
    "                )\n",
    "\n",
    "    # Save the created PDF\n",
    "    pdf_doc.save(output_path)\n",
    "    pdf_doc.close()\n",
    "    print(f\"  PDF generated to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def clean_block_data(blocks):\n",
    "    \n",
    "    remove_text = ['Purchase','Amount','thereafter','.','. ',',',':','st',\";\",\"-\",'st ',' ','th', 'th ', 'rd', 'rd ', 'nd', 'nd ','','`','(Date of Allotment)']\n",
    "    \n",
    "    sorted_blocks = sorted(blocks, key=lambda x: (x[3][1],x[3][0]))\n",
    "    \n",
    "    cleaned_blocks = []\n",
    "    for block in sorted_blocks:\n",
    "        size, text, color, origin, bbox = block\n",
    "        if text not in remove_text:\n",
    "            cleaned_blocks.append(block)\n",
    " \n",
    "    processed_blocks = []\n",
    "    # adjust size based on color and size\n",
    "    for block in cleaned_blocks:\n",
    "        size, text, color, origin, bbox = block\n",
    "        text = text.strip()\n",
    "        if size in [9.0,8.0] and color == -1:\n",
    "            size = 20.0  # Update size to 20.0\n",
    "        processed_blocks.append([size, text, color, origin, bbox])\n",
    "                \n",
    "\n",
    "    # group blocks by rounded y-coordinate\n",
    "    grouped_blocks = defaultdict(list)\n",
    "    for block in processed_blocks:\n",
    "        y_coord = math.ceil(block[3][1])# Extract and round the y-coordinate\n",
    "        size = block[0]\n",
    "        grouped_blocks[(y_coord,size)].append(block)\n",
    "\n",
    "    # Combine blocks with the same y-coordinate\n",
    "    combined_blocks = []\n",
    "    for key, group in grouped_blocks.items():\n",
    "        \n",
    "        if key[1] == 20:\n",
    "            combined_text = \" \".join(item[1] for item in group).strip()\n",
    "            if combined_text:  # Ignore whitespace-only text\n",
    "                size, color, origin, bbox = group[0][0], group[0][2], group[0][3],group[0][4]\n",
    "                combined_blocks.append([size, combined_text, color, origin,bbox])\n",
    "        \n",
    "        else:\n",
    "            for item in group:\n",
    "                combined_blocks.append(item)\n",
    "\n",
    "    return combined_blocks\n",
    "\n",
    "def process_text_data(text_data):\n",
    "    \n",
    "    updated_text_data = {}\n",
    "\n",
    "    for fund, data in text_data.items():\n",
    "        blocks = data\n",
    "        cleaned_blocks = clean_block_data(blocks)\n",
    "        updated_text_data[fund] = cleaned_blocks\n",
    "\n",
    "    return updated_text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = [ i for i in range(6,26)]\n",
    "bbox = [(30,150,215,760)]\n",
    "\n",
    "data = get_clipped_data(canara_path, dry_run_path, pages, bbox, fund_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data  = extract_span_data(data,[])\n",
    "cleaned_data = process_text_data(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data['canara robeco value fund (crvf)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data['canara robeco value fund (crvf)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_size = 20\n",
    "content_size = 10 #anything less than\n",
    "final_text_data = dict()\n",
    "final_matrix = dict()\n",
    "\n",
    "for fund, items in cleaned_data.items(): #ech fund\n",
    "    \n",
    "    #step 1 extract size, coord\n",
    "    coordinates = list()\n",
    "    sizes = set()\n",
    "    \n",
    "    for item in items: #size,text,color,origin\n",
    "        origin = tuple(item[3])\n",
    "        coordinates.append(origin)\n",
    "        sizes.add(item[0])\n",
    "    \n",
    "    coordinates = sorted(set(coordinates), key=lambda c: (c[1], c[0]))  # Sort by y, then x\n",
    "    sizes = sorted(sizes, reverse=True)  \n",
    "    \n",
    "    #step 2 create matrix\n",
    "    coord_to_index = {coord: idx for idx, coord in enumerate(coordinates)}  # (x,y) at pos 0 etc. ROWS\n",
    "    size_to_index = {font: idx for idx, font in enumerate(sizes)}  # COLUMNS\n",
    "    matrix = np.zeros((len(coordinates), len(sizes)), dtype=object)\n",
    "    \n",
    "    \n",
    "    #step 3\n",
    "    nested_dict = {}\n",
    "    current_header = None\n",
    "    for item in items:\n",
    "        origin = tuple(item[3])\n",
    "        size = item[0]\n",
    "        text = item[1].strip()\n",
    "        \n",
    "        #populate the matrix\n",
    "        if origin in coord_to_index and size in size_to_index:\n",
    "            row = coord_to_index[origin]\n",
    "            col = size_to_index[size]\n",
    "            \n",
    "            if matrix[row,col] == 0:\n",
    "                matrix[row,col] ==r\"nil\"\n",
    "            matrix[row,col] == text\n",
    "        \n",
    "        #build nested dict\n",
    "        if size == header_size:\n",
    "            current_header = \"_\".join([i for i in text.split(\" \") if i != '']).lower()\n",
    "            nested_dict[current_header] = []\n",
    "        elif size<= content_size and current_header:\n",
    "            nested_dict[current_header].append(item)\n",
    "            \n",
    "    final_text_data[fund] = nested_dict        \n",
    "    matrix_df = pd.DataFrame(matrix, index=coordinates, columns=sizes)\n",
    "    final_matrix[fund] = matrix_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pdf_from_data(data:list, output_path:str):\n",
    "    \n",
    "    pdf_doc = fitz.open()\n",
    "    \n",
    "    for header, items in data.items():\n",
    "        \n",
    "        page = pdf_doc.new_page()\n",
    "        text_position = 72  # for title initalize something\n",
    "\n",
    "        #section title\n",
    "        title_font_size = 24\n",
    "        try:\n",
    "            page.insert_text(\n",
    "                (72, text_position), #initalizor\n",
    "                header,\n",
    "                fontsize=title_font_size,\n",
    "                fontname=\"helv\",\n",
    "                color=(0, 0, 1),\n",
    "            )        \n",
    "        except Exception as e:\n",
    "            print(f\"Error while parsing fund {e}\")\n",
    "            \n",
    "        for item in items:\n",
    "            \n",
    "            bbox = item[3] #origin coords\n",
    "            text = item[1]\n",
    "            size = item[0]\n",
    "            color = item[2]\n",
    "   \n",
    "            #Errror in fitz font \n",
    "            try:\n",
    "                page.insert_text(\n",
    "                    (bbox[0], bbox[1]),\n",
    "                    text,\n",
    "                    fontsize=size,\n",
    "                    fontname=\"helv\",\n",
    "                    color=tuple(int(color & 0xFFFFFF) for _ in range(3)))#unsigned int value so (0,0,0)\n",
    "                \n",
    "            except Exception:\n",
    "                page.insert_text(\n",
    "                    (bbox[0], bbox[1]),\n",
    "                    text,\n",
    "                    fontsize=size,\n",
    "                    fontname=\"helv\",\n",
    "                    color=(1, 0, 0),\n",
    "                )\n",
    "\n",
    "    # Save the created PDF\n",
    "    pdf_doc.save(output_path)\n",
    "    pdf_doc.close()\n",
    "    print(f\" PDF generated to: {output_path}\")\n",
    "\n",
    "def extract_data_from_pdf(path:str):\n",
    "    \n",
    "    # def replace_main_key(string: str):\n",
    "    #     replace_key = string\n",
    "    #     if re.match(r'^NAV.*as on', string, re.IGNORECASE):\n",
    "    #         replace_key = \"NAV\" \n",
    "    #     elif \"market\" in string.lower():\n",
    "    #         replace_key = \"Market Cap\"\n",
    "    #     elif re.match(r\"Assets Under Management\", string, re.IGNORECASE):\n",
    "    #         replace_key = \"Assets Under Management\"   \n",
    "    #     return replace_key\n",
    "    \n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        final_data = []\n",
    "        final_data_generated = {}\n",
    "        \n",
    "        for page in pdf.pages:\n",
    "            # extract text from the page\n",
    "            text = page.extract_text()\n",
    "            final_data.append(text)\n",
    "        \n",
    "        #store them in a dict for each page\n",
    "        for data in final_data[1:]:\n",
    "            content = data.split('\\n')\n",
    "            main_key = content[0]\n",
    "            values = content[1:]\n",
    "        \n",
    "            final_data_generated[main_key] = values\n",
    "\n",
    "        #sort the headers in lex order\n",
    "        sorted_final_generated = {key: final_data_generated[key] for key in sorted(final_data_generated)}\n",
    "\n",
    "    return sorted_final_generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "canara robeco flexicap fund (crfcf)\n",
      " PDF generated to: C:\\Users\\Kaustubh.keny\\OneDrive - Cogencis Information Services Ltd\\Documents\\mywork-repo\\output\\DryRun.pdf\n",
      "canara robeco blue chip equity fund (crbcef)\n",
      " PDF generated to: C:\\Users\\Kaustubh.keny\\OneDrive - Cogencis Information Services Ltd\\Documents\\mywork-repo\\output\\DryRun.pdf\n",
      "canara robeco emerging equities (cree)\n",
      " PDF generated to: C:\\Users\\Kaustubh.keny\\OneDrive - Cogencis Information Services Ltd\\Documents\\mywork-repo\\output\\DryRun.pdf\n",
      "canara robeco small cap fund (crscf)\n",
      " PDF generated to: C:\\Users\\Kaustubh.keny\\OneDrive - Cogencis Information Services Ltd\\Documents\\mywork-repo\\output\\DryRun.pdf\n",
      "canara robeco infrastructure (cri)\n",
      " PDF generated to: C:\\Users\\Kaustubh.keny\\OneDrive - Cogencis Information Services Ltd\\Documents\\mywork-repo\\output\\DryRun.pdf\n",
      "canara robeco consumer trends fund (crctf)\n",
      " PDF generated to: C:\\Users\\Kaustubh.keny\\OneDrive - Cogencis Information Services Ltd\\Documents\\mywork-repo\\output\\DryRun.pdf\n",
      "canara robeco equity tax saver fund (cretsf)\n",
      " PDF generated to: C:\\Users\\Kaustubh.keny\\OneDrive - Cogencis Information Services Ltd\\Documents\\mywork-repo\\output\\DryRun.pdf\n",
      "canara robeco focused equity fund (crfef)\n",
      " PDF generated to: C:\\Users\\Kaustubh.keny\\OneDrive - Cogencis Information Services Ltd\\Documents\\mywork-repo\\output\\DryRun.pdf\n",
      "canara robeco value fund (crvf)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot save with zero pages",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18244\\800918817.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfinal_extracted_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfund\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitems\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfinal_text_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfund\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mgenerate_pdf_from_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdry_run_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mextract_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_data_from_pdf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdry_run_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mfinal_extracted_text\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfund\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18244\\3316650543.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(data, output_path)\u001b[0m\n\u001b[0;32m     45\u001b[0m                     \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m                 \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;31m# Save the created PDF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m     \u001b[0mpdf_doc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m     \u001b[0mpdf_doc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33mf\"\u001b[0m\u001b[1;33m PDF generated to: \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Kaustubh.keny\\OneDrive - Cogencis Information Services Ltd\\Documents\\mywork-repo\\venv\\Lib\\site-packages\\pymupdf\\__init__.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, filename, garbage, clean, deflate, deflate_images, deflate_fonts, incremental, ascii, expand, linear, no_new_id, appearance, pretty, encryption, permissions, owner_pw, user_pw, preserve_metadata, use_objstms, compression_effort)\u001b[0m\n\u001b[0;32m   5603\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"save to original must be incremental\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5604\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlinear\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0muse_objstms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5605\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"'linear' and 'use_objstms' cannot both be requested\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5606\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_count\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5607\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cannot save with zero pages\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5608\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mincremental\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5609\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5610\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"incremental needs original file\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot save with zero pages"
     ]
    }
   ],
   "source": [
    "final_extracted_text = dict()\n",
    "for fund, items in final_text_data.items():\n",
    "    print(fund)\n",
    "    generate_pdf_from_data(items, dry_run_path)\n",
    "    extract_data = extract_data_from_pdf(dry_run_path)\n",
    "    final_extracted_text[fund] = extract_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
