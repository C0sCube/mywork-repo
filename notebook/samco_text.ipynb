{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import fitz\n",
    "import warnings , math, collections , os, re, pprint\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) \n",
    "\n",
    "\n",
    "path = r\"C:\\Users\\Kaustubh.keny\\OneDrive - Cogencis Information Services Ltd\\Documents\\mywork-repo\"\n",
    "#path = r\"C:\\Users\\rando\\OneDrive\\Documents\\mywork-repo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "samco_path = path + r\"\\files\\58_29-Feb-24_FS.pdf\"\n",
    "dry_path  = path + r\"\\output\\DryRun.pdf\"\n",
    "indice_path = path + r\"\\output\\pkl\\indices_var.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the indices \n",
    "\n",
    "def get_indices(path:str):\n",
    "    final_indices = set()\n",
    "    with open(path , 'rb') as file:\n",
    "        indices = pickle.load(file)  \n",
    "        for k,v in indices.items():\n",
    "            temp = [k] + v\n",
    "            for t in temp:\n",
    "                final_indices.add(t)\n",
    "    \n",
    "    return final_indices\n",
    "      \n",
    "final_indices = get_indices(indice_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Highlights important financial indices in the pdf, does other pre\n",
    "analysis of data.\n",
    "Args: list of indices, string of pdf path\n",
    "Returns: dict of pages highlighted, string of output pdf, dict of pages contaiting FUND NAMES\n",
    "\"\"\"\n",
    "def check_indice_highlight(indices_variations:list, path:str):\n",
    "    \n",
    "    doc = fitz.open(path)\n",
    "    page_count = doc.page_count #No of pages\n",
    "\n",
    "    pages = [i for i in range(page_count)]\n",
    "    important_pages = dict.fromkeys(pages, 0)\n",
    "    fund_titles = dict.fromkeys(pages, \"\")\n",
    "    fund_pattern = r\"^(samco|tata|canara|icici).*fund$\" #REGEX FOR FUND TITLE\n",
    "\n",
    "\n",
    "    for page_num, page in enumerate(doc):\n",
    "        \n",
    "        text_instances = page.get_text('dict')[\"blocks\"]\n",
    "        sorted_text_instances = sorted(text_instances, key=lambda x: (x['bbox'][1], x['bbox'][0])) #top to bottom, left to right\n",
    "        \n",
    "        # rect = fitz.Rect((35,120,250,765))\n",
    "        # page.add_highlight_annot(rect)\n",
    "\n",
    "        for pgn,block in enumerate(sorted_text_instances):     \n",
    "            if \"lines\" in block: \n",
    "                for line in block[\"lines\"]: \n",
    "                    for span in line[\"spans\"]:\n",
    "                        if span['flags'] in [20,25]:  # learn flag logic , rn set for all flags value\n",
    "                            span_text = span['text'].lower()\n",
    "                            \n",
    "                            #FUND HEADER CONDITIONS\n",
    "                            if all([pgn in range(0,5), \n",
    "                                re.match(fund_pattern, span_text, re.IGNORECASE),\n",
    "                                span['size']> 20]):\n",
    "                                #print(span_text, page_num)\n",
    "                                fund_titles[page_num] = span_text\n",
    "                                \n",
    "                                \n",
    "                            for term in indices_variations:  \n",
    "                                pattern = r'\\b' + re.escape(term.lower()) + r'\\b' #CHECK IMP FINANCE INDICES\n",
    "                                if re.search(pattern, span_text):\n",
    "    \n",
    "                                    #count highlights\n",
    "                                    if page_num in important_pages:\n",
    "                                        important_pages[page_num] +=1\n",
    "                                    #mark content\n",
    "                                    rect = fitz.Rect(span['bbox']) \n",
    "                                    page.add_highlight_annot(rect)\n",
    "                                    break  #optional , one highlight\n",
    "\n",
    "    if important_pages:\n",
    "        output_path = path.replace('.pdf', '_highlighted.pdf')\n",
    "        doc.save(output_path)\n",
    "        doc.close()\n",
    "        return important_pages, output_path, fund_titles\n",
    "    else:\n",
    "        doc.close()\n",
    "        return important_pages, None, fund_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>highlight_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>samco flexi cap fund</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>samco flexi cap fund</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>samco elss tax saver fund</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>samco elss tax saver fund</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>samco active momentum fund</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>samco active momentum fund</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>samco dynamic asset allocation fund</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>samco dynamic asset allocation fund</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>samco overnight fund</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td></td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td></td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td></td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  title  highlight_count\n",
       "0                                                      0\n",
       "1                                                     11\n",
       "2                                                      0\n",
       "3                  samco flexi cap fund                9\n",
       "4                  samco flexi cap fund                1\n",
       "5             samco elss tax saver fund                8\n",
       "6             samco elss tax saver fund                1\n",
       "7            samco active momentum fund                8\n",
       "8            samco active momentum fund                1\n",
       "9   samco dynamic asset allocation fund               12\n",
       "10  samco dynamic asset allocation fund                1\n",
       "11                 samco overnight fund               11\n",
       "12                                                    12\n",
       "13                                                    12\n",
       "14                                                    12\n",
       "15                                                     4\n",
       "16                                                     0\n",
       "17                                                     0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highlight_pages, saved_path, fund_pages =  check_indice_highlight(final_indices, samco_path)\n",
    "pagedf = pd.DataFrame({'title': fund_pages.values(),'highlight_count': highlight_pages.values()})\n",
    "pagedf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fund is located only on certain pages, based on no. of highlights we know which pages are imp. automate this content later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = [3,5,7,9,11,12]\n",
    "bbox = [(35,120,250,765)]\n",
    "def get_data_clipped(input:str, output:str, pageSelect:list, bbox:list[set]):\n",
    "    \n",
    "    document = fitz.open(input)\n",
    "    finalData = []\n",
    "    \n",
    "    for pgn, pages in enumerate(document):\n",
    "        if pgn in pageSelect:\n",
    "            page = document[pgn]\n",
    "        \n",
    "            blocks = page.get_text('dict', clip = bbox[0])['blocks'] #get all blocks\n",
    "            filtered_blocks = [block for block in blocks if block['type']==0 and 'lines' in block]\n",
    "            sorted_blocks = sorted(filtered_blocks, key= lambda x: (x['bbox'][1], x['bbox'][0]))\n",
    "            finalData.append({\n",
    "                \"page\": pgn,\n",
    "                \"block\": sorted_blocks\n",
    "            })\n",
    "            \n",
    "    return finalData\n",
    "\n",
    "def clean_data_combine(data: list, check_color: int, replace_size):\n",
    "    \n",
    "    remove_text = ['.','. ',',',':','st',\";\",\"-\",'st ',' ','th', 'th ', 'rd', 'rd ', 'nd', 'nd ']\n",
    "    \n",
    "    for page in data:\n",
    "        for blocks in page['block']:\n",
    "            for lines in  blocks.get('lines',[]):\n",
    "            \n",
    "                #REMOVE WASTE VALUES\n",
    "                lines['spans'] = [\n",
    "                    span for span in lines.get(\"spans\", [])\n",
    "                            if span.get(\"text\").strip() not in remove_text\n",
    "                ]\n",
    "                \n",
    "                #MAKE SAME COLOR AND SIZE\n",
    "                for span in lines.get('spans',[]):\n",
    "                    if span['color'] == check_color and span['size'] > 7:\n",
    "                        span['size'] = replace_size\n",
    "                \n",
    "                \n",
    "                #COMBINE SPANS HAVING SIMILAR PROP\n",
    "                combined_spans = []\n",
    "                for span in lines.get(\"spans\", []):\n",
    "                    if combined_spans and all(\n",
    "                        combined_spans[-1].get(key) == span.get(key)\n",
    "                        for key in [\"flags\", \"size\", \"color\"]\n",
    "                    ):\n",
    "                        # Combine text if spans are similar\n",
    "                        combined_spans[-1][\"text\"] += \" \" + span[\"text\"]\n",
    "                    else:\n",
    "                        combined_spans.append(span)\n",
    "                lines[\"spans\"] = combined_spans\n",
    "                \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_span_data(data:list, name:str): #all\n",
    "    final_data = dict()\n",
    "    for pgn,page in enumerate(data):\n",
    "        pgn_content = []\n",
    "        for blocks in page['block']:\n",
    "            for line in blocks['lines']:\n",
    "                spans = line.get('spans',[])\n",
    "                for span in spans:\n",
    "                    pgn_content.append(span[name])\n",
    "                \n",
    "        final_data[f\"Page: {pgn + 1}\"] = pgn_content\n",
    "    \n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data_clipped(samco_path, dry_path,pages, bbox)\n",
    "#text_data = return_span_data(data, 'text')\n",
    "#data[2]['block']\n",
    "clean_data = clean_data_combine(data, -1,12.0)\n",
    "#set font whose color is __ to size __ \n",
    "#Bad Logic here btw\n",
    "#Page 0,1,2,3 indexes-> 3,5,7,9 pages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_matrix_structure(data: list, header_font: float, content_font: float):\n",
    "    # Step 1: collect font sizes and coordinates\n",
    "    coordinates = []\n",
    "    fonts = set()\n",
    "\n",
    "    for block in data['block']:\n",
    "        for line in block['lines']:\n",
    "            for span in line['spans']:\n",
    "                origin = tuple(span['origin'])  # Top-left coordinates\n",
    "                coordinates.append(origin)\n",
    "                fonts.add(span['size'])\n",
    "\n",
    "    coordinates = sorted(set(coordinates), key=lambda c: (c[1], c[0]))  # Sort by y, then x\n",
    "    fonts = sorted(fonts, reverse=True)  # Descending order of font size\n",
    "\n",
    "    # Step 2: create the matrix\n",
    "    coord_to_index = {coord: idx for idx, coord in enumerate(coordinates)}  # (x,y) at pos 0 etc. ROWS\n",
    "    font_to_index = {font: idx for idx, font in enumerate(fonts)}  # COLUMNS\n",
    "    matrix = np.zeros((len(coordinates), len(fonts)), dtype=object)  # Set all to zeros initially\n",
    "\n",
    "    # Step 3: matrix populate and nested dict add\n",
    "    nested_dict = {}\n",
    "    current_subheader = None\n",
    "\n",
    "    for block in data['block']:\n",
    "        for line in block['lines']:\n",
    "            for span in line['spans']:\n",
    "                origin = tuple(span['origin'])  # Top-left x,y\n",
    "                font = span['size']\n",
    "                text_preview = span['text']  # Get the first two words of the text\n",
    "\n",
    "                # Populate the matrix with text preview\n",
    "                if origin in coord_to_index and font in font_to_index:\n",
    "                    row = coord_to_index[origin]\n",
    "                    col = font_to_index[font]\n",
    "                    if matrix[row, col] == 0:\n",
    "                        matrix[row, col] = \"na\"\n",
    "                    matrix[row, col] = text_preview\n",
    "\n",
    "                # Build the nested dictionary\n",
    "                if font == header_font:\n",
    "                    current_subheader = span\n",
    "                    nested_dict[current_subheader['text']] = []\n",
    "                elif font <= content_font and current_subheader:\n",
    "                    nested_dict[current_subheader['text']].append(span)\n",
    "\n",
    "    matrix_df = pd.DataFrame(matrix, index=coordinates, columns=fonts)\n",
    "\n",
    "    return nested_dict, matrix_df\n",
    "def generate_pdf_from_data(data:list, output_path:str):\n",
    "    pdf_doc = fitz.open()\n",
    "    \n",
    "    for section, spans in data.items():\n",
    "    \n",
    "        page = pdf_doc.new_page()\n",
    "        text_position = 72  # for title initalize something\n",
    "\n",
    "        #section title\n",
    "        title_font_size = 14 \n",
    "        try:\n",
    "            page.insert_text(\n",
    "                (72, text_position), #initalizor\n",
    "                section,\n",
    "                fontsize=title_font_size,\n",
    "                fontname=\"helv\",\n",
    "                color=(0, 0, 1),\n",
    "            )        \n",
    "        except Exception as e:\n",
    "            print(f\"The error is {e}\")\n",
    "            \n",
    "        #content title\n",
    "        for span in spans:\n",
    "            bbox = span.get(\"bbox\", [0, 0, 0, 0])  # default \n",
    "\n",
    "            #Errror in fitz font \n",
    "            try:\n",
    "                page.insert_text(\n",
    "                    (bbox[0], bbox[1]),\n",
    "                    span[\"text\"],\n",
    "                    fontsize=span[\"size\"],\n",
    "                    fontname=\"helv\",\n",
    "                    color=tuple(int(span[\"color\"] & 0xFFFFFF) for _ in range(3)))#unsigned int value so (0,0,0)\n",
    "                \n",
    "            except Exception:\n",
    "                page.insert_text(\n",
    "                    (bbox[0], bbox[1]),\n",
    "                    span[\"text\"],\n",
    "                    fontsize=span[\"size\"],\n",
    "                    fontname=\"helv\",\n",
    "                    color=(1, 0, 0),\n",
    "                )\n",
    "\n",
    "    # Save the created PDF\n",
    "    pdf_doc.save(output_path)\n",
    "    pdf_doc.close()\n",
    "    print(f\"PDF successfully generated to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_invest_data(key:str,data:list):\n",
    "    investment_objective = data\n",
    "    values = \" \".join(txt for txt in investment_objective)\n",
    "\n",
    "    data = {\n",
    "        key:values\n",
    "    }\n",
    "\n",
    "    return data\n",
    "\n",
    "def return_scheme_data(key:str,data:list):\n",
    "    scheme_data = data\n",
    "    main_key = key\n",
    "    structured_data = {main_key: {}}\n",
    "\n",
    "    # Patterns\n",
    "    date_pattern = r\"^(.*?date)\\s(\\d{2}-[A-Za-z]{3}-\\d{4})$\"\n",
    "    benchmark_pattern = r\"^(Benchmark)\\s+(.*)$\"\n",
    "    application_pattern = r\"(?:路)?\\d+(?:,\\d{3})*(?:\\.\\d+)?/-\"\n",
    "\n",
    "    for data in scheme_data:\n",
    "        if re.search(date_pattern, data, re.IGNORECASE):\n",
    "            match = re.match(date_pattern, data, re.IGNORECASE)\n",
    "            if match:\n",
    "                key = match.group(1)\n",
    "                value = match.group(2)\n",
    "                structured_data[main_key][key] = value\n",
    "        elif re.search(benchmark_pattern, data, re.IGNORECASE):\n",
    "            match = re.match(benchmark_pattern, data, re.IGNORECASE)\n",
    "            if match:\n",
    "                key = match.group(1)\n",
    "                value = match.group(2)\n",
    "                structured_data[main_key][key] = value\n",
    "        elif re.search(r\"\\b(min|application)\\b\", data, re.IGNORECASE):\n",
    "            matches = re.findall(application_pattern, data, re.IGNORECASE)\n",
    "            if matches:\n",
    "                cleaned_matches = [match.replace('路', '') for match in matches]\n",
    "                structured_data[main_key][\"min_appl_amt\"] = cleaned_matches\n",
    "        elif re.search(r\"\\b(additional.* and in multiples of)\\b\", data, re.IGNORECASE):\n",
    "            matches = re.findall(application_pattern, data, re.IGNORECASE)\n",
    "            if matches:\n",
    "                cleaned_matches = [match.replace('路', '') for match in matches]\n",
    "                structured_data[main_key][\"additional_amt\"] = cleaned_matches\n",
    "\n",
    "    return structured_data\n",
    "\n",
    "def return_fund_data(key:str,data:list):\n",
    "    fund_manager = data\n",
    "    main_key = key\n",
    "    strucuted_data = {main_key:[]}\n",
    "    current_entry = None\n",
    "    name_pattern = r'^(Ms\\.|Mr\\.)'\n",
    "    manage_pattern = r'^\\(|\\)$'\n",
    "    date_pattern = r'\\b\\w+ \\d{1,2}, \\d{4}\\b'\n",
    "    experience_pattern = r'^Total Experience: (.+)$'\n",
    "\n",
    "    for data in fund_manager:\n",
    "        if re.match(name_pattern,data):\n",
    "            if current_entry:\n",
    "                strucuted_data[main_key].append(current_entry)\n",
    "            current_entry = {\n",
    "                'name': data.split(\",\")[0].strip().lower(),\n",
    "                'designation': \"\".join(data.split(\",\")[1:]).strip().lower()\n",
    "            }\n",
    "            #print(data.split(\",\")[0],\"\".join(data.split(\",\")[1:]))\n",
    "        elif re.match(manage_pattern,data):\n",
    "            if \"inception\" in data.lower():\n",
    "                current_entry['managing_since'] = 'inception'\n",
    "            else:\n",
    "                date = re.search(date_pattern, data)\n",
    "                current_entry['managing_since'] = date.group() if date != None else None\n",
    "        elif re.match(experience_pattern,data):\n",
    "            current_entry['total_experience'] = data.split(\":\")[1].strip().lower()\n",
    "            #print(data.split(\":\")[1])\n",
    "\n",
    "        \n",
    "    if current_entry:  # Append the last entry\n",
    "        strucuted_data[main_key].append(current_entry)\n",
    "            \n",
    "    return strucuted_data\n",
    "\n",
    "def return_nav_data(key:str,data:list):\n",
    "    main_key = key\n",
    "    structured_data = {main_key: {}}\n",
    "    \n",
    "    growth_pattern = r\"([\\w\\s]+):\\s*路([\\d.]+)\"\n",
    "    \n",
    "    for line in data:\n",
    "        matches = re.findall(growth_pattern, line)\n",
    "        for key, value in matches:\n",
    "            structured_data[main_key][key.strip().lower()] = value\n",
    "        \n",
    "    return structured_data\n",
    "\n",
    "def return_quant_data(key:str,data:list):\n",
    "    qunatitative_data = data\n",
    "    main_key = key\n",
    "\n",
    "    strucuted_data = {main_key:{}}\n",
    "    current_entry = None\n",
    "    comment = \"\"\n",
    "\n",
    "    ratio_pattern = r\"\\b(ratio|turnover)\\b\"\n",
    "    annual_pattern = r'\\b(annualised|YTM)\\b'\n",
    "    macaulay_pattern = r\"\\b(macaulay.*duration)\\b\"\n",
    "    residual_pattern = r\"\\b(residual.*maturity)\\b\"\n",
    "    modified_pattern = r\"\\b(modified.*duration)\\b\"\n",
    "\n",
    "    for data in qunatitative_data:\n",
    "        if re.search(ratio_pattern,data, re.IGNORECASE):\n",
    "            key = data.split(\":\")[0].lower().strip()\n",
    "            value = data.split(\":\")[1].lower().strip()\n",
    "        elif re.search(annual_pattern,data, re.IGNORECASE):\n",
    "            key = data.split(\":\")[0].lower().strip()\n",
    "            value = data.split(\":\")[1].lower().strip()\n",
    "        elif re.search(macaulay_pattern,data, re.IGNORECASE):\n",
    "            key = data.split(\":\")[0].lower().strip()\n",
    "            value = data.split(\":\")[1].lower().strip()\n",
    "        elif re.search(residual_pattern,data, re.IGNORECASE):\n",
    "            key = data.split(\":\")[0].lower().strip()\n",
    "            value = data.split(\":\")[1].lower().strip()\n",
    "        elif re.search(modified_pattern,data, re.IGNORECASE):\n",
    "            key = data.split(\":\")[0].lower().strip()\n",
    "            value = data.split(\":\")[1].lower().strip()\n",
    "        else:\n",
    "            comment+= data\n",
    "        strucuted_data[main_key][key] = value\n",
    "    \n",
    "    strucuted_data[main_key]['comment'] = comment\n",
    "\n",
    "    return strucuted_data\n",
    "\n",
    "def return_aum_data(key:str,data:list):\n",
    "    \n",
    "    aum = data\n",
    "    main_key = key\n",
    "    strucuted_data = {main_key:{}}\n",
    "\n",
    "    pattern = r\"\\b\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)? Crs\\b\"\n",
    "\n",
    "    for data in aum:\n",
    "        if re.search(r'average', data, re.IGNORECASE):\n",
    "            match = re.search(pattern, data)\n",
    "            key = 'avg_aum'\n",
    "        else:\n",
    "            match = re.search(pattern, data)\n",
    "            key = \"aum\"\n",
    "        \n",
    "        strucuted_data[main_key][key] = match.group()\n",
    "\n",
    "    return strucuted_data\n",
    "\n",
    "def return_mar_data(key:str,data:list):\n",
    "    return {\n",
    "        key: None\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_main_key(string: str):\n",
    "    replace_key = string\n",
    "    if re.match(r'^NAV.*as on', string, re.IGNORECASE):\n",
    "        replace_key = \"NAV\" \n",
    "    elif \"market\" in string.lower():\n",
    "        replace_key = \"Market Cap\"\n",
    "    elif re.match(r\"Assets Under Management\", string, re.IGNORECASE):\n",
    "        replace_key = \"Assets Under Management\"\n",
    "    \n",
    "    return replace_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['samco flexi cap fund',\n",
       " 'samco elss tax saver fund',\n",
       " 'samco active momentum fund',\n",
       " 'samco dynamic asset allocation fund',\n",
       " 'samco overnight fund']"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fund_name = [txt for txt in pagedf.title.to_list() if txt !=\"\"]\n",
    "fund_unique_name = []\n",
    "for fund in fund_name:\n",
    "    if fund not in fund_unique_name:\n",
    "        fund_unique_name.append(fund)\n",
    "fund_unique_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samco flexi cap fund\n",
      "PDF successfully generated to: C:\\Users\\Kaustubh.keny\\OneDrive - Cogencis Information Services Ltd\\Documents\\mywork-repo\\output\\samcoDryRun1.pdf\n",
      "samco elss tax saver fund\n",
      "PDF successfully generated to: C:\\Users\\Kaustubh.keny\\OneDrive - Cogencis Information Services Ltd\\Documents\\mywork-repo\\output\\samcoDryRun2.pdf\n",
      "samco active momentum fund\n",
      "PDF successfully generated to: C:\\Users\\Kaustubh.keny\\OneDrive - Cogencis Information Services Ltd\\Documents\\mywork-repo\\output\\samcoDryRun3.pdf\n",
      "samco dynamic asset allocation fund\n",
      "PDF successfully generated to: C:\\Users\\Kaustubh.keny\\OneDrive - Cogencis Information Services Ltd\\Documents\\mywork-repo\\output\\samcoDryRun4.pdf\n",
      "samco overnight fund\n",
      "PDF successfully generated to: C:\\Users\\Kaustubh.keny\\OneDrive - Cogencis Information Services Ltd\\Documents\\mywork-repo\\output\\samcoDryRun5.pdf\n"
     ]
    }
   ],
   "source": [
    "header_font = 12\n",
    "content_font = 8\n",
    "page_counter = 0\n",
    "\n",
    "grand_final_data = {}\n",
    "matrix_final = {}\n",
    "\n",
    "for fund,page in zip(fund_unique_name,clean_data):\n",
    "    \n",
    "    result, matrix  = create_matrix_structure(page,header_font, content_font)\n",
    "    page_counter +=1\n",
    "    #GENERATE PDF INSTSANCE FOR EACH PAGE CONTENT\n",
    "    print(fund)\n",
    "    file_path = path + r\"\\output\\samcoDryRun\"+ str(page_counter) +r\".pdf\"\n",
    "    generate_pdf_from_data(result, file_path)\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        final_data = []\n",
    "        final_data_generated = {}\n",
    "        \n",
    "        for page in pdf.pages:\n",
    "            # extract text from the page\n",
    "            text = page.extract_text()\n",
    "            final_data.append(text)\n",
    "        \n",
    "        #store them in a dict for each page\n",
    "        for data in final_data:\n",
    "            content = data.split('\\n')\n",
    "            main_key = replace_main_key(content[0])\n",
    "            values = content[1:]\n",
    "        \n",
    "            final_data_generated[main_key] = values\n",
    "\n",
    "        #sort the headers in lex order\n",
    "        sorted_final_generated = {key: final_data_generated[key] for key in sorted(final_data_generated)}\n",
    "            \n",
    "    \n",
    "    master_key = fund\n",
    "    matrix_final[master_key] = matrix\n",
    "    grand_final_data[master_key] = sorted_final_generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['samco flexi cap fund', 'samco elss tax saver fund', 'samco active momentum fund', 'samco dynamic asset allocation fund', 'samco overnight fund'])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grand_final_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_indices = set()\n",
    "for k, value in grand_final_data.items():\n",
    "    for indices in value.keys():\n",
    "        imp_indices.add(indices)\n",
    "        \n",
    "if \"Market Cap\" not in imp_indices:\n",
    "    imp_indices.add(\"Market Cap\")\n",
    "modified_indices = sorted(imp_indices)\n",
    "\n",
    "function_indices = [return_aum_data,\n",
    "        return_fund_data,\n",
    "        return_invest_data,\n",
    "        return_mar_data,\n",
    "        return_nav_data,\n",
    "        return_quant_data,\n",
    "        return_scheme_data]\n",
    "\n",
    "function_map = {}\n",
    "\n",
    "for k, v in zip(modified_indices,function_indices):\n",
    "    #print(k)\n",
    "    function_map[k] = v\n",
    "\n",
    "def perform_operation(operation, data):\n",
    "    \n",
    "    if operation in function_map.keys():\n",
    "        # Call the mapped function\n",
    "        return function_map[operation](data)\n",
    "    else:\n",
    "        return \"Invalid operation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Assets Under Management',\n",
       " 'Fund Manager',\n",
       " 'Investment Objective',\n",
       " 'Market Cap',\n",
       " 'NAV',\n",
       " 'Quantitative Data',\n",
       " 'Scheme Details']"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#function_map\n",
    "modified_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samco flexi cap fund\n",
      "Done for _________________\n",
      "samco elss tax saver fund\n",
      "Done for _________________\n",
      "samco active momentum fund\n",
      "Done for _________________\n",
      "samco dynamic asset allocation fund\n",
      "Done for _________________\n",
      "samco overnight fund\n",
      "Done for _________________\n"
     ]
    }
   ],
   "source": [
    "import pprint, json\n",
    "\n",
    "grand_dictionary = {}\n",
    "for master_k in grand_final_data.keys():\n",
    "\n",
    "    page_data = grand_final_data[master_k]\n",
    "    print(master_k)\n",
    "    page_content = list()\n",
    "    \n",
    "    # for main_k in sample.keys():\n",
    "    #     print(main_k)\n",
    "    \n",
    "    for main_k, main_content in page_data.items():\n",
    "        #print(main_k)\n",
    "\n",
    "        hello = function_map[main_k]\n",
    "        take = hello(main_k, main_content)\n",
    "        page_content.append(take)\n",
    "    print(\"Done for _________________\")    \n",
    "    grand_dictionary[master_k] = page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON CREATED\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(path +r\"\\output\\dump58_29-Feb-24_FS.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(grand_dictionary, file, ensure_ascii=False, indent=4)\n",
    "    print(\"JSON CREATED\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
