{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import fitz\n",
    "import camelot\n",
    "import warnings , math, collections , os, re\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\Kaustubh.keny\\OneDrive - Cogencis Information Services Ltd\\Documents\\mywork-repo\"\n",
    "#path = r\"C:\\Users\\rando\\OneDrive\\Documents\\mywork-repo\"\n",
    "\n",
    "#file data paths\n",
    "samco_path = path + r\"\\files\\SamcoFactSheet2024.pdf\"\n",
    "\n",
    "#dry run paths\n",
    "dry_run_path = path + r\"\\output\\DryRun.pdf\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAMCO PATHS\n",
    "#samco output path\n",
    "no_image_path = path + r\"\\output\\sam\\NoImgPdf.pdf\"\n",
    "textual_pdf_path = path + r\"\\output\\sam\\TextualPdf.pdf\"\n",
    "tabular_pdf_path = path + r\"\\output\\sam\\TabularPdf.pdf\"\n",
    "\n",
    "#pickle data paths samco\n",
    "pickle_text = r\"\\output\\pkl\\sam\\textual_data.pkl\"\n",
    "pickle_tab = r\"\\output\\pkl\\sam\\tabular_data.pkl\"\n",
    "pickle_nonimg = r\"\\output\\pkl\\sam\\nonimg_data.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = [3,5,7,9,11,13,15]\n",
    "bbox = [(31,15,575,115),(35,120,250,765)] #for header and other for content\n",
    "\n",
    "\n",
    "def get_data_clipped(input:str, output:str, pageSelect:list, bbox:list[set]):\n",
    "    \n",
    "    document = fitz.open(input)\n",
    "    finalData = []\n",
    "    \n",
    "    for pgn, pages in enumerate(document):\n",
    "        if pgn in pageSelect:\n",
    "            page = document[pgn]\n",
    "            \n",
    "            title_blocks = page.get_text('dict', clip = bbox[0])['blocks'] #get title\n",
    "            blocks = page.get_text('dict', clip = bbox[1])['blocks'] #get all blocks\n",
    "            filtered_blocks = [block for block in title_blocks if block['type']==0] + [block for block in blocks if block['type']==0]\n",
    "            sorted_blocks = sorted(filtered_blocks, key= lambda x: (x['bbox'][1], x['bbox'][0]))\n",
    "            finalData.append({\n",
    "                \"page\": pgn,\n",
    "                \"block\": sorted_blocks\n",
    "            })\n",
    "            \n",
    "    return finalData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data_clipped(samco_path, dry_run_path, pages, bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[2]['block']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for block in data[2]['block']:\n",
    "    if 'lines' in block:\n",
    "        for lines in block['lines']:\n",
    "            for span in lines['spans']:\n",
    "                print(span['size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_text_data = []\n",
    "for page in data:\n",
    "    page_content = []\n",
    "    for blocks in page['block']:\n",
    "        if 'lines' in blocks:\n",
    "            for line in blocks['lines']:\n",
    "                spans = line.get('spans',[])\n",
    "                text = \"\".join(span['text'] for span in spans)\n",
    "                page_content.append(text)\n",
    "    \n",
    "    final_text_data.append(page_content)           \n",
    "    \n",
    "            \n",
    "final_text_data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Recursively remove specific keys from nested dictionaries or lists.\n",
    "    Args:data (dict | list): The input data (nested structure of dictionaries and lists).\n",
    "    keys_to_remove (set): Keys to be removed from the dictionaries.\n",
    "    Returns:dict | list: Data with specified keys removed.\"\"\"\n",
    "def remove_keys(data:list, keys_to_remove:set):\n",
    "    if isinstance(data, list):\n",
    "        # Process each element in the list\n",
    "        return [remove_keys(item, keys_to_remove) for item in data]\n",
    "    elif isinstance(data, dict):\n",
    "        # Process each key-value pair in the dictionary\n",
    "        return {key: remove_keys(value, keys_to_remove) for key, value in data.items() if key not in keys_to_remove}\n",
    "    else:\n",
    "        # Return data as is if it's neither a dict nor a list\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_to_remove = {}\n",
    "cleaned_data = remove_keys(data, keys_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data[3]['block']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data_combine_spans(data:list, check_color:int,replace_size):\n",
    "    \n",
    "    #remove redundant text\n",
    "    texts_to_remove = [':','st',\";\",\"-\",'st ',' ']\n",
    "    for cleaned_data in data:\n",
    "        for block in cleaned_data.get(\"block\", []):\n",
    "                for line in block.get(\"lines\", []):\n",
    "                    # Filter out spans whose text matches any of the texts to remove\n",
    "                    line[\"spans\"] = [\n",
    "                        span for span in line.get(\"spans\", [])\n",
    "                        if span.get(\"text\").strip() not in texts_to_remove\n",
    "                    ] \n",
    "                    \n",
    "    #make size of text with same color equal to size\n",
    "    for cleaned_data in data:\n",
    "        for block in cleaned_data.get(\"block\", []):\n",
    "                for line in block.get(\"lines\", []):\n",
    "                    for span in line.get('spans',[]):\n",
    "                        if span['color'] == check_color and span['size']> 7:\n",
    "                            span['size'] = replace_size              \n",
    "                        \n",
    "    #combine spans with same font, flag,size,color etc\n",
    "    for cleaned_data in data:\n",
    "        for block in cleaned_data.get(\"block\", []):\n",
    "            for line in block.get(\"lines\", []):\n",
    "                combined_spans = []\n",
    "                for span in line.get(\"spans\", []):\n",
    "                    if combined_spans and all(\n",
    "                        combined_spans[-1].get(key) == span.get(key)\n",
    "                        for key in [\"flags\", \"size\", \"color\"]\n",
    "                    ):\n",
    "                        # Combine text if spans are similar\n",
    "                        combined_spans[-1][\"text\"] += \" \" + span[\"text\"]\n",
    "                    else:\n",
    "                        combined_spans.append(span)\n",
    "                line[\"spans\"] = combined_spans\n",
    "    \n",
    "    return data\n",
    "\n",
    "combined_data = clean_data_combine_spans(cleaned_data, -1, 9.0) #set font whose color is __ to size __ \n",
    "#Bad Logic here btw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data[3]['block'] #Page 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_matrix_structure(data:list, title_font:float, subheader_font:float, content_max_font:float):\n",
    "    # Step 1: Extract all unique coordinates\n",
    "    coordinates = []\n",
    "    fonts = set()\n",
    "\n",
    "    for block in data['block']:\n",
    "        for line in block['lines']:\n",
    "            for span in line['spans']:\n",
    "                origin = tuple(span['origin'])  # Extract (x, y) coordinates from origin\n",
    "                coordinates.append(origin)\n",
    "                fonts.add(span['size'])\n",
    "\n",
    "    coordinates = sorted(set(coordinates), key=lambda c: (c[1], c[0]))  # Sort by y, then x\n",
    "    fonts = sorted(fonts)\n",
    "\n",
    "    # Step 2: Initialize the matrix\n",
    "    coord_to_index = {coord: idx for idx, coord in enumerate(coordinates)}\n",
    "    font_to_index = {font: idx for idx, font in enumerate(fonts)}\n",
    "    matrix = np.zeros((len(coordinates), len(fonts)), dtype=object)\n",
    "\n",
    "    # Step 3: Fill the matrix\n",
    "    for block in data['block']:\n",
    "        for line in block['lines']:\n",
    "            for span in line['spans']:\n",
    "                origin = tuple(span['origin'])  # Get (x, y) coordinates from origin\n",
    "                font = span['size']\n",
    "                if origin in coord_to_index and font in font_to_index:\n",
    "                    row = coord_to_index[origin]\n",
    "                    col = font_to_index[font]\n",
    "                    if matrix[row, col] == 0:\n",
    "                        matrix[row, col] = []\n",
    "                    matrix[row, col].append(span)  # Append the entire span dictionary\n",
    "\n",
    "    # Step 4: Generate the nested dictionary\n",
    "    nested_dict = {}\n",
    "    current_title = None\n",
    "    current_subheader = None\n",
    "\n",
    "    for row_idx, coord in enumerate(coordinates):\n",
    "        for col_idx, font in enumerate(fonts):\n",
    "            if matrix[row_idx, col_idx] != 0:\n",
    "                spans = matrix[row_idx, col_idx]\n",
    "\n",
    "                for span in spans:\n",
    "                    if font == title_font:\n",
    "                        current_title = span\n",
    "                        nested_dict[current_title['text']] = {}\n",
    "                    elif font == subheader_font and current_title:\n",
    "                        current_subheader = span\n",
    "                        nested_dict[current_title['text']][current_subheader['text']] = []\n",
    "                    elif font <= content_max_font and current_subheader:\n",
    "                        nested_dict[current_title['text']][current_subheader['text']].append(span)\n",
    "\n",
    "    return nested_dict\n",
    "\n",
    "def generate_pdf_from_data(data:list, output_path:str):\n",
    "    \"\"\"\n",
    "    Generates a PDF from the nested dictionary data structure.\n",
    "\n",
    "    Parameters:\n",
    "        data (dict): The nested dictionary containing sections and fitz spans.\n",
    "        output_path (str): The file path where the PDF will be saved.\n",
    "    \"\"\"\n",
    "    # Create a new PDF document\n",
    "    pdf_document = fitz.open()\n",
    "    \n",
    "    for section, spans in data.items():\n",
    "        # Add a new page for each section\n",
    "        page = pdf_document.new_page()\n",
    "        text_position = 72  # Initial vertical position (used only for section titles)\n",
    "\n",
    "        # Add section title\n",
    "        title_font_size = 14\n",
    "        page.insert_text(\n",
    "            (72, text_position),\n",
    "            section,\n",
    "            fontsize=title_font_size,\n",
    "            fontname=\"helv\",\n",
    "            color=(0, 0, 1),\n",
    "        )\n",
    "\n",
    "        # Iterate through each span in the section\n",
    "        for span in spans:\n",
    "            bbox = span.get(\"bbox\", [0, 0, 0, 0])  # Use bbox for exact placement\n",
    "\n",
    "            # Error handling for font issues\n",
    "            try:\n",
    "                page.insert_text(\n",
    "                    (bbox[0], bbox[1]),  # Use bbox coordinates for exact placement\n",
    "                    span[\"text\"],\n",
    "                    fontsize=span[\"size\"],\n",
    "                    fontname=\"helv\",\n",
    "                    color=tuple(int(span[\"color\"] & 0xFFFFFF) for _ in range(3)),  # Convert span color\n",
    "                )\n",
    "            except Exception:\n",
    "                page.insert_text(\n",
    "                    (bbox[0], bbox[1]),  # Use bbox coordinates for exact placement\n",
    "                    span[\"text\"],\n",
    "                    fontsize=span[\"size\"],\n",
    "                    fontname=\"helv\",\n",
    "                    color=(1, 0, 0),  # Fallback color for errors\n",
    "                )\n",
    "\n",
    "    # Save the created PDF\n",
    "    pdf_document.save(output_path)\n",
    "    pdf_document.close()\n",
    "    print(f\"PDF successfully generated and saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_font = 24.0  # Example title font size\n",
    "subheader_font = 9.0  # Example subheader font size\n",
    "content_max_font = 8.0  # Example maximum content font size\n",
    "\n",
    "grand_final_data = {}\n",
    "\n",
    "for page_data in combined_data: #each page data in doc  \n",
    "    result = create_matrix_structure(page_data, title_font, subheader_font, content_max_font)\n",
    "    master_key = list(result.keys()) #fund name\n",
    "    content = result[master_key[0]] #content of each fund\n",
    "    \n",
    "    \n",
    "    generate_pdf_from_data(content, path + r\"\\output\\samcoDryRun.pdf\")\n",
    "    with pdfplumber.open(path +r\"\\output\\samcoDryRun.pdf\") as pdf:\n",
    "        final_data = []\n",
    "        final_data_generated = {}\n",
    "        \n",
    "        for page in pdf.pages:\n",
    "            # extract text from the page\n",
    "            text = page.extract_text()\n",
    "            final_data.append(text)\n",
    "        \n",
    "        #store them in a dict for each page\n",
    "        for data in final_data:\n",
    "            content = data.split('\\n')\n",
    "            main_key = content[0]\n",
    "            values = content[1:]\n",
    "        \n",
    "            final_data_generated[main_key] = values\n",
    "        \n",
    "        #sort the headers in lex order\n",
    "        sorted_final_generated = {key: final_data_generated[key] for key in sorted(final_data_generated)}\n",
    "            \n",
    "    \n",
    "    grand_final_data[master_key[0]] = sorted_final_generated\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fund names\n",
    "list(grand_final_data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REGEX FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_invest_data(key:str,data:list):\n",
    "    investment_objective = data\n",
    "    values = \" \".join(txt for txt in investment_objective)\n",
    "\n",
    "    data = {\n",
    "        key:values\n",
    "    }\n",
    "\n",
    "    return data\n",
    "\n",
    "def return_scheme_data(key:str,data:list):\n",
    "    scheme_data = data\n",
    "    main_key = key\n",
    "    structured_data = {main_key: {}}\n",
    "\n",
    "    # Patterns\n",
    "    date_pattern = r\"^(.*?date)\\s(\\d{2}-[A-Za-z]{3}-\\d{4})$\"\n",
    "    benchmark_pattern = r\"^(Benchmark)\\s+(.*)$\"\n",
    "    application_pattern = r\"(?:·)?\\d+(?:,\\d{3})*(?:\\.\\d+)?/-\"\n",
    "\n",
    "    for data in scheme_data:\n",
    "        if re.search(date_pattern, data, re.IGNORECASE):\n",
    "            match = re.match(date_pattern, data, re.IGNORECASE)\n",
    "            if match:\n",
    "                key = match.group(1)\n",
    "                value = match.group(2)\n",
    "                structured_data[main_key][key] = value\n",
    "        elif re.search(benchmark_pattern, data, re.IGNORECASE):\n",
    "            match = re.match(benchmark_pattern, data, re.IGNORECASE)\n",
    "            if match:\n",
    "                key = match.group(1)\n",
    "                value = match.group(2)\n",
    "                structured_data[main_key][key] = value\n",
    "        elif re.search(r\"\\b(min|application)\\b\", data, re.IGNORECASE):\n",
    "            matches = re.findall(application_pattern, data, re.IGNORECASE)\n",
    "            if matches:\n",
    "                cleaned_matches = [match.replace('·', '') for match in matches]\n",
    "                structured_data[main_key][\"min_appl_amt\"] = cleaned_matches\n",
    "        elif re.search(r\"\\b(additional.* and in multiples of)\\b\", data, re.IGNORECASE):\n",
    "            matches = re.findall(application_pattern, data, re.IGNORECASE)\n",
    "            if matches:\n",
    "                cleaned_matches = [match.replace('·', '') for match in matches]\n",
    "                structured_data[main_key][\"additional_amt\"] = cleaned_matches\n",
    "\n",
    "    return structured_data\n",
    "\n",
    "def return_fund_data(key:str,data:list):\n",
    "    fund_manager = data\n",
    "    main_key = key\n",
    "    strucuted_data = {main_key:[]}\n",
    "    current_entry = None\n",
    "    name_pattern = r'^(Ms\\.|Mr\\.)'\n",
    "    manage_pattern = r'^\\(|\\)$'\n",
    "    date_pattern = r'\\b\\w+ \\d{1,2}, \\d{4}\\b'\n",
    "    experience_pattern = r'^Total Experience: (.+)$'\n",
    "\n",
    "    for data in fund_manager:\n",
    "        if re.match(name_pattern,data):\n",
    "            if current_entry:\n",
    "                strucuted_data[main_key].append(current_entry)\n",
    "            current_entry = {\n",
    "                'name': data.split(\",\")[0].strip().lower(),\n",
    "                'designation': \"\".join(data.split(\",\")[1:]).strip().lower()\n",
    "            }\n",
    "            #print(data.split(\",\")[0],\"\".join(data.split(\",\")[1:]))\n",
    "        elif re.match(manage_pattern,data):\n",
    "            if \"inception\" in data.lower():\n",
    "                current_entry['managing_since'] = 'inception'\n",
    "            else:\n",
    "                date = re.search(date_pattern, data)\n",
    "                current_entry['managing_since'] = date.group() if date != None else None\n",
    "        elif re.match(experience_pattern,data):\n",
    "            current_entry['total_experience'] = data.split(\":\")[1].strip().lower()\n",
    "            #print(data.split(\":\")[1])\n",
    "\n",
    "        \n",
    "    if current_entry:  # Append the last entry\n",
    "        strucuted_data[main_key].append(current_entry)\n",
    "            \n",
    "    return strucuted_data\n",
    "\n",
    "def return_nav_data(key:str,data:list):\n",
    "    main_key = key\n",
    "    structured_data = {main_key: {}}\n",
    "    \n",
    "    growth_pattern = r\"([\\w\\s]+):\\s*·([\\d.]+)\"\n",
    "    \n",
    "    for line in data:\n",
    "        matches = re.findall(growth_pattern, line)\n",
    "        for key, value in matches:\n",
    "            structured_data[main_key][key.strip().lower()] = value\n",
    "        \n",
    "    return structured_data\n",
    "\n",
    "def return_quant_data(key:str,data:list):\n",
    "    qunatitative_data = data\n",
    "    main_key = key\n",
    "\n",
    "    strucuted_data = {main_key:{}}\n",
    "    current_entry = None\n",
    "    comment = \"\"\n",
    "\n",
    "    ratio_pattern = r\"\\b(ratio|turnover)\\b\"\n",
    "    annual_pattern = r'\\b(annualised|YTM)\\b'\n",
    "    macaulay_pattern = r\"\\b(macaulay.*duration)\\b\"\n",
    "    residual_pattern = r\"\\b(residual.*maturity)\\b\"\n",
    "    modified_pattern = r\"\\b(modified.*duration)\\b\"\n",
    "\n",
    "    for data in qunatitative_data:\n",
    "        if re.search(ratio_pattern,data, re.IGNORECASE):\n",
    "            key = data.split(\":\")[0].lower().strip()\n",
    "            value = data.split(\":\")[1].lower().strip()\n",
    "        elif re.search(annual_pattern,data, re.IGNORECASE):\n",
    "            key = data.split(\":\")[0].lower().strip()\n",
    "            value = data.split(\":\")[1].lower().strip()\n",
    "        elif re.search(macaulay_pattern,data, re.IGNORECASE):\n",
    "            key = data.split(\":\")[0].lower().strip()\n",
    "            value = data.split(\":\")[1].lower().strip()\n",
    "        elif re.search(residual_pattern,data, re.IGNORECASE):\n",
    "            key = data.split(\":\")[0].lower().strip()\n",
    "            value = data.split(\":\")[1].lower().strip()\n",
    "        elif re.search(modified_pattern,data, re.IGNORECASE):\n",
    "            key = data.split(\":\")[0].lower().strip()\n",
    "            value = data.split(\":\")[1].lower().strip()\n",
    "        else:\n",
    "            comment+= data\n",
    "        strucuted_data[main_key][key] = value\n",
    "    \n",
    "    strucuted_data[main_key]['comment'] = comment\n",
    "\n",
    "    return strucuted_data\n",
    "\n",
    "def return_aum_data(key:str,data:list):\n",
    "    \n",
    "    aum = data\n",
    "    main_key = key\n",
    "    strucuted_data = {main_key:{}}\n",
    "\n",
    "    pattern = r\"\\b\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)? Crs\\b\"\n",
    "\n",
    "    for data in aum:\n",
    "        if re.search(r'average', data, re.IGNORECASE):\n",
    "            match = re.search(pattern, data)\n",
    "            key = 'avg_aum'\n",
    "        else:\n",
    "            match = re.search(pattern, data)\n",
    "            key = \"aum\"\n",
    "        \n",
    "        strucuted_data[main_key][key] = match.group()\n",
    "\n",
    "    return strucuted_data\n",
    "\n",
    "def return_mar_data(key:str,data:list):\n",
    "    return {\n",
    "        key: None\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = grand_final_data['Samco Active Momentum Fund']\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AUM as on October 31, 2024 · 850.06 Crs', 'Average AUM for Month of October 2024 · 852.46 Crs']\n",
      "['Mr. Paras Matalia, Fund Manager & Head - Research Equity', '(Managing this scheme since inception)', 'Total Experience: Around 9 years', 'Mr. Umeshkumar Mehta, Director, CIO & Fund Manager', '(Managing the scheme since August 01, 2023)', 'Total Experience: Over 20 years', 'Mr. Dhawal Ghanshyam Dhanani', '(Dedicated Fund Manager for Overseas investments since inception)', 'Total Experience: Around 6 years']\n",
      "['The investment objective of the Scheme is to seek to', 'generate long-term capital appreciation by investing in', 'stocks showing strong momentum. Momentum stocks are', 'such that exhibit positive price momentum · based on the', 'phenomenon that stocks which have performed well in the', 'past relative to other stocks (winners) continue to perform', 'well in the future, and stocks that have performed', 'relatively poorly (losers) continue to perform poorly.', 'However, there can be no assurance or guarantee that the', 'investment objective of the scheme would be achieved.']\n",
      "['Regular Growth · 14.53', 'Direct Growth · 14.81']\n",
      "['Portfolio Turnover Ratio: 5.11 times', 'Lower of sales or purchases divided by average AUM for last rolling 12 months']\n",
      "['Inception Date 05-Jul-2023', '(Date of Allotment)', 'Benchmark Nifty 500 TRI', 'Min.Application ·5000/- and in multiples of ·1/-', 'Amount thereafter', 'Additional ·500/- and in multiples of ·1/- thereafter', 'Purchase', 'Entry Load NIL', 'Exit Load 1.00% If the investment is redeemed', 'or switched out on or before 365 days', 'from the date of allotment of units.', 'No Exit Load will be charged if', 'investment is redeemed or switched', 'out after 365 days from the date of', 'allotment of units.', '(With effect from October 03, 2024)', 'Total Expense Regular Plan Direct Plan', 'Ratio (TER)', '2.26% 0.86%', 'as on October 31,', '2024 Including Goods and Service Tax on', 'Management Fees.']\n"
     ]
    }
   ],
   "source": [
    "print(sample['Assets Under Management (AUM)'])\n",
    "print(sample['Fund Manager'])\n",
    "print(sample['Investment Objective'])\n",
    "print(sample['NAV as on 31 October 2024 (· per unit)'])\n",
    "print(sample['Quantitative Data'])\n",
    "print(sample['Scheme Details'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Assets Under Management (AUM)': {'aum': '850.06 Crs',\n",
       "  'avg_aum': '852.46 Crs'}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_aum_data(\"Assets Under Management (AUM)\",sample['Assets Under Management (AUM)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Fund Manager': [{'name': 'mr. paras matalia',\n",
       "   'designation': 'fund manager & head - research equity',\n",
       "   'managing_since': 'inception',\n",
       "   'total_experience': 'around 9 years'},\n",
       "  {'name': 'mr. umeshkumar mehta',\n",
       "   'designation': 'director cio & fund manager',\n",
       "   'managing_since': 'August 01, 2023',\n",
       "   'total_experience': 'over 20 years'},\n",
       "  {'name': 'mr. dhawal ghanshyam dhanani',\n",
       "   'designation': '',\n",
       "   'managing_since': 'inception',\n",
       "   'total_experience': 'around 6 years'}]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_fund_data(\"Fund Manager\",sample['Fund Manager'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_indices = set()\n",
    "for k, value in grand_final_data.items():\n",
    "    for indices in value.keys():\n",
    "        imp_indices.add(indices)\n",
    "        \n",
    "imp_indices = sorted(imp_indices)\n",
    "\n",
    "function_indices = [return_aum_data,\n",
    "        return_fund_data,\n",
    "        return_invest_data,\n",
    "        return_mar_data,\n",
    "        return_nav_data,\n",
    "        return_quant_data,\n",
    "        return_scheme_data]\n",
    "\n",
    "function_map = {}\n",
    "\n",
    "for k, v in zip(imp_indices,function_indices):\n",
    "    function_map[k] = v\n",
    "\n",
    "#print(function_map)\n",
    "\n",
    "def perform_operation(operation, data):\n",
    "    \n",
    "    if operation in function_map.keys():\n",
    "        # Call the mapped function\n",
    "        return function_map[operation](data)\n",
    "    else:\n",
    "        return \"Invalid operation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint, json\n",
    "\n",
    "grand_dictionary = {}\n",
    "for master_k in grand_final_data.keys():\n",
    "\n",
    "    sample = grand_final_data[master_k]\n",
    "    \n",
    "    page_content = list()\n",
    "    \n",
    "    for main_k, main_content in sample.items():\n",
    "        \n",
    "        hello = function_map[main_k]\n",
    "        page_content.append(hello(main_k, main_content))\n",
    "    \n",
    "    grand_dictionary[master_k] = page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path +r\"\\output\\dump.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(grand_dictionary, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
