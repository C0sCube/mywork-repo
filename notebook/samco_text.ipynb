{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import fitz\n",
    "import warnings , math, collections , os, re, pprint, json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) \n",
    "\n",
    "\n",
    "#path = r\"C:\\Users\\Kaustubh.keny\\OneDrive - Cogencis Information Services Ltd\\Documents\\mywork-repo\"\n",
    "path = r\"C:\\Users\\rando\\OneDrive\\Documents\\mywork-repo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "samco_path = path + r\"\\files\\58_31-Dec-24_FS.pdf\"\n",
    "dry_path  = path + r\"\\output\\DryRun.pdf\"\n",
    "indice_path = path + r\"\\output\\pkl\\indices_var.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_financial_indices(path:str):\n",
    "    final_indices = set()\n",
    "    with open(path , 'rb') as file:\n",
    "        indices = pickle.load(file)  \n",
    "        for k,v in indices.items():\n",
    "            temp = [k] + v\n",
    "            for t in temp:\n",
    "                final_indices.add(t)\n",
    "    \n",
    "    return final_indices\n",
    "\n",
    "\n",
    "\"\"\" Highlights important financial indices in the pdf, does other pre\n",
    "analysis of data.\n",
    "Args: list of indices, string of pdf path\n",
    "Returns: dict of pages highlighted, string of output pdf, dict of pages contaiting FUND NAMES\n",
    "\"\"\"\n",
    "def check_indice_highlight(path:str, indices_variations:list, fund_pattern:str):\n",
    "    doc = fitz.open(path)\n",
    "    page_count = doc.page_count #No of pages\n",
    "    \n",
    "    pages = [i for i in range(page_count)]\n",
    "    important_pages = dict.fromkeys(pages, 0)\n",
    "    fund_titles = dict.fromkeys(pages, \"\")\n",
    "\n",
    "\n",
    "    for page_num, page in enumerate(doc):\n",
    "        \n",
    "        text_instances = page.get_text('dict')[\"blocks\"]\n",
    "        \n",
    "        #sort for all data in pdf document \n",
    "        sorted_text_instances = sorted(text_instances, key=lambda x: (x['bbox'][1], x['bbox'][0]))\n",
    "        \n",
    "        # rect = fitz.Rect((35,120,250,765))\n",
    "        # page.add_highlight_annot(rect)\n",
    "\n",
    "        for pgn,block in enumerate(sorted_text_instances):     \n",
    "            if \"lines\" not in block:\n",
    "                continue\n",
    "            \n",
    "            for line in block[\"lines\"]: \n",
    "                for span in line[\"spans\"]:\n",
    "                    if span['flags'] in [20,25]:  # learn flag logic , rn set for all flags value\n",
    "                        span_text = span['text'].strip().lower()\n",
    "                        \n",
    "                        \n",
    "                        #FUND PAGE CHECK\n",
    "                        conditions = [\n",
    "                            pgn in range(0,6),\n",
    "                            re.match(fund_pattern, span_text, re.IGNORECASE),\n",
    "                            span['size'] >18\n",
    "                        ]\n",
    "                        if all(conditions):\n",
    "        \n",
    "                            fund_titles[page_num] = span_text  \n",
    "                        \n",
    "                        #CHECK IMP FINANCE INDICES  \n",
    "                        for term in indices_variations:  \n",
    "                            pattern = r'\\b' + re.escape(term.lower()) + r'\\b'\n",
    "                            if re.search(pattern, span_text):\n",
    "\n",
    "                                #count highlights\n",
    "                                important_pages[page_num] +=1\n",
    "                                #mark content\n",
    "                                rect = fitz.Rect(span['bbox']) \n",
    "                                page.add_highlight_annot(rect)\n",
    "                                break  #optional , one highlight\n",
    "\n",
    "    \n",
    "    output_path = None\n",
    "    if any(important_pages.values()):\n",
    "        output_path = path.replace('.pdf', '_highlighted.pdf')\n",
    "        doc.save(output_path)\n",
    "\n",
    "    doc.close()\n",
    "    return important_pages, output_path, fund_titles\n",
    "\n",
    "\n",
    "\"\"\" Get the clipped data in the bbox provided and store in nested dict\n",
    "Args: input path, dryrun path, important pages, bbox coords\n",
    "Returns: dict { 'page' : int 'block': dict}\"\"\"\n",
    "def get_clipped_data(input:str, output:str, pageSelect:list, bbox:list[set], fund_names:dict):\n",
    "    \n",
    "    document = fitz.open(input)\n",
    "    finalData = []\n",
    "    \n",
    "    for pgn in pageSelect:\n",
    "        #get the page\n",
    "        page = document[pgn]\n",
    "        fundName = fund_names[pgn]\n",
    "    \n",
    "        blocks = page.get_text('dict', clip = bbox[0])['blocks'] #get all blocks\n",
    "        \n",
    "        filtered_blocks = [block for block in blocks if block['type']==0 and 'lines' in block]\n",
    "        sorted_blocks = sorted(filtered_blocks, key= lambda x: (x['bbox'][1], x['bbox'][0]))\n",
    "        \n",
    "        finalData.append({\n",
    "            \"page\": pgn,\n",
    "            \"fundname\": fundName,\n",
    "            \"block\": sorted_blocks,\n",
    "        })\n",
    "            \n",
    "    return finalData\n",
    "\n",
    "def clean_data_combine(data: list, check_color: int, replace_size):\n",
    "    \n",
    "    remove_text = ['.','. ',',',':','st',\";\",\"-\",'st ',' ','th', 'th ', 'rd', 'rd ', 'nd', 'nd ']\n",
    "    \n",
    "    for page in data:\n",
    "        for blocks in page['block']:\n",
    "            for lines in  blocks.get('lines',[]):\n",
    "            \n",
    "                #REMOVE WASTE VALUES\n",
    "                lines['spans'] = [\n",
    "                    span for span in lines.get(\"spans\", [])\n",
    "                            if span.get(\"text\").strip() not in remove_text\n",
    "                ]\n",
    "                \n",
    "                #MAKE SAME COLOR AND SIZE\n",
    "                for span in lines.get('spans',[]):\n",
    "                    if span['color'] == check_color and span['size'] > 7:\n",
    "                        span['size'] = replace_size\n",
    "                \n",
    "                \n",
    "                #COMBINE SPANS HAVING SIMILAR PROP\n",
    "                combined_spans = []\n",
    "                for span in lines.get(\"spans\", []):\n",
    "                    if combined_spans and all(\n",
    "                        combined_spans[-1].get(key) == span.get(key)\n",
    "                        for key in [\"flags\", \"size\", \"color\"]\n",
    "                    ):\n",
    "                        # Combine text if spans are similar\n",
    "                        combined_spans[-1][\"text\"] += \" \" + span[\"text\"]\n",
    "                    else:\n",
    "                        combined_spans.append(span)\n",
    "                lines[\"spans\"] = combined_spans\n",
    "                \n",
    "    return data\n",
    "\n",
    "def return_span_data(data:list, name:str): #all\n",
    "    final_data = dict()\n",
    "    for pgn,page in enumerate(data):\n",
    "        pgn_content = []\n",
    "        for blocks in page['block']:\n",
    "            for line in blocks['lines']:\n",
    "                spans = line.get('spans',[])\n",
    "                for span in spans:\n",
    "                    pgn_content.append(span[name])\n",
    "                \n",
    "        final_data[f\"Page: {pgn + 1}\"] = pgn_content\n",
    "    \n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  title  highlight_count\n",
      "0                                                      0\n",
      "1                                                     11\n",
      "2                                                      0\n",
      "3            samco active momentum fund                9\n",
      "4            samco active momentum fund                1\n",
      "5   samco dynamic asset allocation fund               12\n",
      "6   samco dynamic asset allocation fund                1\n",
      "7                  samco flexi cap fund                9\n",
      "8                  samco flexi cap fund                1\n",
      "9                  samco multi cap fund                8\n",
      "10                 samco multi cap fund                1\n",
      "11     samco special opportunities fund                8\n",
      "12     samco special opportunities fund                1\n",
      "13            samco elss tax saver fund                8\n",
      "14            samco elss tax saver fund                1\n",
      "15    samco multi asset allocation fund               11\n",
      "16    samco multi asset allocation fund                1\n",
      "17                 samco overnight fund               11\n",
      "18                 samco arbitrage fund                9\n",
      "19                                                    12\n",
      "20                                                    12\n",
      "21                                                    12\n",
      "22                                                    12\n",
      "23                                                    12\n",
      "24                                                     4\n",
      "25                                                     0\n",
      "26                                                     0\n"
     ]
    }
   ],
   "source": [
    "file_path = samco_path\n",
    "final_indices = get_financial_indices(indice_path)\n",
    "fund_pattern = r\"^(samco|tata).*fund$\"\n",
    "highlight_pages, saved_path, fund_pages =  check_indice_highlight(file_path, final_indices, fund_pattern)\n",
    "\n",
    "\n",
    "pagedf = pd.DataFrame({'title': fund_pages.values(),'highlight_count': highlight_pages.values()})\n",
    "\n",
    "\"\"\"_summary_ fund is located only on certain pages, based on no. of \n",
    "highlights we know which pages are imp. automate this content later\n",
    "\"\"\"\n",
    "print(pagedf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = [3,5,7,9,11,13,15,17,18]\n",
    "bbox = [(35,120,250,765)]\n",
    "\n",
    "data = get_clipped_data(samco_path, dry_path, pages, bbox, fund_pages)\n",
    "#text_data = return_span_data(data, 'text')\n",
    "#data[2]['block']\n",
    "clean_data = clean_data_combine(data, -1,12.0)\n",
    "#set font whose color is __ to size __ \n",
    "#Bad Logic here btw\n",
    "#Page 0,1,2,3 indexes-> 3,5,7,9 pages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_matrix_structure(data: list, header_font: float, content_font: float):\n",
    "    # Step 1: collect font sizes and coordinates\n",
    "    coordinates = []\n",
    "    fonts = set()\n",
    "\n",
    "    for block in data['block']:\n",
    "        for line in block['lines']:\n",
    "            for span in line['spans']:\n",
    "                origin = tuple(span['origin'])  # Top-left coordinates\n",
    "                coordinates.append(origin)\n",
    "                fonts.add(span['size'])\n",
    "\n",
    "    coordinates = sorted(set(coordinates), key=lambda c: (c[1], c[0]))  # Sort by y, then x\n",
    "    fonts = sorted(fonts, reverse=True)  # Descending order of font size\n",
    "\n",
    "    # Step 2: create the matrix\n",
    "    coord_to_index = {coord: idx for idx, coord in enumerate(coordinates)}  # (x,y) at pos 0 etc. ROWS\n",
    "    font_to_index = {font: idx for idx, font in enumerate(fonts)}  # COLUMNS\n",
    "    matrix = np.zeros((len(coordinates), len(fonts)), dtype=object)  # Set all to zeros initially\n",
    "\n",
    "    # Step 3: matrix populate and nested dict add\n",
    "    nested_dict = {}\n",
    "    current_subheader = None\n",
    "\n",
    "    for block in data['block']:\n",
    "        for line in block['lines']:\n",
    "            for span in line['spans']:\n",
    "                origin = tuple(span['origin'])  # Top-left x,y\n",
    "                font = span['size']\n",
    "                text_preview = span['text']  # Get the first two words of the text\n",
    "\n",
    "                # Populate the matrix with text preview\n",
    "                if origin in coord_to_index and font in font_to_index:\n",
    "                    row = coord_to_index[origin]\n",
    "                    col = font_to_index[font]\n",
    "                    if matrix[row, col] == 0:\n",
    "                        matrix[row, col] = \"na\"\n",
    "                    matrix[row, col] = text_preview\n",
    "\n",
    "                # Build the nested dictionary\n",
    "                if font == header_font:\n",
    "                    current_subheader = span\n",
    "                    nested_dict[current_subheader['text']] = []\n",
    "                elif font <= content_font and current_subheader:\n",
    "                    nested_dict[current_subheader['text']].append(span)\n",
    "\n",
    "    matrix_df = pd.DataFrame(matrix, index=coordinates, columns=fonts)\n",
    "\n",
    "    return nested_dict, matrix_df\n",
    "def generate_pdf_from_data(data:list, output_path:str):\n",
    "    pdf_doc = fitz.open()\n",
    "    \n",
    "    for section, spans in data.items():\n",
    "    \n",
    "        page = pdf_doc.new_page()\n",
    "        text_position = 72  # for title initalize something\n",
    "\n",
    "        #section title\n",
    "        title_font_size = 14 \n",
    "        try:\n",
    "            page.insert_text(\n",
    "                (72, text_position), #initalizor\n",
    "                section,\n",
    "                fontsize=title_font_size,\n",
    "                fontname=\"helv\",\n",
    "                color=(0, 0, 1),\n",
    "            )        \n",
    "        except Exception as e:\n",
    "            print(f\"The error is {e}\")\n",
    "            \n",
    "        #content title\n",
    "        for span in spans:\n",
    "            bbox = span.get(\"bbox\", [0, 0, 0, 0])  # default \n",
    "\n",
    "            #Errror in fitz font \n",
    "            try:\n",
    "                page.insert_text(\n",
    "                    (bbox[0], bbox[1]),\n",
    "                    span[\"text\"],\n",
    "                    fontsize=span[\"size\"],\n",
    "                    fontname=\"helv\",\n",
    "                    color=tuple(int(span[\"color\"] & 0xFFFFFF) for _ in range(3)))#unsigned int value so (0,0,0)\n",
    "                \n",
    "            except Exception:\n",
    "                page.insert_text(\n",
    "                    (bbox[0], bbox[1]),\n",
    "                    span[\"text\"],\n",
    "                    fontsize=span[\"size\"],\n",
    "                    fontname=\"helv\",\n",
    "                    color=(1, 0, 0),\n",
    "                )\n",
    "\n",
    "    # Save the created PDF\n",
    "    pdf_doc.save(output_path)\n",
    "    pdf_doc.close()\n",
    "    print(f\"  PDF generated to: {output_path}\")\n",
    "\n",
    "def extract_data_from_pdf(path:str):\n",
    "    \n",
    "    def replace_main_key(string: str):\n",
    "        replace_key = string\n",
    "        if re.match(r'^NAV.*as on', string, re.IGNORECASE):\n",
    "            replace_key = \"NAV\" \n",
    "        elif \"market\" in string.lower():\n",
    "            replace_key = \"Market Cap\"\n",
    "        elif re.match(r\"Assets Under Management\", string, re.IGNORECASE):\n",
    "            replace_key = \"Assets Under Management\"   \n",
    "        return replace_key\n",
    "    \n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        final_data = []\n",
    "        final_data_generated = {}\n",
    "        \n",
    "        for page in pdf.pages:\n",
    "            # extract text from the page\n",
    "            text = page.extract_text()\n",
    "            final_data.append(text)\n",
    "        \n",
    "        #store them in a dict for each page\n",
    "        for data in final_data:\n",
    "            content = data.split('\\n')\n",
    "            main_key = replace_main_key(content[0])\n",
    "            values = content[1:]\n",
    "        \n",
    "            final_data_generated[main_key] = values\n",
    "\n",
    "        #sort the headers in lex order\n",
    "        sorted_final_generated = {key: final_data_generated[key] for key in sorted(final_data_generated)}\n",
    "\n",
    "    return sorted_final_generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samco Active Momentum Fund\n",
      "  PDF generated to: C:\\Users\\rando\\OneDrive\\Documents\\mywork-repo\\output\\SamcoDryRun1.pdf\n",
      "Samco Dynamic Asset Allocation Fund\n",
      "  PDF generated to: C:\\Users\\rando\\OneDrive\\Documents\\mywork-repo\\output\\SamcoDryRun2.pdf\n",
      "Samco Flexi Cap Fund\n",
      "  PDF generated to: C:\\Users\\rando\\OneDrive\\Documents\\mywork-repo\\output\\SamcoDryRun3.pdf\n",
      "Samco Multi Cap Fund\n",
      "  PDF generated to: C:\\Users\\rando\\OneDrive\\Documents\\mywork-repo\\output\\SamcoDryRun4.pdf\n",
      "Samco Special Opportunities Fund\n",
      "  PDF generated to: C:\\Users\\rando\\OneDrive\\Documents\\mywork-repo\\output\\SamcoDryRun5.pdf\n",
      "Samco Elss Tax Saver Fund\n",
      "  PDF generated to: C:\\Users\\rando\\OneDrive\\Documents\\mywork-repo\\output\\SamcoDryRun6.pdf\n",
      "Samco Multi Asset Allocation Fund\n",
      "  PDF generated to: C:\\Users\\rando\\OneDrive\\Documents\\mywork-repo\\output\\SamcoDryRun7.pdf\n",
      "Samco Overnight Fund\n",
      "  PDF generated to: C:\\Users\\rando\\OneDrive\\Documents\\mywork-repo\\output\\SamcoDryRun8.pdf\n",
      "Samco Arbitrage Fund\n",
      "  PDF generated to: C:\\Users\\rando\\OneDrive\\Documents\\mywork-repo\\output\\SamcoDryRun9.pdf\n"
     ]
    }
   ],
   "source": [
    "header_font = 12 #set by user\n",
    "content_font = 8 #anything below this is content\n",
    "#check clean_data content to determine this font sizes\n",
    "\n",
    "grand_final_data = {}\n",
    "matrix_final = {}\n",
    "\n",
    "for counter, page in enumerate(clean_data):\n",
    "    \n",
    "    fund_name = page['fundname'].title()\n",
    "    \n",
    "    result, matrix  = create_matrix_structure(page, header_font, content_font)\n",
    "    \n",
    "\n",
    "    file_path = os.path.join(path, 'output', f\"SamcoDryRun{counter + 1}.pdf\")\n",
    "    print(fund_name)\n",
    "    \n",
    "    #GENERATE AND EXTRACT DATA\n",
    "    generate_pdf_from_data(result, file_path)\n",
    "    page_data =  extract_data_from_pdf(file_path)\n",
    "                \n",
    "    \n",
    "    master_key = fund_name\n",
    "    matrix_final[master_key] = matrix\n",
    "    grand_final_data[master_key] = page_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REGEX FUNCTIONS\n",
    "\n",
    "def return_invest_data(key:str,data:list):\n",
    "    investment_objective = data\n",
    "    values = \" \".join(txt for txt in investment_objective)\n",
    "\n",
    "    data = {\n",
    "        key:values\n",
    "    }\n",
    "\n",
    "    return data\n",
    "\n",
    "def return_scheme_data(key:str,data:list):\n",
    "    scheme_data = data\n",
    "    main_key = key\n",
    "    structured_data = {main_key: {}}\n",
    "\n",
    "    # Patterns\n",
    "    date_pattern = r\"^(.*?date)\\s(\\d{2}-[A-Za-z]{3}-\\d{4})$\"\n",
    "    benchmark_pattern = r\"^(Benchmark)\\s+(.*)$\"\n",
    "    application_pattern = r\"(?:路)?\\d+(?:,\\d{3})*(?:\\.\\d+)?/-\"\n",
    "\n",
    "    for data in scheme_data:\n",
    "        if re.search(date_pattern, data, re.IGNORECASE):\n",
    "            match = re.match(date_pattern, data, re.IGNORECASE)\n",
    "            if match:\n",
    "                key = match.group(1)\n",
    "                value = match.group(2)\n",
    "                structured_data[main_key][key] = value\n",
    "        elif re.search(benchmark_pattern, data, re.IGNORECASE):\n",
    "            match = re.match(benchmark_pattern, data, re.IGNORECASE)\n",
    "            if match:\n",
    "                key = match.group(1)\n",
    "                value = match.group(2)\n",
    "                structured_data[main_key][key] = value\n",
    "        elif re.search(r\"\\b(min|application)\\b\", data, re.IGNORECASE):\n",
    "            matches = re.findall(application_pattern, data, re.IGNORECASE)\n",
    "            if matches:\n",
    "                cleaned_matches = [match.replace('路', '') for match in matches]\n",
    "                structured_data[main_key][\"min_appl_amt\"] = cleaned_matches\n",
    "        elif re.search(r\"\\b(additional.* and in multiples of)\\b\", data, re.IGNORECASE):\n",
    "            matches = re.findall(application_pattern, data, re.IGNORECASE)\n",
    "            if matches:\n",
    "                cleaned_matches = [match.replace('路', '') for match in matches]\n",
    "                structured_data[main_key][\"additional_amt\"] = cleaned_matches\n",
    "\n",
    "    return structured_data\n",
    "\n",
    "def return_fund_data(key:str,data:list):\n",
    "    fund_manager = data\n",
    "    main_key = key\n",
    "    strucuted_data = {main_key:[]}\n",
    "    current_entry = None\n",
    "    name_pattern = r'^(Ms\\.|Mr\\.)'\n",
    "    manage_pattern = r'^\\(|\\)$'\n",
    "    date_pattern = r'\\b\\w+ \\d{1,2}, \\d{4}\\b'\n",
    "    experience_pattern = r'^Total Experience: (.+)$'\n",
    "\n",
    "    for data in fund_manager:\n",
    "        if re.match(name_pattern,data):\n",
    "            if current_entry:\n",
    "                strucuted_data[main_key].append(current_entry)\n",
    "            current_entry = {\n",
    "                'name': data.split(\",\")[0].strip().lower(),\n",
    "                'designation': \"\".join(data.split(\",\")[1:]).strip().lower()\n",
    "            }\n",
    "            #print(data.split(\",\")[0],\"\".join(data.split(\",\")[1:]))\n",
    "        elif re.match(manage_pattern,data):\n",
    "            if \"inception\" in data.lower():\n",
    "                current_entry['managing_since'] = 'inception'\n",
    "            else:\n",
    "                date = re.search(date_pattern, data)\n",
    "                current_entry['managing_since'] = date.group() if date != None else None\n",
    "        elif re.match(experience_pattern,data):\n",
    "            current_entry['total_experience'] = data.split(\":\")[1].strip().lower()\n",
    "            #print(data.split(\":\")[1])\n",
    "\n",
    "        \n",
    "    if current_entry:  # Append the last entry\n",
    "        strucuted_data[main_key].append(current_entry)\n",
    "            \n",
    "    return strucuted_data\n",
    "\n",
    "def return_nav_data(key:str,data:list):\n",
    "    main_key = key\n",
    "    structured_data = {main_key: {}}\n",
    "    \n",
    "    growth_pattern = r\"([\\w\\s]+):\\s*路([\\d.]+)\"\n",
    "    \n",
    "    for line in data:\n",
    "        matches = re.findall(growth_pattern, line)\n",
    "        for key, value in matches:\n",
    "            structured_data[main_key][key.strip().lower()] = value\n",
    "        \n",
    "    return structured_data\n",
    "\n",
    "def return_quant_data(key:str,data:list):\n",
    "    qunatitative_data = data\n",
    "    main_key = key\n",
    "\n",
    "    strucuted_data = {main_key:{}}\n",
    "    current_entry = None\n",
    "    comment = \"\"\n",
    "\n",
    "    ratio_pattern = r\"\\b(ratio|turnover)\\b\"\n",
    "    annual_pattern = r'\\b(annualised|YTM)\\b'\n",
    "    macaulay_pattern = r\"\\b(macaulay.*duration)\\b\"\n",
    "    residual_pattern = r\"\\b(residual.*maturity)\\b\"\n",
    "    modified_pattern = r\"\\b(modified.*duration)\\b\"\n",
    "\n",
    "    for data in qunatitative_data:\n",
    "        if re.search(ratio_pattern,data, re.IGNORECASE):\n",
    "            key = data.split(\":\")[0].lower().strip()\n",
    "            value = data.split(\":\")[1].lower().strip()\n",
    "        elif re.search(annual_pattern,data, re.IGNORECASE):\n",
    "            key = data.split(\":\")[0].lower().strip()\n",
    "            value = data.split(\":\")[1].lower().strip()\n",
    "        elif re.search(macaulay_pattern,data, re.IGNORECASE):\n",
    "            key = data.split(\":\")[0].lower().strip()\n",
    "            value = data.split(\":\")[1].lower().strip()\n",
    "        elif re.search(residual_pattern,data, re.IGNORECASE):\n",
    "            key = data.split(\":\")[0].lower().strip()\n",
    "            value = data.split(\":\")[1].lower().strip()\n",
    "        elif re.search(modified_pattern,data, re.IGNORECASE):\n",
    "            key = data.split(\":\")[0].lower().strip()\n",
    "            value = data.split(\":\")[1].lower().strip()\n",
    "        else:\n",
    "            comment+= data\n",
    "        strucuted_data[main_key][key] = value\n",
    "    \n",
    "    strucuted_data[main_key]['comment'] = comment\n",
    "\n",
    "    return strucuted_data\n",
    "\n",
    "def return_aum_data(key:str,data:list):\n",
    "    \n",
    "    aum = data\n",
    "    main_key = key\n",
    "    strucuted_data = {main_key:{}}\n",
    "\n",
    "    pattern = r\"\\b\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)? Crs\\b\"\n",
    "\n",
    "    for data in aum:\n",
    "        if re.search(r'average', data, re.IGNORECASE):\n",
    "            match = re.search(pattern, data)\n",
    "            key = 'avg_aum'\n",
    "        elif re.search(pattern, data):\n",
    "            match = re.search(pattern, data)\n",
    "            key = \"aum\"\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        if match:\n",
    "            strucuted_data[main_key][key] = match.group()\n",
    "\n",
    "    return strucuted_data\n",
    "\n",
    "def return_mar_data(key:str,data:list):\n",
    "    return {\n",
    "        key: {}\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Samco Active Momentum Fund',\n",
       " 'Samco Dynamic Asset Allocation Fund',\n",
       " 'Samco Flexi Cap Fund',\n",
       " 'Samco Multi Cap Fund',\n",
       " 'Samco Special Opportunities Fund',\n",
       " 'Samco Elss Tax Saver Fund',\n",
       " 'Samco Multi Asset Allocation Fund',\n",
       " 'Samco Overnight Fund',\n",
       " 'Samco Arbitrage Fund']"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(grand_final_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Assets Under Management',\n",
       " 'Fund Manager',\n",
       " 'Investment Objective',\n",
       " 'Market Cap',\n",
       " 'NAV',\n",
       " 'Quantitative Data',\n",
       " 'Scheme Details']"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract unique indices and ensure \"Market Cap\" is included\n",
    "imp_indices = {indices for value in grand_final_data.values() for indices in value.keys()}\n",
    "imp_indices.add(\"Market Cap\")\n",
    "modified_indices = sorted(imp_indices)\n",
    "\n",
    "modified_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map indices to funct , it has to be sorted\n",
    "function_indices = [\n",
    "    return_aum_data,\n",
    "    return_fund_data,\n",
    "    return_invest_data,\n",
    "    return_mar_data,\n",
    "    return_nav_data,\n",
    "    return_quant_data,\n",
    "    return_scheme_data,\n",
    "]\n",
    "\n",
    "function_map = {index: func for index, func in zip(modified_indices, function_indices)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samco Active Momentum Fund\n",
      "Done for _________________\n",
      "Samco Dynamic Asset Allocation Fund\n",
      "Done for _________________\n",
      "Samco Flexi Cap Fund\n",
      "Done for _________________\n",
      "Samco Multi Cap Fund\n",
      "Done for _________________\n",
      "Samco Special Opportunities Fund\n",
      "Done for _________________\n",
      "Samco Elss Tax Saver Fund\n",
      "Done for _________________\n",
      "Samco Multi Asset Allocation Fund\n",
      "Done for _________________\n",
      "Samco Overnight Fund\n",
      "Done for _________________\n",
      "Samco Arbitrage Fund\n",
      "Done for _________________\n"
     ]
    }
   ],
   "source": [
    "# Perform operation based on the function map\n",
    "def perform_operation(operation, data):\n",
    "    func = function_map.get(operation)\n",
    "    if func:\n",
    "        try:\n",
    "            return func(data)\n",
    "        except Exception as e:\n",
    "            return f\"Error executing function for {operation}: {e}\"\n",
    "    return \"Invalid operation\"\n",
    "\n",
    "\n",
    "\n",
    "grand_dictionary = {}\n",
    "for master_k, page_data in grand_final_data.items():\n",
    "    print(master_k)\n",
    "    page_content = []\n",
    "\n",
    "    for main_k, main_content in page_data.items():\n",
    "        func = function_map.get(main_k)\n",
    "        if func:\n",
    "            try:\n",
    "                result = func(main_k, main_content)\n",
    "                page_content.append(result)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing -> {main_k}: {e}\")\n",
    "        else:\n",
    "            print(f\"Warning: No function mapped for {main_k}\")\n",
    "\n",
    "    grand_dictionary[master_k] = page_content\n",
    "    print(\"Done for _________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON CREATED\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(path +r\"\\output\\dump58_31-Dec-24_FS.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(grand_dictionary, file, ensure_ascii=False, indent=4)\n",
    "    print(\"JSON CREATED\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envPDF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
