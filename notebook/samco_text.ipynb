{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import fitz\n",
    "import warnings , math, collections , os, re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\Kaustubh.keny\\OneDrive - Cogencis Information Services Ltd\\Documents\\mywork-repo\"\n",
    "#path = r\"C:\\Users\\rando\\OneDrive\\Documents\\mywork-repo\"\n",
    "\n",
    "samco_path = path + r\"\\files\\58_29-Feb-24_FS.pdf\"\n",
    "dry_run_path = path + r\"\\output\\DryRun.pdf\"\n",
    "indice_path = path + r\"\\output\\pkl\\indices_var.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_financial_indices(path:str):\n",
    "    final_indices = set()\n",
    "    with open(path , 'rb') as file:\n",
    "        indices = pickle.load(file)  \n",
    "        for k,v in indices.items():\n",
    "            temp = [k] + v\n",
    "            for t in temp:\n",
    "                final_indices.add(t)\n",
    "    \n",
    "    return list(final_indices)\n",
    "\n",
    "\"\"\" Highlights important financial indices in the pdf, does other pre\n",
    "analysis of data.\n",
    "Args: list of indices, string of pdf path\n",
    "Returns: dict of pages highlighted, string of output pdf, dict of pages contaiting FUND NAMES\n",
    "\"\"\"\n",
    "def check_indice_highlight(path:str, indices_variations:list, fund_pattern:str, fund_size:int):\n",
    "    doc = fitz.open(path)\n",
    "    page_count = doc.page_count #No of pages\n",
    "    \n",
    "    pages = [i for i in range(page_count)]\n",
    "    important_pages = dict.fromkeys(pages, 0)\n",
    "    fund_titles = dict.fromkeys(pages, \"\")\n",
    "\n",
    "\n",
    "    for page_num, page in enumerate(doc):\n",
    "        \n",
    "        text_instances = page.get_text('dict')[\"blocks\"]\n",
    "        \n",
    "        #sort for all data in pdf document \n",
    "        sorted_text_instances = sorted(text_instances, key=lambda x: (x['bbox'][1], x['bbox'][0]))\n",
    "        \n",
    "        # rect = fitz.Rect((35,120,250,765))\n",
    "        # page.add_highlight_annot(rect)\n",
    "\n",
    "        for pgn,block in enumerate(sorted_text_instances):     \n",
    "            if \"lines\" not in block:\n",
    "                continue\n",
    "            \n",
    "            for line in block[\"lines\"]: \n",
    "                for span in line[\"spans\"]:\n",
    "                    if span['flags'] in [20,25]:  # learn flag logic , rn set for all flags value\n",
    "                        span_text = span['text'].strip().lower()\n",
    "                        \n",
    "                        \n",
    "                        #FUND PAGE CHECK\n",
    "                        conditions = [\n",
    "                            pgn in range(0,15),\n",
    "                            re.match(fund_pattern, span_text, re.IGNORECASE),\n",
    "                            span['size'] >fund_size\n",
    "                        ]\n",
    "                        if all(conditions):\n",
    "        \n",
    "                            fund_titles[page_num] = span_text  \n",
    "                        \n",
    "                        #CHECK IMP FINANCE INDICES  \n",
    "                        for term in indices_variations:  \n",
    "                            pattern = r'\\b' + re.escape(term.lower()) + r'\\b'\n",
    "                            if re.search(pattern, span_text):\n",
    "\n",
    "                                #count highlights\n",
    "                                important_pages[page_num] +=1\n",
    "                                #mark content\n",
    "                                rect = fitz.Rect(span['bbox']) \n",
    "                                page.add_highlight_annot(rect)\n",
    "                                break  #optional , one highlight\n",
    "\n",
    "    \n",
    "    output_path = None\n",
    "    if any(important_pages.values()):\n",
    "        output_path = path.replace('.pdf', '_highlighted.pdf')\n",
    "        doc.save(output_path)\n",
    "\n",
    "    doc.close()\n",
    "    return important_pages, output_path, fund_titles\n",
    "\n",
    "\n",
    "\"\"\" Get the clipped data in the bbox provided and store in nested dict\n",
    "Args: input path, dryrun path, important pages, bbox coords\n",
    "Returns: dict { 'page' : int 'block': dict}\"\"\"\n",
    "def get_clipped_data(input:str, output:str, pageSelect:list, bbox:list[set], fund_names:dict):\n",
    "    \n",
    "    document = fitz.open(input)\n",
    "    finalData = []\n",
    "    \n",
    "    for pgn in pageSelect:\n",
    "        #get the page\n",
    "        page = document[pgn]\n",
    "        fundName = fund_names[pgn]\n",
    "\n",
    "        #get all block\n",
    "        final_blocks = []\n",
    "        for box in bbox:\n",
    "            blocks = page.get_text('dict', clip = box)['blocks'] #get all blocks\n",
    "            filtered_blocks = [block for block in blocks if block['type']==0 and 'lines' in block] #only text blocks\n",
    "            sorted_blocks = sorted(filtered_blocks, key= lambda x: (x['bbox'][1], x['bbox'][0]))\n",
    "            final_blocks.extend(sorted_blocks)\n",
    "        \n",
    "        \n",
    "        finalData.append({\n",
    "            \"page\": pgn,\n",
    "            \"fundname\": fundName,\n",
    "            \"block\": final_blocks,\n",
    "        })\n",
    "            \n",
    "    return finalData\n",
    "\n",
    "\n",
    "def get_pdf_data(input:str, pageSelect:list, fund_names:dict):\n",
    "    \n",
    "    document = fitz.open(input)\n",
    "    finalData = []\n",
    "    \n",
    "    for pgn in pageSelect:\n",
    "        #get the page\n",
    "        page = document[pgn]\n",
    "        fundName = fund_names[pgn]\n",
    "    \n",
    "        blocks = page.get_text('dict')['blocks'] #get all blocks\n",
    "        \n",
    "        filtered_blocks = [block for block in blocks if block['type']==0 and 'lines' in block]\n",
    "        sorted_blocks = sorted(filtered_blocks, key= lambda x: (x['bbox'][1], x['bbox'][0]))\n",
    "        \n",
    "        finalData.append({\n",
    "            \"page\": pgn,\n",
    "            \"fundname\": fundName,\n",
    "            \"block\": sorted_blocks,\n",
    "        })\n",
    "            \n",
    "    return finalData\n",
    "\n",
    "def extract_span_data(data:list, name:list): #all\n",
    "    final_data = dict()\n",
    "    for pgn,page in enumerate(data):\n",
    "        pgn_content = []\n",
    "        for blocks in page['block']:\n",
    "            for line in blocks['lines']:\n",
    "                spans = line.get('spans',[])\n",
    "                for span in spans:\n",
    "                    \n",
    "                    text = span['text'].strip()\n",
    "                    size = span['size']\n",
    "                    color = span['color']\n",
    "                    origin = span['origin']\n",
    "                    bbox = span['bbox']\n",
    "                \n",
    "                    pgn_content.append([size,text,color,origin,bbox])\n",
    "                    \n",
    "        final_data[page['fundname']] = pgn_content\n",
    "    \n",
    "    return final_data\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def clean_block_data(blocks):\n",
    "    \n",
    "    remove_text = ['Purchase','Amount','thereafter','.','. ',',',':','st',\";\",\"-\",'st ',' ','th', 'th ', 'rd', 'rd ', 'nd', 'nd ','','`','(Date of Allotment)']\n",
    "    \n",
    "    sorted_blocks = sorted(blocks, key=lambda x: (x[3][1],x[3][0]))\n",
    "    \n",
    "    cleaned_blocks = []\n",
    "    for block in sorted_blocks:\n",
    "        size, text, color, origin, bbox = block\n",
    "        if text not in remove_text:\n",
    "            cleaned_blocks.append(block)\n",
    " \n",
    "    processed_blocks = []\n",
    "    # adjust size based on color and size\n",
    "    for block in cleaned_blocks:\n",
    "        size, text, color, origin, bbox = block\n",
    "        text = text.strip()\n",
    "        if size in [9.0,8.0] and color == -1:\n",
    "            size = 20.0  # Update size to 20.0\n",
    "        processed_blocks.append([size, text, color, origin, bbox])\n",
    "                \n",
    "\n",
    "    # group blocks by rounded y-coordinate\n",
    "    grouped_blocks = defaultdict(list)\n",
    "    for block in processed_blocks:\n",
    "        y_coord = math.ceil(block[3][1])# Extract and round the y-coordinate\n",
    "        size = block[0]\n",
    "        grouped_blocks[(y_coord,size)].append(block)\n",
    "\n",
    "    # Combine blocks with the same y-coordinate\n",
    "    combined_blocks = []\n",
    "    for key, group in grouped_blocks.items():\n",
    "        \n",
    "        if key[1] == 20:\n",
    "            combined_text = \" \".join(item[1] for item in group).strip()\n",
    "            if combined_text:  # Ignore whitespace-only text\n",
    "                size, color, origin, bbox = group[0][0], group[0][2], group[0][3],group[0][4]\n",
    "                combined_blocks.append([size, combined_text, color, origin,bbox])\n",
    "        \n",
    "        else:\n",
    "            for item in group:\n",
    "                combined_blocks.append(item)\n",
    "\n",
    "    return combined_blocks\n",
    "\n",
    "def process_text_data(text_data):\n",
    "    \n",
    "    updated_text_data = {}\n",
    "\n",
    "    for fund, data in text_data.items():\n",
    "        blocks = data\n",
    "        cleaned_blocks = clean_block_data(blocks)\n",
    "        updated_text_data[fund] = cleaned_blocks\n",
    "\n",
    "    return updated_text_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "financial_indices = get_financial_indices(indice_path)\n",
    "pdfInd = pd.DataFrame({'indexes': financial_indices})\n",
    "\n",
    "excel_path  = path+ r'\\files\\financial_indices.xlsx'\n",
    "pdfInd.indexes.to_excel(excel_path) #remove first col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path  = samco_path\n",
    "financial_indices = get_financial_indices(indice_path)\n",
    "fund_pattern = r\"^(samco|tata|canara)\"\n",
    "fund_size = 16\n",
    "\n",
    "highlight_pages, saved_path, fund_pages =  check_indice_highlight(file_path, financial_indices, fund_pattern, fund_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagedf = pd.DataFrame({'title': fund_pages.values(),'highlight_count': highlight_pages.values()})\n",
    "\n",
    "\"\"\"_summary_ fund is located only on certain pages, based on no. of \n",
    "highlights we know which pages are imp. automate this content later\n",
    "\"\"\"\n",
    "pagedf.to_excel(path + r'\\output\\example.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = [ 3,5,7,9,11]\n",
    "bbox = [(35,120,250,765)]\n",
    "fund_pages = fund_pages\n",
    "file_path = samco_path\n",
    "\n",
    "data = get_clipped_data(file_path, dry_run_path, pages, bbox, fund_pages)\n",
    "text_data = extract_span_data(data,[])\n",
    "cleaned_data = process_text_data(text_data) #personalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_size = 20\n",
    "content_size = 10\n",
    "\n",
    "def create_nested_dict(cleaned_data:dict,header_size:float, content_size:float):\n",
    "    final_text_data = dict()\n",
    "    final_matrix = dict()\n",
    "\n",
    "    for fund, items in cleaned_data.items(): #ech fund\n",
    "        \n",
    "        #step 1 extract size, coord\n",
    "        coordinates = list()\n",
    "        sizes = set()\n",
    "        \n",
    "        for item in items: #size,text,color,origin\n",
    "            origin = tuple(item[3])\n",
    "            coordinates.append(origin)\n",
    "            sizes.add(item[0])\n",
    "        \n",
    "        coordinates = sorted(set(coordinates), key=lambda c: (c[1], c[0]))  # Sort by y, then x\n",
    "        sizes = sorted(sizes, reverse=True)  \n",
    "        \n",
    "        #step 2 create matrix\n",
    "        coord_to_index = {coord: idx for idx, coord in enumerate(coordinates)}  # (x,y) at pos 0 etc. ROWS\n",
    "        size_to_index = {font: idx for idx, font in enumerate(sizes)}  # COLUMNS\n",
    "        matrix = np.zeros((len(coordinates), len(sizes)), dtype=object)\n",
    "        \n",
    "        \n",
    "        #step 3\n",
    "        nested_dict = {}\n",
    "        current_header = None\n",
    "        for item in items:\n",
    "            origin = tuple(item[3])\n",
    "            size = item[0]\n",
    "            text = item[1].strip()\n",
    "            \n",
    "            #populate the matrix\n",
    "            if origin in coord_to_index and size in size_to_index:\n",
    "                row = coord_to_index[origin]\n",
    "                col = size_to_index[size]\n",
    "                \n",
    "                if matrix[row,col] == 0:\n",
    "                    matrix[row,col] ==r\"nil\"\n",
    "                matrix[row,col] == text\n",
    "            \n",
    "            #build nested dict\n",
    "            if size == header_size:\n",
    "                current_header = \" \".join(txt for txt in text.split(\" \") if txt !=\"\").lower()\n",
    "                nested_dict[current_header] = []\n",
    "            elif size<= content_size and current_header:\n",
    "                nested_dict[current_header].append(item)\n",
    "                \n",
    "        final_text_data[fund] = nested_dict        \n",
    "        matrix_df = pd.DataFrame(matrix, index=coordinates, columns=sizes)\n",
    "        final_matrix[fund] = matrix_df\n",
    "    \n",
    "    return final_text_data, final_matrix\n",
    "def generate_pdf_from_data(data:list, output_path:str):\n",
    "    \n",
    "    pdf_doc = fitz.open()\n",
    "    \n",
    "    for header, items in data.items():\n",
    "        \n",
    "        page = pdf_doc.new_page()\n",
    "        text_position = 72  # for title initalize something\n",
    "\n",
    "        #section title\n",
    "        title_font_size = 24\n",
    "        try:\n",
    "            page.insert_text(\n",
    "                (72, text_position), #initalizor\n",
    "                header,\n",
    "                fontsize=title_font_size,\n",
    "                fontname=\"helv\",\n",
    "                color=(0, 0, 1),\n",
    "            )        \n",
    "        except Exception as e:\n",
    "            print(f\"Error while parsing fund {e}\")\n",
    "            \n",
    "        for item in items:\n",
    "            \n",
    "            bbox = item[3] #origin coords\n",
    "            text = item[1]\n",
    "            size = item[0]\n",
    "            color = item[2]\n",
    "   \n",
    "            #Errror in fitz font \n",
    "            try:\n",
    "                page.insert_text(\n",
    "                    (bbox[0], bbox[1]),\n",
    "                    text,\n",
    "                    fontsize=size,\n",
    "                    fontname=\"helv\",\n",
    "                    color=tuple(int(color & 0xFFFFFF) for _ in range(3)))#unsigned int value so (0,0,0)\n",
    "                \n",
    "            except Exception:\n",
    "                page.insert_text(\n",
    "                    (bbox[0], bbox[1]),\n",
    "                    text,\n",
    "                    fontsize=size,\n",
    "                    fontname=\"helv\",\n",
    "                    color=(1, 0, 0),\n",
    "                )\n",
    "\n",
    "    # Save the created PDF\n",
    "    pdf_doc.save(output_path)\n",
    "    pdf_doc.close()\n",
    "    print(f\" PDF generated to: {output_path}\")\n",
    "\n",
    "def extract_data_from_pdf(path:str):\n",
    "    \n",
    "    def replace_main_key(string: str):\n",
    "        replace_key = string\n",
    "        if re.match(r'^nav.*', string, re.IGNORECASE):\n",
    "            replace_key = \"nav\"\n",
    "        elif re.match(r\"^market\", string, re.IGNORECASE):\n",
    "            replace_key = \"market_capital\"  \n",
    "        elif re.match(r\"^assets\", string, re.IGNORECASE):\n",
    "            replace_key = \"assets_under_management\"\n",
    "        elif re.match(r\"^fund\", string, re.IGNORECASE):\n",
    "            replace_key = \"fund_manager\" \n",
    "        elif re.match(r\"^scheme\", string, re.IGNORECASE):\n",
    "            replace_key = \"scheme_details\" \n",
    "        elif re.match(r\"^investment\", string, re.IGNORECASE):\n",
    "            replace_key = \"investment_objective\"\n",
    "        elif re.match(r\"^quanti\", string, re.IGNORECASE):\n",
    "            replace_key = \"quantitative_data\"\n",
    "        elif re.match(r\"^portfolio\", string, re.IGNORECASE):\n",
    "            replace_key = \"portfilio\" \n",
    "        elif re.match(r\"^industry\", string, re.IGNORECASE):\n",
    "            replace_key = \"industry_allocation_of_equity\"       \n",
    "        return replace_key\n",
    "    \n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        final_data = []\n",
    "        final_data_generated = {}\n",
    "        \n",
    "        for page in pdf.pages:\n",
    "            # extract text from the page\n",
    "            text = page.extract_text()\n",
    "            final_data.append(text)\n",
    "        #print(final_data)\n",
    "        \n",
    "        #store them in a dict for each page\n",
    "        for data in final_data:\n",
    "            content = data.split('\\n')\n",
    "            main_key = replace_main_key(content[0])\n",
    "            print(main_key)\n",
    "            values = content[1:]\n",
    "        \n",
    "            final_data_generated[main_key] = values\n",
    "\n",
    "        #sort the headers in lex order\n",
    "        sorted_final_generated = {key: final_data_generated[key] for key in sorted(final_data_generated)}\n",
    "\n",
    "    return sorted_final_generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_text_data, final_matrix = create_nested_dict(cleaned_data, 20.0 , 10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_extracted_text = dict()\n",
    "for fund, items in final_text_data.items():\n",
    "    print(fund)\n",
    "    generate_pdf_from_data(items, dry_run_path)\n",
    "    extract_data = extract_data_from_pdf(dry_run_path)\n",
    "    final_extracted_text[fund] = extract_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON CREATED\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(path +r\"\\output\\dump58_29-Feb-24_FS.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(final_extracted_text, file, ensure_ascii=False, indent=4)\n",
    "    print(\"JSON CREATED\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['assets_under_management',\n",
       " 'fund_manager',\n",
       " 'investment_objective',\n",
       " 'nav',\n",
       " 'quantitative_data',\n",
       " 'scheme_details']"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_indices = sorted({indices for value in final_extracted_text.values() for indices in value.keys()})\n",
    "imp_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REGEX FUNCTIONS\n",
    "\n",
    "def return_invest_data(key:str,data:list):\n",
    "    investment_objective = data\n",
    "    values = \" \".join(txt for txt in investment_objective)\n",
    "\n",
    "    data = {\n",
    "        key:values\n",
    "    }\n",
    "\n",
    "    return data\n",
    "\n",
    "def return_scheme_data(key:str,data:list):\n",
    "    scheme_data = data\n",
    "    main_key = key\n",
    "    structured_data = {main_key: {}}\n",
    "\n",
    "    # Patterns\n",
    "    date_pattern = r\"^(.*?date)\\s(\\d{2}-[A-Za-z]{3}-\\d{4})$\"\n",
    "    benchmark_pattern = r\"^(Benchmark)\\s+(.*)$\"\n",
    "    application_pattern = r\"(?:路)?\\d+(?:,\\d{3})*(?:\\.\\d+)?/-\"\n",
    "\n",
    "    for data in scheme_data:\n",
    "        if re.search(date_pattern, data, re.IGNORECASE):\n",
    "            match = re.match(date_pattern, data, re.IGNORECASE)\n",
    "            if match:\n",
    "                key = match.group(1)\n",
    "                value = match.group(2)\n",
    "                structured_data[main_key][key] = value\n",
    "        elif re.search(benchmark_pattern, data, re.IGNORECASE):\n",
    "            match = re.match(benchmark_pattern, data, re.IGNORECASE)\n",
    "            if match:\n",
    "                key = match.group(1)\n",
    "                value = match.group(2)\n",
    "                structured_data[main_key][key] = value\n",
    "        elif re.search(r\"\\b(min|application)\\b\", data, re.IGNORECASE):\n",
    "            matches = re.findall(application_pattern, data, re.IGNORECASE)\n",
    "            if matches:\n",
    "                cleaned_matches = [match.replace('路', '') for match in matches]\n",
    "                structured_data[main_key][\"min_appl_amt\"] = cleaned_matches\n",
    "        elif re.search(r\"\\b(additional.* and in multiples of)\\b\", data, re.IGNORECASE):\n",
    "            matches = re.findall(application_pattern, data, re.IGNORECASE)\n",
    "            if matches:\n",
    "                cleaned_matches = [match.replace('路', '') for match in matches]\n",
    "                structured_data[main_key][\"additional_amt\"] = cleaned_matches\n",
    "\n",
    "    return structured_data\n",
    "\n",
    "def return_fund_data(key:str,data:list):\n",
    "    fund_manager = data\n",
    "    main_key = key\n",
    "    strucuted_data = {main_key:[]}\n",
    "    current_entry = None\n",
    "    name_pattern = r'^(Ms\\.|Mr\\.)'\n",
    "    manage_pattern = r'^\\(|\\)$'\n",
    "    date_pattern = r'\\b\\w+ \\d{1,2}, \\d{4}\\b'\n",
    "    experience_pattern = r'^Total Experience: (.+)$'\n",
    "\n",
    "    for data in fund_manager:\n",
    "        if re.match(name_pattern,data):\n",
    "            if current_entry:\n",
    "                strucuted_data[main_key].append(current_entry)\n",
    "            current_entry = {\n",
    "                'name': data.split(\",\")[0].strip().lower(),\n",
    "                'designation': \"\".join(data.split(\",\")[1:]).strip().lower()\n",
    "            }\n",
    "            #print(data.split(\",\")[0],\"\".join(data.split(\",\")[1:]))\n",
    "        elif re.match(manage_pattern,data):\n",
    "            if \"inception\" in data.lower():\n",
    "                current_entry['managing_since'] = 'inception'\n",
    "            else:\n",
    "                date = re.search(date_pattern, data)\n",
    "                current_entry['managing_since'] = date.group() if date != None else None\n",
    "        elif re.match(experience_pattern,data):\n",
    "            current_entry['total_experience'] = data.split(\":\")[1].strip().lower()\n",
    "            #print(data.split(\":\")[1])\n",
    "\n",
    "        \n",
    "    if current_entry:  # Append the last entry\n",
    "        strucuted_data[main_key].append(current_entry)\n",
    "            \n",
    "    return strucuted_data\n",
    "\n",
    "def return_nav_data(key:str,data:list):\n",
    "    main_key = key\n",
    "    structured_data = {main_key: {}}\n",
    "    \n",
    "    growth_pattern = r\"([\\w\\s]+):\\s*路([\\d.]+)\"\n",
    "    \n",
    "    for line in data:\n",
    "        matches = re.findall(growth_pattern, line)\n",
    "        for key, value in matches:\n",
    "            structured_data[main_key][key.strip().lower()] = value\n",
    "        \n",
    "    return structured_data\n",
    "\n",
    "def return_quant_data(key:str,data:list):\n",
    "    qunatitative_data = data\n",
    "    main_key = key\n",
    "\n",
    "    strucuted_data = {main_key:{}}\n",
    "    current_entry = None\n",
    "    comment = \"\"\n",
    "\n",
    "    ratio_pattern = r\"\\b(ratio|turnover)\\b\"\n",
    "    annual_pattern = r'\\b(annualised|YTM)\\b'\n",
    "    macaulay_pattern = r\"\\b(macaulay.*duration)\\b\"\n",
    "    residual_pattern = r\"\\b(residual.*maturity)\\b\"\n",
    "    modified_pattern = r\"\\b(modified.*duration)\\b\"\n",
    "\n",
    "    for data in qunatitative_data:\n",
    "        if re.search(ratio_pattern,data, re.IGNORECASE):\n",
    "            key = data.split(\":\")[0].lower().strip()\n",
    "            value = data.split(\":\")[1].lower().strip()\n",
    "        elif re.search(annual_pattern,data, re.IGNORECASE):\n",
    "            key = data.split(\":\")[0].lower().strip()\n",
    "            value = data.split(\":\")[1].lower().strip()\n",
    "        elif re.search(macaulay_pattern,data, re.IGNORECASE):\n",
    "            key = data.split(\":\")[0].lower().strip()\n",
    "            value = data.split(\":\")[1].lower().strip()\n",
    "        elif re.search(residual_pattern,data, re.IGNORECASE):\n",
    "            key = data.split(\":\")[0].lower().strip()\n",
    "            value = data.split(\":\")[1].lower().strip()\n",
    "        elif re.search(modified_pattern,data, re.IGNORECASE):\n",
    "            key = data.split(\":\")[0].lower().strip()\n",
    "            value = data.split(\":\")[1].lower().strip()\n",
    "        else:\n",
    "            comment+= data\n",
    "        strucuted_data[main_key][key] = value\n",
    "    \n",
    "    strucuted_data[main_key]['comment'] = comment\n",
    "\n",
    "    return strucuted_data\n",
    "\n",
    "def return_aum_data(key:str,data:list):\n",
    "    \n",
    "    aum = data\n",
    "    main_key = key\n",
    "    strucuted_data = {main_key:{}}\n",
    "\n",
    "    pattern = r\"\\b\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)? Crs\\b\"\n",
    "\n",
    "    for data in aum:\n",
    "        if re.search(r'average', data, re.IGNORECASE):\n",
    "            match = re.search(pattern, data)\n",
    "            key = 'avg_aum'\n",
    "        elif re.search(pattern, data):\n",
    "            match = re.search(pattern, data)\n",
    "            key = \"aum\"\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        if match:\n",
    "            strucuted_data[main_key][key] = match.group()\n",
    "\n",
    "    return strucuted_data\n",
    "\n",
    "def return_dummy_data(key:str,data:list):\n",
    "    return {key:{}}\n",
    "\n",
    "\n",
    "def return_required_regex(string: str):\n",
    "        replace_key = string\n",
    "        if re.match(r'^nav.*', string, re.IGNORECASE):\n",
    "            replace_key = \"nav\"\n",
    "        elif re.match(r\"^market\", string, re.IGNORECASE):\n",
    "            replace_key = \"market_capital\"  \n",
    "        elif re.match(r\"^assets\", string, re.IGNORECASE):\n",
    "            replace_key = \"assets_under_management\"\n",
    "        elif re.match(r\"^fund\", string, re.IGNORECASE):\n",
    "            replace_key = \"fund_manager\" \n",
    "        elif re.match(r\"^scheme\", string, re.IGNORECASE):\n",
    "            replace_key = \"scheme_details\" \n",
    "        elif re.match(r\"^investment\", string, re.IGNORECASE):\n",
    "            replace_key = \"investment_objective\"\n",
    "        elif re.match(r\"^quanti\", string, re.IGNORECASE):\n",
    "            replace_key = \"quantitative_data\"\n",
    "        elif re.match(r\"^portfolio\", string, re.IGNORECASE):\n",
    "            replace_key = \"portfilio\" \n",
    "        elif re.match(r\"^industry\", string, re.IGNORECASE):\n",
    "            replace_key = \"industry_allocation_of_equity\"       \n",
    "        return replace_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assets_under_management <function return_aum_data at 0x000001EF93CC2340>\n",
      "fund_manager <function return_dummy_data at 0x000001EF93CC1120>\n",
      "investment_objective <function return_fund_data at 0x000001EF93CC3060>\n",
      "nav <function return_invest_data at 0x000001EF93CC1BC0>\n",
      "quantitative_data <function return_nav_data at 0x000001EF93CC16C0>\n",
      "scheme_details <function return_quant_data at 0x000001EF93CC23E0>\n"
     ]
    }
   ],
   "source": [
    "# Map indices to funct , it has to be sorted\n",
    "function_indices = [\n",
    "    return_aum_data,\n",
    "    return_dummy_data,\n",
    "    return_fund_data,\n",
    "    return_invest_data,\n",
    "    return_nav_data,\n",
    "    return_quant_data,\n",
    "    return_scheme_data,\n",
    "]\n",
    "\n",
    "function_indices = sorted(function_indices, key=lambda x: str(x))\n",
    "\n",
    "\n",
    "for key, funct in zip(imp_indices,function_indices):\n",
    "    print(key, funct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform operation based on the function map\n",
    "def perform_operation(operation, data):\n",
    "    func = function_map.get(operation)\n",
    "    if func:\n",
    "        try:\n",
    "            return func(data)\n",
    "        except Exception as e:\n",
    "            return f\"Error executing function for {operation}: {e}\"\n",
    "    return \"Invalid operation\"\n",
    "\n",
    "\n",
    "\n",
    "grand_dictionary = {}\n",
    "for master_k, page_data in final_text_data.items():\n",
    "    print(master_k)\n",
    "    page_content = []\n",
    "\n",
    "    for main_k, main_content in page_data.items():\n",
    "        func = function_map.get(main_k)\n",
    "        if func:\n",
    "            try:\n",
    "                result = func(main_k, main_content)\n",
    "                page_content.append(result)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing -> {main_k}: {e}\")\n",
    "        else:\n",
    "            print(f\"Warning: No function mapped for {main_k}\")\n",
    "\n",
    "    grand_dictionary[master_k] = page_content\n",
    "    print(\"Done for _________________\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envPDF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
