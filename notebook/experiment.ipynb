{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint, json, math, os, sys\n",
    "import fitz, pdfplumber\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "dir_path = \"C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\office-work\\\\mywork-repo\"\n",
    "fund_path = \"C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\Jan 25\"\n",
    "\n",
    "# dir_path = \"C:\\\\Users\\\\rando\\\\OneDrive\\\\Documents\\\\mywork-repo\"\n",
    "# fund_path =  \"C:\\\\Users\\\\rando\\\\OneDrive\\\\Documents\\\\Jan 25\"\n",
    "sys.path.append(os.path.abspath(dir_path))\n",
    "\n",
    "from app.utils import Helper\n",
    "from app.parse_regex import *\n",
    "\n",
    "dry_path = r'\\data\\output\\DryRun.pdf'\n",
    "fin_path = r'\\data\\input\\financial_indices.xlsx'\n",
    "mutual_fund = Helper.get_fund_paths(fund_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_clipped_data(input:str, pages:list, bboxes:list):\n",
    "        \n",
    "        document = fitz.open(input)\n",
    "        final_list = []\n",
    "    \n",
    "        for pgn in pages:\n",
    "            page = document[pgn]\n",
    "            \n",
    "            all_blocks = [] #store every data from bboxes\n",
    "            \n",
    "            for bbox in bboxes:\n",
    "                blocks, seen_blocks = [], set()  #store unique blocks based on content and bbox\n",
    "                \n",
    "                page_blocks = page.get_text('dict', clip=bbox)['blocks']\n",
    "                for block in page_blocks:\n",
    "                    if block['type'] == 0 and 'lines' in block: #type 0 means text block\n",
    "                        #hash_key\n",
    "                        block_key = (tuple(block['bbox']), tuple(tuple(line['spans'][0]['text'] for line in block['lines'])))\n",
    "                        if block_key not in seen_blocks:\n",
    "                            seen_blocks.add(block_key)\n",
    "                            blocks.append(block)\n",
    "\n",
    "                sorted_blocks = sorted(blocks, key=lambda x: (x['bbox'][1], x['bbox'][0]))\n",
    "                all_blocks.append(sorted_blocks)\n",
    "\n",
    "            final_list.append({\n",
    "                \"pgn\": pgn,\n",
    "                \"block\": all_blocks #will be list[list,list,..]\n",
    "            })\n",
    "\n",
    "        document.close()\n",
    "        return final_list\n",
    "    \n",
    "def extract_data_relative_line(path: str, line_x: float, side: str):\n",
    "    doc = fitz.open(path)\n",
    "    pages = doc.page_count\n",
    "\n",
    "    final_list = []\n",
    "\n",
    "    for pgn in range(pages):\n",
    "        page = doc[pgn]\n",
    "\n",
    "        blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "        sorted_blocks = sorted(blocks, key=lambda x: (x[\"bbox\"][1], x[\"bbox\"][0]))\n",
    "        extracted_blocks = []\n",
    "\n",
    "        # Keep track of blocks to avoid duplicates\n",
    "        added_blocks = set()\n",
    "\n",
    "        for block in sorted_blocks:\n",
    "            block_id = id(block)  # Unique identifier for the block\n",
    "\n",
    "            for line in block.get(\"lines\", []):\n",
    "                for span in line.get(\"spans\", []):\n",
    "                    origin = span[\"origin\"]\n",
    "                    x0, _ = origin\n",
    "\n",
    "                    # Check the side condition\n",
    "                    if side == \"left\" and x0 < line_x and block_id not in added_blocks:\n",
    "                        extracted_blocks.append(block)\n",
    "                        added_blocks.add(block_id)  # Mark block as added\n",
    "                    elif side == \"right\" and x0 > line_x and block_id not in added_blocks:\n",
    "                        extracted_blocks.append(block)\n",
    "                        added_blocks.add(block_id)  # Mark block as added\n",
    "\n",
    "      \n",
    "        final_list.append({\n",
    "            \"pgn\": pgn,\n",
    "            \"blocks\": extracted_blocks\n",
    "        })\n",
    "\n",
    "    doc.close()\n",
    "\n",
    "    return final_list\n",
    "  \n",
    "def get_clipped_data(input:str, bboxes:list[set], *args):\n",
    "    \n",
    "        document = fitz.open(input)\n",
    "        final_list = []\n",
    "        if args:\n",
    "            pages = list(args)\n",
    "        else:\n",
    "            pages = [i for i in document.page_count]\n",
    "        \n",
    "        for pgn in pages:\n",
    "            page = document[pgn]\n",
    "\n",
    "            blocks = []\n",
    "            for bbox in bboxes:\n",
    "                blocks.extend(page.get_text('dict', clip = bbox)['blocks']) #get all blocks\n",
    "            \n",
    "            filtered_blocks = [block for block in blocks if block['type']== 0 and 'lines' in block]\n",
    "            # sorted_blocks = sorted(filtered_blocks, key= lambda x: (x['bbox'][1], x['bbox'][0]))\n",
    "            \n",
    "             # Extract text from sorted blocks\n",
    "            extracted_text = []\n",
    "            for block in filtered_blocks:\n",
    "                block_text = []\n",
    "                for line in block['lines']:\n",
    "                    line_text = \" \".join(span['text'] for span in line['spans'])\n",
    "                    block_text.append(line_text)\n",
    "                extracted_text.append(\"\\n\".join(block_text))\n",
    "            \n",
    "            final_list.append({\n",
    "            \"pgn\": pgn,\n",
    "            \"block\": filtered_blocks,\n",
    "            \"text\": extracted_text\n",
    "            })\n",
    "            \n",
    "            \n",
    "        document.close()\n",
    "        return final_list\n",
    "    \n",
    "def get_clipped_text(input:str, bboxes:list[set],*args):\n",
    "\n",
    "    document = fitz.open(input)\n",
    "    final_list = []\n",
    "    \n",
    "    if args:\n",
    "        pages = list(args)\n",
    "    else:\n",
    "        pages = [i for i in document.page_count]\n",
    "    \n",
    "    for pgn in pages:\n",
    "        page = document[pgn]\n",
    "        blocks = []\n",
    "        for bbox in bboxes:\n",
    "            blocks = page.get_text('text', clip = bbox).split('\\n') #get all blocks\n",
    "        final_list.append({\n",
    "        \"pgn\": pgn,\n",
    "        \"block\": blocks\n",
    "        })   \n",
    "    document.close()\n",
    "    return final_list\n",
    "\n",
    "def get_proper_fund_names(path: str, pages: list):\n",
    "    doc = fitz.open(path)\n",
    "    title = {}\n",
    "\n",
    "    for pgn in pages:\n",
    "        page = doc[pgn]\n",
    "        blocks = page.get_text(\"dict\")['blocks']\n",
    "        text_all = \" \".join(\n",
    "            span[\"text\"].strip()\n",
    "            for block in blocks[:4]\n",
    "            for line in block.get(\"lines\", [])\n",
    "            for span in line.get(\"spans\", [])\n",
    "            if span[\"text\"].strip()\n",
    "        )\n",
    "\n",
    "        text_all = re.sub(r'[^A-Za-z0-9\\s]+', '', text_all).strip()\n",
    "        matches = re.findall(r\"((?:LIC\\s*MF|BLNCED|LOW)\\s+.*?(?:FUND|ETF|FTF|FOF|PLAN|SAVER))\", text_all, re.IGNORECASE)\n",
    "\n",
    "        title[pgn] = matches[0] if matches else \"\"\n",
    "\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\Jan 25\\\\HDFC Mutual Fund\\\\12_31-Jan-2025_1_FS.pdf'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_path  = mutual_fund[\"HDFC Mutual Fund\"]\n",
    "sample_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified PDF saved to: C:\\Users\\rando\\OneDrive\\Documents\\mywork-repo\\data\\output\\DryRun.pdf\n"
     ]
    }
   ],
   "source": [
    "lines = [\n",
    "    ((165, 0), (165, 812)),# Vertical line\n",
    "    ((0, 90), (812, 90))\n",
    "]\n",
    "pages = [12, 14,16]\n",
    "bboxes = [(0, 115, 220, 812)]\n",
    "pages = [i for i in range(1,110)]\n",
    "Helper.draw_lines_on_pdf(sample_path, lines, bboxes, pages, dir_path +dry_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proper_fund_names(path: str):\n",
    "        pattern =  \"(QUANTUM.*?(?:FUND|ETF|EOF|FOF|FTF|PATH|ELSS|FUNDS))\"\n",
    "        title = {}\n",
    "        \n",
    "        with fitz.open(path) as doc:\n",
    "            for pgn, page in enumerate(doc):\n",
    "                text = \" \".join(page.get_text(\"text\", clip=(0, 0, 450, 90)).split(\"\\n\"))\n",
    "                text = re.sub(\"[^A-Za-z0-9\\\\s\\\\-\\\\(\\\\).,]+\", \"\", text).strip()\n",
    "                if matches := re.findall(pattern, text):\n",
    "                    title[pgn] = matches[0]\n",
    "        return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = get_proper_fund_names(sample_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Helper.get_all_pdf_data(sample_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import camelot\n",
    "def get_something(path: str):\n",
    "    pattern = r\"^(REDEMPTION PROCEEDS|FEATURES|ASSET ALLOCATION|FUND MANAGER|SCHEME|Sr\\. No\\.)$\"\n",
    "\n",
    "    with fitz.open(path) as doc:\n",
    "        exists = defaultdict(int)\n",
    "        for pgn, page in enumerate(doc):\n",
    "            page_text = [t.strip() for t in page.get_text().split(\"\\n\")]\n",
    "            for text in page_text:\n",
    "                if re.match(pattern,text):\n",
    "                    exists[pgn]+=1\n",
    "                    \n",
    "    \n",
    "    return [str(pgn+1) for pgn, count in exists.items() if count > 4] #camelot starts pages from 1\n",
    "\n",
    "pages = get_something(sample_path)\n",
    "\n",
    "imp_pages = \",\".join(pages)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = camelot.read_pdf(sample_path,pages=imp_pages, flavor=\"stream\", table_areas= [\"30,50,612,812\"],column_tol = 4, split_text = True) #[\"30,50,612,812\"]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import camelot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def clean_column_name(x):\n",
    "    cleaned = re.sub(r\"[^A-Za-z0-9\\s]\", \"\", str(x), flags=re.IGNORECASE) #Corrected re.sub.\n",
    "    cleaned = \"_\".join(cleaned.split())\n",
    "    return cleaned\n",
    "\n",
    "with pd.ExcelWriter(\"cleaned_tables.xlsx\", engine=\"xlsxwriter\") as writer:\n",
    "    for i, table in enumerate(tables):\n",
    "        df = table.df\n",
    "        df = df.map(lambda x: \" \".join(str(x).split(\"\\n\")).strip())\n",
    "        df.replace(\"\", np.nan, inplace=True)\n",
    "        df.iloc[2:, 0] = df.iloc[2:, 0].ffill()\n",
    "        df = df.iloc[1:, :]\n",
    "        df.columns = df.iloc[0, :].apply(clean_column_name)\n",
    "        # print(df.columns)\n",
    "        df = df.iloc[1:, :]\n",
    "        sheet_name = f\"Table_{i+1}\"\n",
    "\n",
    "        # workbook = writer.book\n",
    "        # worksheet = writer.sheets[sheet_name]\n",
    "\n",
    "        # row = 1\n",
    "        # last_value = None\n",
    "        # start_row = None\n",
    "\n",
    "        # for j, value in enumerate(df.iloc[:, 0], start=1):\n",
    "        #     if pd.notna(value):  # New group found\n",
    "        #         if last_value is not None and start_row is not None:\n",
    "        #             worksheet.merge_range(start_row, 0, row - 1, 0, last_value)  # Merge previous block\n",
    "        #         last_value = value\n",
    "        #         start_row = row\n",
    "        #     row += 1\n",
    "\n",
    "        # # Merge last group\n",
    "        # if last_value is not None and start_row is not None:\n",
    "        #     worksheet.merge_range(start_row, 0, row - 1, 0, last_value)\n",
    "    \n",
    "        \n",
    "\n",
    "print(\"Cleaned tables saved with merged spanning rows!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{14: 'LIC MF LARGE CAP FUND',\n",
    " 15: 'LIC MF LARGE& MID CAPFUND',\n",
    " 17: 'LIC MF MULTICAP FUND',\n",
    " 18: \"LIC MF V't CAP FUND\",\n",
    " 19: 'SMALLCAP FUND',\n",
    " 21: 'LIC MF DIV_DEND YIELD FUND',\n",
    " 22: 'LIC MF VALUE FUND',\n",
    " 23: 'LIC MF FOCUSED FUND',\n",
    " 24: 'LIC MF INFRASTRUCTURE FUND',\n",
    " 25: 'LIC MF Poin MANGFACTURING FUND',\n",
    " 26: 'BANKING & FINANC-AL SERVICES FUND',\n",
    " 27: 'HEALTHCARE FUND',\n",
    " 28: 'LiC MF EL_SS TAX SAVER',\n",
    " 29: 'LIC MF AGGRESSIVE HYBRiD FUND',\n",
    " 30: 'LIC MF B’L*NCED ADVANTAGE FUND',\n",
    " 31: 'LIC MF EQUITY SAVINGS FUND',\n",
    " 32: 'LIC MF CONSERWATIVE nip?!) FUND',\n",
    " 33: 'LIC MF ARBITRAGE FUND',\n",
    " 36: 'LIC MF OVERNIGHT FUND',\n",
    " 37: 'LIC MF LIQUID FUND',\n",
    " 38: 'LIC MF ULTRA SHORT DURATION FUND',\n",
    " 40: 'LIC MF LOW’ DURATION FUND',\n",
    " 41: 'LIC MF MEDIUM-F2°LONG DURATION FUND',\n",
    " 42: 'LIC MF BANK&ONG & PSU FUND',\n",
    " 43: 'LIC MF_. SHORT DURATION FUND',\n",
    " 45: 'LIC MGI FUND',\n",
    " 46: 'LIC MF l HILDRENS FUND',\n",
    " 47: 'BSE SENSEX ETF',\n",
    " 48: 'LIC MF NIFTY 50ETF',\n",
    " 49: 'LIC MF NIFTY 100 ETF',\n",
    " 50: 'LIC MF NIFTY MIDCAP 100 ETF',\n",
    " 51: 'LIC MF NIFTY 8-13 YR G-SECETF',\n",
    " 52: 'LIC MF BSE SENSEX INDEX FUND',\n",
    " 53: 'LIC MF NIFTY 50 INDEX FUND',\n",
    " 54: 'LIC MF NIFTY NEXT 50 INDEX FUND',\n",
    " 55: 'LIC MF G2LD EXCHANGE TRADED FUND',\n",
    " 56: 'LIC MF GSLD ETF',\n",
    " 57: 'LIC MF Large Cap Fund',\n",
    " 58: 'LIC MF Mid cap Fund',\n",
    " 59: 'LIC MF Focused Fund',\n",
    " 60: 'Lic MF ELSs Tax Saver',\n",
    " 61: 'LIC MF Arbitrage Fund',\n",
    " 63: 'LIC MF Nifty Midcap 100 ETF',\n",
    " 64: 'LIC MF Large Cap Fund',\n",
    " 65: 'LIC MF Dividend Yield Fund',\n",
    " 66: 'LIC MEF Healthcare Fund',\n",
    " 67: 'LIC MF Unit Linked Insurance Scheme __LIC MF Overnight Fund',\n",
    " 68: 'LIC MF Short Duration Fund',\n",
    " 69: 'LIC MF BSE Sensex ETF',\n",
    " 70: 'LIC MF BSE Sensex Index Fund',\n",
    " 73: 'LIC MF Healthcare Fund',\n",
    " 74: 'LIC MF Liquid Fund',\n",
    " 75: 'LIC MF Nifty 50 ETF',\n",
    " 76: 'LIC Mutual Fund'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NIPPON DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def via_block(path:str):\n",
    "    pattern = r\"FUNDS AT A GLANCE\"\n",
    "    amc_pattern = \"^(Nippon India|CPSE).*(?=Plan|Next 50|Sensex|Fund|Path|ETF|FOF|EOF|Funds|$)\"\n",
    "    imp_pages = []\n",
    "    with fitz.open(path) as doc:\n",
    "        for pgn, page in enumerate(doc):\n",
    "                page_blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "                sorted_blocks = sorted(page_blocks, key=lambda x: (x[\"bbox\"][1], x[\"bbox\"][0]))\n",
    "                for block_count, block in enumerate(sorted_blocks[:10]):\n",
    "                    if \"lines\" not in block:\n",
    "                        continue\n",
    "                    for line in block[\"lines\"]:\n",
    "                        for span in line[\"spans\"]:\n",
    "                            text = span[\"text\"].strip()\n",
    "                            if re.match(pattern,text):\n",
    "                                imp_pages.append(pgn)\n",
    "                                \n",
    "        amc_fund = defaultdict(list)\n",
    "    \n",
    "        for pgn in imp_pages:\n",
    "            page = doc[pgn]\n",
    "            page_blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "            sorted_blocks = sorted(page_blocks, key=lambda x: (x[\"bbox\"][1], x[\"bbox\"][0]))\n",
    "            for block_count, block in enumerate(sorted_blocks):\n",
    "                if \"lines\" not in block:\n",
    "                    continue\n",
    "                for line in block[\"lines\"]:\n",
    "                    for span in line[\"spans\"]:\n",
    "                        text = span[\"text\"].strip()\n",
    "                        color = span['color']\n",
    "                        if re.match(amc_pattern,text)and color == -1:\n",
    "                            # matches = re.findall(amc_pattern,text)\n",
    "                            amc_fund[pgn].append(text)\n",
    "                            \n",
    "    return imp_pages, dict(amc_fund)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages,amc = via_block(sample_path)\n",
    "pages = list(map(str,[x+1 for x in pages]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_scheme = defaultdict(list)\n",
    "for key, value in amc.items():\n",
    "    # print(key)\n",
    "    set1 = ['Scheme Name']+value[:4]\n",
    "    set2 = ['Scheme Name']+value[4:]\n",
    "    final_scheme[key+1].append(set1)\n",
    "    final_scheme[key+1].append(set2)\n",
    "\n",
    "final_scheme = dict(final_scheme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_pages = \",\".join(pages)\n",
    "tables = camelot.read_pdf(sample_path,pages=imp_pages, flavor=\"lattice\", line_scale = 40)  #table_areas = [\"0,0,580,690\"]            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "with pd.ExcelWriter(\"merged_tables.xlsx\", engine=\"openpyxl\") as writer:\n",
    "    count = 0  # Toggle between 0 and 1\n",
    "    \n",
    "    for i, table in enumerate(tables):\n",
    "        df = table.df\n",
    "        if df.shape[1] < 3:\n",
    "            continue\n",
    "\n",
    "    \n",
    "        df = df.map(lambda x: \" \".join(x.split(\"\\n\")).strip())\n",
    "        df = df.map(lambda x: np.nan if not x.strip() else x)\n",
    "        df.set_index(df.columns[0], inplace=True)\n",
    "\n",
    "        for check in [\"Scheme Name\", \"Market Capitalization\"]:\n",
    "            if check in df.index:\n",
    "                df.drop(check, inplace=True)\n",
    "                \n",
    "        df_cleaned = df[~df.index.isna()]\n",
    "        df_cleaned = df_cleaned[df_cleaned.index != \"\"]\n",
    "        df_cleaned = df_cleaned.reset_index()\n",
    "        df_fill = df_cleaned.ffill(axis=1)\n",
    "\n",
    "\n",
    "        sch_vals = final_scheme[table.page][count]\n",
    "        count = 1 - count  # Toggle between 0 and 1\n",
    "\n",
    "        if len(sch_vals) == 5:\n",
    "            df_fill.loc[-1] = sch_vals \n",
    "            df_fill = df_fill.sort_index().reset_index(drop=True)\n",
    "\n",
    "        # Write to a new sheet\n",
    "        df_fill.to_excel(writer, sheet_name=f\"Table_{i+1}\", index=False)\n",
    "\n",
    "print(\"All tables saved in separate sheets in 'merged_tables.xlsx' 🚀\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hello =  {\n",
    "    \"number\": 0,\n",
    "    \"type\": 0,\n",
    "    \"bbox\": (0,0,0,0), #406.72119140625, 439.4930419921875, 565.697265625, 484.5830383300781\n",
    "    \"lines\": [\n",
    "        {\n",
    "            \"spans\": [\n",
    "                {\n",
    "                    \"size\": 30.0,\n",
    "                    \"flags\": 20,\n",
    "                    \"font\": \"Montserrat-Regular\", #set this\n",
    "                    \"color\": -1, #set this\n",
    "                    \"ascender\": 1.0429999828338623,\n",
    "                    \"descender\": -0.2619999945163727,\n",
    "                    \"text\": \"DUMMYDUMMYDUMMYDUMMY\",\n",
    "                    \"origin\": (406.72119140625, 458.26702880859375),\n",
    "                    \"bbox\": (0,0,0,0), #406.72119140625,439.4930419921875,565.697265625,462.9830322265625,\n",
    "                }\n",
    "            ],\n",
    "            \"wmode\": 0,\n",
    "            \"dir\": (1.0, 0.0),\n",
    "            \"bbox\": (0,0,0,0), #406.72119140625,439.4930419921875,565.697265625,462.9830322265625,\n",
    "        },\n",
    "        \n",
    "    ],\n",
    "},\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
