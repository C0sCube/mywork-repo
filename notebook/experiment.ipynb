{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint, json, math, os, sys\n",
    "import fitz, pdfplumber, ocrmypdf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# dir_path = \"C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\office-work\\\\mywork-repo\"\n",
    "# fund_path = \"C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\Feb 25\"\n",
    "\n",
    "dir_path = \"C:\\\\Users\\\\rando\\\\OneDrive\\\\Documents\\\\mywork-repo\"\n",
    "fund_path =  \"C:\\\\Users\\\\rando\\\\OneDrive\\\\Documents\\\\Feb25\"\n",
    "sys.path.append(os.path.abspath(dir_path))\n",
    "\n",
    "from app.utils import Helper\n",
    "from app.parse_regex import *\n",
    "\n",
    "dry_path = r'\\data\\output\\DryRun.pdf'\n",
    "fin_path = r'\\data\\input\\financial_indices.xlsx'\n",
    "mutual_fund = Helper.get_fund_paths(fund_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_clipped_data(input:str, pages:list, bboxes:list):\n",
    "        \n",
    "        document = fitz.open(input)\n",
    "        final_list = []\n",
    "    \n",
    "        for pgn in pages:\n",
    "            page = document[pgn]\n",
    "            \n",
    "            all_blocks = [] #store every data from bboxes\n",
    "            \n",
    "            for bbox in bboxes:\n",
    "                blocks, seen_blocks = [], set()  #store unique blocks based on content and bbox\n",
    "                \n",
    "                page_blocks = page.get_text('dict', clip=bbox)['blocks']\n",
    "                for block in page_blocks:\n",
    "                    if block['type'] == 0 and 'lines' in block: #type 0 means text block\n",
    "                        #hash_key\n",
    "                        block_key = (tuple(block['bbox']), tuple(tuple(line['spans'][0]['text'] for line in block['lines'])))\n",
    "                        if block_key not in seen_blocks:\n",
    "                            seen_blocks.add(block_key)\n",
    "                            blocks.append(block)\n",
    "\n",
    "                sorted_blocks = sorted(blocks, key=lambda x: (x['bbox'][1], x['bbox'][0]))\n",
    "                all_blocks.append(sorted_blocks)\n",
    "\n",
    "            final_list.append({\n",
    "                \"pgn\": pgn,\n",
    "                \"block\": all_blocks #will be list[list,list,..]\n",
    "            })\n",
    "\n",
    "        document.close()\n",
    "        return final_list\n",
    "    \n",
    "def extract_data_relative_line(path: str, line_x: float, side: str):\n",
    "    doc = fitz.open(path)\n",
    "    pages = doc.page_count\n",
    "\n",
    "    final_list = []\n",
    "\n",
    "    for pgn in range(pages):\n",
    "        page = doc[pgn]\n",
    "\n",
    "        blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "        sorted_blocks = sorted(blocks, key=lambda x: (x[\"bbox\"][1], x[\"bbox\"][0]))\n",
    "        extracted_blocks = []\n",
    "\n",
    "        # Keep track of blocks to avoid duplicates\n",
    "        added_blocks = set()\n",
    "\n",
    "        for block in sorted_blocks:\n",
    "            block_id = id(block)  # Unique identifier for the block\n",
    "\n",
    "            for line in block.get(\"lines\", []):\n",
    "                for span in line.get(\"spans\", []):\n",
    "                    origin = span[\"origin\"]\n",
    "                    x0, _ = origin\n",
    "\n",
    "                    # Check the side condition\n",
    "                    if side == \"left\" and x0 < line_x and block_id not in added_blocks:\n",
    "                        extracted_blocks.append(block)\n",
    "                        added_blocks.add(block_id)  # Mark block as added\n",
    "                    elif side == \"right\" and x0 > line_x and block_id not in added_blocks:\n",
    "                        extracted_blocks.append(block)\n",
    "                        added_blocks.add(block_id)  # Mark block as added\n",
    "\n",
    "      \n",
    "        final_list.append({\n",
    "            \"pgn\": pgn,\n",
    "            \"blocks\": extracted_blocks\n",
    "        })\n",
    "\n",
    "    doc.close()\n",
    "\n",
    "    return final_list\n",
    "  \n",
    "def get_clipped_data(input:str, bboxes:list[set], *args):\n",
    "    \n",
    "        document = fitz.open(input)\n",
    "        final_list = []\n",
    "        if args:\n",
    "            pages = list(args)\n",
    "        else:\n",
    "            pages = [i for i in document.page_count]\n",
    "        \n",
    "        for pgn in pages:\n",
    "            page = document[pgn]\n",
    "\n",
    "            blocks = []\n",
    "            for bbox in bboxes:\n",
    "                blocks.extend(page.get_text('dict', clip = bbox)['blocks']) #get all blocks\n",
    "            \n",
    "            filtered_blocks = [block for block in blocks if block['type']== 0 and 'lines' in block]\n",
    "            # sorted_blocks = sorted(filtered_blocks, key= lambda x: (x['bbox'][1], x['bbox'][0]))\n",
    "            \n",
    "             # Extract text from sorted blocks\n",
    "            extracted_text = []\n",
    "            for block in filtered_blocks:\n",
    "                block_text = []\n",
    "                for line in block['lines']:\n",
    "                    line_text = \" \".join(span['text'] for span in line['spans'])\n",
    "                    block_text.append(line_text)\n",
    "                extracted_text.append(\"\\n\".join(block_text))\n",
    "            \n",
    "            final_list.append({\n",
    "            \"pgn\": pgn,\n",
    "            \"block\": filtered_blocks,\n",
    "            \"text\": extracted_text\n",
    "            })\n",
    "            \n",
    "            \n",
    "        document.close()\n",
    "        return final_list\n",
    "    \n",
    "def get_clipped_text(input:str, bboxes:list[set],*args):\n",
    "\n",
    "    document = fitz.open(input)\n",
    "    final_list = []\n",
    "    \n",
    "    if args:\n",
    "        pages = list(args)\n",
    "    else:\n",
    "        pages = [i for i in document.page_count]\n",
    "    \n",
    "    for pgn in pages:\n",
    "        page = document[pgn]\n",
    "        blocks = []\n",
    "        for bbox in bboxes:\n",
    "            blocks = page.get_text('text', clip = bbox).split('\\n') #get all blocks\n",
    "        final_list.append({\n",
    "        \"pgn\": pgn,\n",
    "        \"block\": blocks\n",
    "        })   \n",
    "    document.close()\n",
    "    return final_list\n",
    "\n",
    "def get_proper_fund_names(path: str, pages: list):\n",
    "    doc = fitz.open(path)\n",
    "    title = {}\n",
    "\n",
    "    for pgn in pages:\n",
    "        page = doc[pgn]\n",
    "        blocks = page.get_text(\"dict\")['blocks']\n",
    "        text_all = \" \".join(\n",
    "            span[\"text\"].strip()\n",
    "            for block in blocks[:4]\n",
    "            for line in block.get(\"lines\", [])\n",
    "            for span in line.get(\"spans\", [])\n",
    "            if span[\"text\"].strip()\n",
    "        )\n",
    "\n",
    "        text_all = re.sub(r'[^A-Za-z0-9\\s]+', '', text_all).strip()\n",
    "        matches = re.findall(r\"((?:LIC\\s*MF|BLNCED|LOW)\\s+.*?(?:FUND|ETF|FTF|FOF|PLAN|SAVER))\", text_all, re.IGNORECASE)\n",
    "\n",
    "        title[pgn] = matches[0] if matches else \"\"\n",
    "\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_path = mutual_fund[\"Axis Mutual Fund\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified PDF saved to: C:\\Users\\rando\\OneDrive\\Documents\\mywork-repo\\data\\output\\DryRun.pdf\n"
     ]
    }
   ],
   "source": [
    "lines = [\n",
    "    ((200, 0), (200, 812)),# Vertical line\n",
    "    ((360, 0), (360, 812)),\n",
    "    # ((0, 145), (812, 145))\n",
    "]\n",
    "pages = [12, 14,16]\n",
    "bboxes = [[435, 17, 610, 103],] #[(0, 85, 180, 812),(180, 85, 360, 812),(0,100,270,812),(0,100,350,812)]\n",
    "pages = [i for i in range(1,110)]\n",
    "Helper.draw_lines_on_pdf(sample_path, lines, bboxes, pages, dir_path +dry_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_clipped_text_all_pages(pdf_path, clip_coords):\n",
    "    results = {}\n",
    "    doc = fitz.open(pdf_path)\n",
    "    clip_rect = fitz.Rect(*clip_coords)\n",
    "    try:\n",
    "        for page_number, page in enumerate(doc):\n",
    "            text = page.get_text(\"text\", clip=clip_rect).strip()\n",
    "            results[page_number] = text\n",
    "    finally:\n",
    "        doc.close()\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clip_coords = (435, 17, 610, 100)  # (x0, y0, x1, y1)\n",
    "\n",
    "clipped_texts = extract_clipped_text_all_pages(sample_path, clip_coords)\n",
    "\n",
    "for page_num, text in clipped_texts.items():\n",
    "    print(f\"Page {page_num + 1}:\\n{text}\\n{'-'*40}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "W A A A\n",
      "\n",
      "AXIS BLUECHIP FUND FA\n",
      "AXIS BLUECHIP FUND  7\n",
      "AXIS FOCUSED FUND\n",
      "AXIS FOCUSED FUND 8\n",
      "AXIS ELSS TAX SAVER FUND\n",
      "AXIS ELSS TAX SAVER FUND 9\n",
      "AXIS GROWTH OPPORTUNITIES FUND FACTSH\n",
      "AXIS GROWTH OPPORTUNITIES FUND  10\n",
      "AXIS FLEXI CAP FUND\n",
      "AXIS FLEXI CAP FUND 11\n",
      "AXIS MULTICAP FUND F\n",
      "AXIS MULTICAP FUND  12\n",
      "AXIS MIDCAP FUND FA\n",
      "AXIS MIDCAP FUND  13\n",
      "AXIS INNOVATION FUND FACTSH\n",
      "AXIS INNOVATION FUND  14\n",
      "AXIS SMALL CAP FUND FA\n",
      "AXIS SMALL CAP FUND  15\n",
      "AXIS ESG INTEGRATION STRATEGY FUND FACTSH\n",
      "AXIS ESG INTEGRATION STRATEGY FUND  16\n",
      "AXIS QUANT FUND FAC Feb\n",
      "AXIS QUANT FUND  17\n",
      "AXIS VALUE FUND FAC Fe\n",
      "AXIS VALUE FUND  18\n",
      "AXIS BUSINESS CYCLES FUND FACTSH\n",
      "AXIS BUSINESS CYCLES FUND  19\n",
      "AXIS INDIA MANUFACTURING FUND\n",
      "AXIS INDIA MANUFACTURING FUND 20\n",
      "AXIS CONSUMPTION FUND FAC Fe\n",
      "AXIS CONSUMPTION FUND  21\n",
      "AXIS MOMENTUM FUND FAC Feb\n",
      "AXIS MOMENTUM FUND  22\n",
      "AXIS NIFTY 50 ETFÂ  FACTS (NSE Symbol AXISNIFTY)\n",
      "AXIS NIFTY 50 ETFÂ   23\n",
      "AXIS NIFTY BANK ETFÂ  FACT (NSE Symbol AXISBNKETF)\n",
      "AXIS NIFTY BANK ETFÂ  FACT (NSE Symbol AXISBNKETF 24\n",
      "AXIS NIFTY IT ETFÂ  FACTS (BSE Scrip Code 543347, NSE Symbol AXISTECETF)\n",
      "AXIS NIFTY IT ETFÂ  FACTS (BSE Scrip Code 543347, NSE Symbol AXISTECETF 25\n",
      "AXIS NIFTY HEALTHCARE ETFÂ  FACT (NSE Symbol AXISHCETF, BSE Scrip Code 543348)\n",
      "AXIS NIFTY HEALTHCARE ETFÂ  FACT (NSE Symbol AXISHCETF 26\n",
      "AXIS NIFTY INDIA CONSUMPTION ETFÂ  FA (NSE Symbol AXISCETF, BSE Scrip Code 543357)\n",
      "AXIS NIFTY INDIA CONSUMPTION ETFÂ  FA (NSE Symbol AXISCETF 27\n",
      "AXIS BSE SENSEX ETFÂ  FACTS (BSE Scrip Code 543853, NSE Symbol AXSENSEX)\n",
      "AXIS BSE SENSEX ETFÂ   28\n",
      "AXIS NIFTY 100 INDEX FUNDÂ  FACT\n",
      "AXIS NIFTY 100 INDEX FUNDÂ   29\n",
      "AXIS NIFTY 50 INDEX FUNDÂ  FACT\n",
      "AXIS NIFTY 50 INDEX FUNDÂ   30\n",
      "AXIS BSE SENSEX INDEX FUNDÂ  FAC\n",
      "AXIS BSE SENSEX INDEX FUNDÂ   31\n",
      "AXIS NIFTY NEXT 50 INDEX FUNDÂ  FAC\n",
      "AXIS NIFTY NEXT 50 INDEX FUNDÂ   32\n",
      "AXIS NIFTY SMALLCAP 50 INDEX FUNDÂ  FA\n",
      "AXIS NIFTY SMALLCAP 50 INDEX FUNDÂ   33\n",
      "AXIS NIFTY MIDCAP 50 INDEX FUNDÂ  FAC\n",
      "AXIS NIFTY MIDCAP 50 INDEX FUNDÂ   34\n",
      "AXIS NIFTY IT INDEX FUNDÂ  FACT\n",
      "AXIS NIFTY IT INDEX FUNDÂ   35\n",
      "AXIS NIFTY BANK INDEX FUNDÂ  FACT\n",
      "AXIS NIFTY BANK INDEX FUNDÂ   36\n",
      "AXIS NIFTY 500 INDEX FUNDÂ  FACT\n",
      "AXIS NIFTY 500 INDEX FUNDÂ   37\n",
      "AXIS NIFTY500 VALUE 50 INDEX FUNDÂ  FA\n",
      "AXIS NIFTY500 VALUE 50 INDEX FUNDÂ   38\n",
      "AXIS NIFTY500 MOMENTUM 50 INDEX FUNDÂ  FA\n",
      "AXIS NIFTY500 MOMENTUM 50 INDEX FUNDÂ   39\n",
      "AXIS EQUITY ETFS FOF\n",
      "AXIS EQUITY ETF 40\n",
      "AXIS GLOBAL EQUITY ALPHA FUND OF FUND\n",
      "AXIS GLOBAL EQUITY ALPHA FUND OF FUND 41\n",
      "AXIS GREATER CHINA EQUITY FUND OF FUND\n",
      "AXIS GREATER CHINA EQUITY FUND OF FUND 42\n",
      "AXIS GLOBAL INNOVATION FUND OF FUND\n",
      "AXIS GLOBAL INNOVATION FUND OF FUND 43\n",
      "AXIS NASDAQ 100 FUND OF FUND\n",
      "AXIS NASDAQ 100 FUND OF FUND 44\n",
      "\n",
      "AXIS OVERNIGHT FUND\n",
      "AXIS OVERNIGHT FUND 46\n",
      "\n",
      "AXIS LIQUID FUND\n",
      "AXIS LIQUID FUND 48\n",
      "\n",
      "AXIS ULTRA SHORT DURATION FUND\n",
      "AXIS ULTRA SHORT DURATION FUND 50\n",
      "\n",
      "AXIS FLOATER FUND\n",
      "AXIS FLOATER FUND 52\n",
      "\n",
      "AXIS TREASURY ADVANTAGE FUND\n",
      "AXIS TREASURY ADVANTAGE FUND 54\n",
      "\n",
      "AXIS MONEY MARKET FUND\n",
      "AXIS MONEY MARKET FUND 56\n",
      "\n",
      "AXIS CORPORATE BOND FUND\n",
      "AXIS CORPORATE BOND FUND 58\n",
      "\n",
      "AXIS BANKING  PSU DEBT FUND\n",
      "AXIS BANKING  PSU DEBT FUND 60\n",
      "\n",
      "AXIS SHORT DURATION FUND\n",
      "AXIS SHORT DURATION FUND 62\n",
      "\n",
      "AXIS CREDIT RISK FUND\n",
      "AXIS CREDIT RISK FUND 64\n",
      "\n",
      "AXIS DYNAMIC BOND FUND\n",
      "AXIS DYNAMIC BOND FUND 66\n",
      "\n",
      "AXIS STRATEGIC BOND FUND\n",
      "AXIS STRATEGIC BOND FUND 68\n",
      "\n",
      "AXIS LONG DURATION FUND\n",
      "AXIS LONG DURATION FUND 70\n",
      "AXIS GILT FUND\n",
      "AXIS GILT FUND 71\n",
      "AXIS INCOME ADVANTAGE FUND OF FUNDS\n",
      "AXIS INCOME ADVANTAGE FUND OF FUND 72\n",
      "\n",
      "AXIS NIFTY AAA BOND PLUS SDL APR 2026 5050 ETF                                                                        (NSE Symbol AXISBPSETF)\n",
      "AXIS NIFTY AAA BOND PLUS SDL APR 2026 5050 ETF                                                                        (NSE Symbol AXISBPSETF 74\n",
      "\n",
      "AXIS NIFTY AAA BOND PLUS SDL APR 2026 5050 ETF FOF\n",
      "AXIS NIFTY AAA BOND PLUS SDL APR 2026 5050 ETF FOF 76\n",
      "AXIS US TREASURY DYNAMIC BOND ETF FUND OF FUND\n",
      "AXIS US TREASURY DYNAMIC BOND ETF FUND OF FUND 77\n",
      "AXIS CRISIL IBX 7030 CPSE PLUS SDL APRIL 2025 INDEX FUND\n",
      "AXIS CRISIL IBX 7030 CPSE PLUS SDL APRIL 2025 INDEX FUND 78\n",
      "AXIS CRISIL IBX SDL MAY 2027 INDEX FUND\n",
      "AXIS CRISIL IBX SDL MAY 2027 INDEX FUND 79\n",
      "\n",
      "AXIS NIFTY SDL SEPTEMBER 2026 DEBT INDEX FUND\n",
      "AXIS NIFTY SDL SEPTEMBER 2026 DEBT INDEX FUND 81\n",
      "AXIS CRISIL IBX 5050 GILT PLUS SDL JUNE 2028 INDEX FUND\n",
      "AXIS CRISIL IBX 5050 GILT PLUS SDL JUNE 2028 INDEX FUND 82\n",
      "AXIS CRISIL IBX 5050 GILT PLUS SDL SEPTEMBER 2027 INDEX\n",
      "AXIS CRISIL IBX 5050 GILT PLUS SDL SEPTEMBER 2027 INDEX 83\n",
      "AXIS CRISIL IBX SDL JUNE 2034 DEBT INDEX FUND\n",
      "AXIS CRISIL IBX SDL JUNE 2034 DEBT INDEX FUND 84\n",
      "AXIS CRISIL-IBX AAA BOND NBFC - JUN 2027 INDEX FUND\n",
      "AXIS CRISIL-IBX AAA BOND NBFC - JUN 2027 INDEX FUND 85\n",
      "AXIS CRISIL-IBX AAA BOND FINANCIAL SERVICES - SEP 2027\n",
      "AXIS CRISIL-IBX AAA BOND NBFC-HFC - JUN 2027 INDEX FUND\n",
      "AXIS CRISIL-IBX AAA BOND NBFC-HFC - JUN 2027 INDEX FUND 87\n",
      "\n",
      "KEY HIGHLIGHTS\n",
      "AXIS CONSERVATIVE HYBRID FUND\n",
      "AXIS CONSERVATIVE HYBRID FUND 90\n",
      "\n",
      "AXIS EQUITY SAVINGS FUND FA F\n",
      "AXIS EQUITY SAVINGS FUND  92\n",
      "\n",
      "AXIS MULTI ASSET ALLOCATION FUND\n",
      "AXIS MULTI ASSET ALLOCATION FUND 94\n",
      "\n",
      "AXIS AGGRESSIVE HYBRID FUND\n",
      "AXIS AGGRESSIVE HYBRID FUND 96\n",
      "\n",
      "AXIS CHILDRENS FUND\n",
      "AXIS CHILDRENS FUND 98\n",
      "\n",
      "AXIS BALANCED ADVANTAGE FUND\n",
      "AXIS BALANCED ADVANTAGE FUND 100\n",
      "\n",
      "AXIS ARBITRAGE FUND FAC Feb\n",
      "AXIS ARBITRAGE FUND  102\n",
      "\n",
      "AXIS RETIREMENT FUND - AGGRESSIVE PLAN\n",
      "AXIS RETIREMENT FUND  104\n",
      "\n",
      "AXIS RETIREMENT FUND - DYNAMIC PLAN\n",
      "AXIS RETIREMENT FUND  106\n",
      "\n",
      "AXIS RETIREMENT FUND - CONSERVATIVE PLAN\n",
      "AXIS RETIREMENT FUND  108\n",
      "\n",
      "AXIS GOLD FUND FAC Feb\n",
      "AXIS GOLD FUND  110\n",
      "AXIS SILVER ETF FAC Fe (NSE Symbol AXISILVER)\n",
      "AXIS SILVER ETF  111\n",
      "AXIS GOLD ETF FAC Fe (NSE Scrip Code AXISGOLD, BSE Scrip Code 533570)\n",
      "AXIS GOLD ETF  112\n",
      "AXIS SILVER FUND OF FUND FA F\n",
      "AXIS SILVER FUND OF FUND  113\n",
      "SIP PERFORMANCE OF SELECT SCHEMES (as on 28th February, 2025) The Fund offers flexible and convenient Systematic Investment Plan (SIP) facility. To illustrate the advantages of SIP investments, this is how your investments w\n",
      "SIP PERFORMANCE OF SELECT SCHEMES (as on 28th February, 2025) The Fund offers flexible and convenient Systematic Investment Plan (SIP) facility. To illustrate the advantages of SIP investments, this is how your investments w\n",
      "SIP PERFORMANCE OF SELECT SCHEMES (as on 28th February, 2025) The Fund offers flexible and convenient Systematic Investment Plan (SIP) facility. To illustrate the advantages of SIP investments, this is how your investments w\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ANNEXURE FOR RETURNS OF ALL THE SCHEMES (as on 28th February, 2025)\n",
      "ANNEXURE FOR RETURNS OF ALL THE SCHEMES (as on 28th February, 2025)\n",
      "ANNEXURE FOR RETURNS OF ALL THE SCHEMES (as on 28th February, 2025)\n",
      "ANNEXURE FOR RETURNS OF ALL THE SCHEMES (as on 28th February, 2025)\n",
      "ANNEXURE FOR RETURNS OF ALL THE SCHEMES (as on 28th February, 2025)\n",
      "\n",
      "PRODUCT LABELLING\n",
      "PRODUCT LABELLING\n",
      "PRODUCT LABELLING\n",
      "PRODUCT LABELLING\n",
      "PRODUCT LABELLING\n",
      "PRODUCT LABELLING\n",
      "PRODUCT LABELLING\n",
      "PRODUCT LABELLING\n",
      "PRODUCT LABELLING\n",
      "PRODUCT LABELLING\n",
      "PRODUCT LABELLING\n",
      "PRODUCT LABELLING\n",
      "PRODUCT LABELLING\n",
      "PRODUCT LABELLING\n",
      "PRODUCT LABELLING\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_proper_fund_names(path: str):\n",
    "    pattern = \"(AXIS.*(?:FUND|ETF|INDEX)\\\\s*(?:OF FUND|FOF|FUND)?)\"\n",
    "    title = {}   \n",
    "    with fitz.open(path) as doc:\n",
    "        for pgn, page in enumerate(doc):\n",
    "            text = \" \".join(page.get_text(\"text\", clip=(0, 0, 500, 36)).split(\"\\n\"))\n",
    "            text = re.sub(\"[^A-Za-z0-9\\\\s\\\\-\\\\(\\\\).,]+|\\u2028\", \"\", text).strip()\n",
    "            print(text)\n",
    "            if matches := re.findall(pattern, text, re.DOTALL):\n",
    "                title[pgn] = \" \".join([_ for _ in matches[0].strip().split(\" \") if _])\n",
    "                print(matches[0],pgn)\n",
    "    return title\n",
    "title = get_proper_fund_names(sample_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r\"(360 ONE.*)$\" 0,0,520,50\n",
    "# \"(Aditya Birla.*(?:Plan\\\\*?\\\\#?\\\\'?|Sensex|Fund|Path|ETF|FOF\\\\*?|Scheme|EOF|Funds\\\\*?|Yojna)?)$\" 0,0,470,25\n",
    "# \"(Bajaj.*(?:Fund|Path|ETF|FOF|EOF|Growth))$\" 0,0,470,50\n",
    "# \"(Bank of India.*?(?:Plan|Funds?|ETF|FOF|FTF))\"  0,55,280,105\n",
    "# \"(Baroda BNP.*?(?:Fund|Path|ETF|FTF|FOF|Index|Fund of Fund))\" 0,0,220,120\n",
    "# \"CANARA.*?\\\\)\" 0,0,400,55\n",
    "# \"((?:DSP|Bharat).*?(?:Fund\\\\s*(?:of Fund)?|FUND|ETF|FTF|FOF))\" 0,0,500,40\n",
    "# \"(Edelweiss\\s*.+?(?:Fund|Path|ETF|FOF|Path))\" 0,0,150,100\n",
    "# \"((?:Franklin|Templeton).*?(?:Fund\\\\s*(?:of Funds)?|Plan))\" 0,0,470,80\n",
    "# \"(GROWW.*?FUND)\" 0,0,470,60\n",
    "# \"(HDFC.*?(?:FUND|Fund\\\\s*(?:of Funds?)?|ETF\\\\s*(?:Fund of Funds?)?))\" 0,0,400,60\n",
    "# \"(HSBC.*?(?:FUND|Fund\\\\s*(?:of Funds?)?|ETF\\\\s*(?:Fund of Funds?)?))\" 0,0,600,45\n",
    "# \"(Helios.*)\" 0,0,600,40\n",
    "# \"((?:ICICI|BHARAT).*)\" 0,0,490,30\n",
    "# \"(Invesco India.*?(?:Fund(?:of Funds?)?|ETF|FOF|Path))\" 180,0,590,40\n",
    "# \"(ITI.*?(?:Fund(?:of Funds?)?|ETF|FOF|Path))\" 0,30,300,100\n",
    "# \"(JM.*?(?:Fund(?:of Funds?)?|ETF|FOF|Path))\" 0,0,400,50\n",
    "# \"(KOTAK.*?(?:FUND\\\\s*(?:OF FUNDS?\\\\s*|-\\\\s*\\\\w+)?|ETF|FOF|PATH|FTF))\" 0,0,450,70\n",
    "# \"((?:LI?i?C|BSE|BANK|SMALL|HEALTH).*?(?:FUND|Path|ETF|FTF|EOF|FOF|PLAN|SAVER|FUND\\s*OF\\s*FUND))\" 0,0,400,100\n",
    "# \"\\\\b(Mahindra.*?(?:Fund|ETF|EOF|FOF|FTF|Path|FO))\\\\b\" 170,50,450,80\n",
    "# r'MIRAE ASSET .*?\\b(?:ETF|EOF|FOF|FTF|FUND|FUND OF FUND|INDEX FUND)\\b' 0,0,560,140\n",
    "# \"(NJ.*?(?:FUND\\\\s*(?:OF FUNDS?\\\\s*|-\\\\s*\\\\w+)?|ETF|FOF|PATH|FTF))\" 0,30,480,70\n",
    "# \"([A-Z0-9\\\\s\\\\-]+\\\\s*PGIM INDIA)\" 0,0,300,80\n",
    "# \"(Samco.*?Fund)\" 0,30,600,100\n",
    "# SBI PASSIVE r\"((?:SBI|i\\s*_|S35).*$)\" 0, 0, 400, 50 \n",
    "# SBI NORMAL \"(SBI.*?(?:Fund\\\\s*(?:of funds?|.*?Plan)?|ETF|FTF|FOF))\" 160, 640, 550, 812\n",
    "# r\"(Union\\s*[A-Za-z\\s]+?(?:FUND|PATH|ETF|FOF))\" 0,0,180,150 \n",
    "# \"(Sundaram.*?(?:Fund\\\\s*(?:of funds?|.*?Plan|Series)?|ETF|FTF|FOF))\" 0,0,400,40\n",
    "# \"((?:Tata|Treasury|TATA).*?(?:Funds?(?:.*?Plan)?|ETF|FOF|EOF|Plan.*?(?:days)?))\" 0,0,540,40\n",
    "# \"((?:TAURUS|Taurus).*?(?:FUND|Fund))\" 0,0,480,40\n",
    "# \"(TRUSTMF.*?(?:FUND|Fund))\" 0,0,340,45\n",
    "# \"(quant.*?(?:FUND|Fund))\" 0,0,480,80\n",
    "# \"(UTI.*?(?:FUND(?:\\\\s*OF FUND|.+?DURATION)?|PLAN|ETF|FTF|FOF|Fund|ETF(?: FUND OF FUND)?)(?:\\\\s*\\\\(\\\\s*Erst.+?\\\\))?)\" 0,0,460,35\n",
    "# \"(WhiteOak.*?(?:Fund))\" 0,0,410,60\n",
    "# \"(Zerodha.*?(?:Fund|ETF(?:\\\\s*FoF)?|FoF))\" 0,60,500,180\n",
    "# \"(Motilal.*?(?:Fund\\\\s*(?:o?O?f Funds?(?:.+?Aggressive|.+?Conservative)?)?|ETF(?:.+? of Funds?)?|FoF))\" 0,0,600,60\n",
    "\n",
    "# \"(SHRIRAM.*?(?:FUNDS?|ETF|Fo?O?F|PLANS?))\" 0,0,480,70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Helper.get_all_pdf_data(sample_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with fitz.open(sample_path) as doc:\n",
    "    indices = [\"Fund Manager\", \"Co-Fund Manager\", \"Date of Allotment\", \"Benchmark Index\", \"Minimum Application\", \"Additional Purchase\", \"Entry Load\", \"Exit Load\", \"Portfolio Turnover\", \"Net AUM\", \"Monthly Average AUM\", \"Std Dev\", \"Sharpe Ratio\", \"Portfolio Beta\", \"R Squared\", \"Treynor\", \"YTM\", \"Macaulay\", \"Residual Maturity\", \"NAV\"]\n",
    "    output_path = sample_path.replace(\".pdf\", \"_hltd.pdf\")\n",
    "    highlight_count,found_indices = 0,[]\n",
    "    for pgn, page in enumerate(doc):\n",
    "        blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "        for block in blocks:\n",
    "            if \"lines\" not in block:\n",
    "                continue\n",
    "            for line in block[\"lines\"]:\n",
    "                for span in line[\"spans\"]:\n",
    "                    text = span[\"text\"].strip().lower()\n",
    "                    text =  re.sub(\"[^\\\\w\\\\s]\", \"\", text)\n",
    "                    for indice in indices:\n",
    "                        if re.search(rf\"\\b{re.escape(indice)}\\b\", text,re.IGNORECASE):\n",
    "                            if indice not in found_indices:\n",
    "                                found_indices.append(indice)\n",
    "                                highlight_count += 1\n",
    "                            page.add_highlight_annot(fitz.Rect(span[\"bbox\"]))\n",
    "                            break\n",
    "\n",
    "    doc.save(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#canara\n",
    "# def _update_manager_data(self,main_key:str,manager_data):\n",
    "#     name = r\"(?:Mr\\.?|Mrs\\.?|Ms\\.?)\\s+([A-Z][a-z]+\\s[A-Z][a-z]+)\"\n",
    "#     exp = r\"\\b\\d+\\s*Years?\\b\"\n",
    "#     since = r\"\\bSince\\s*([0-9]+\\s*-\\s*[A-Za-z]+\\.?\\s*-\\s*[0-9]+)\\b\"\n",
    "#     nsample, msample, esample = [], [], []\n",
    "#     nlength = 0\n",
    "#     value = \" \".join(manager_data.values())\n",
    "#     nsample = re.findall(name, value, re.IGNORECASE)\n",
    "#     esample = re.findall(exp, value, re.IGNORECASE)\n",
    "#     msample = re.findall(since, value, re.IGNORECASE)\n",
    "    \n",
    "#     nlength = len(nsample)\n",
    "#     msample += [\"\"] * (nlength - len(msample))\n",
    "#     esample += [\"\"] * (nlength - len(esample))\n",
    "    \n",
    "#     final_list = [self._return_manager_data(since=m,name=n,exp=e)for n, m, e in zip(nsample, msample, esample)]\n",
    "#     return {main_key:final_list}\n",
    "\n",
    "# # hdfc\n",
    "# def _extract_manager_data(self, main_key: str, data, pattern:str):\n",
    "#     DATE_PATTERN = r\"([A-Za-z]+\\s*\\d+),\"\n",
    "#     NAME_PATTERN = r\"([A-Za-z]+\\s[A-Za-z]+)\"\n",
    "#     EXP_PATTERN = r\"over (\\d+ years)\"\n",
    "#     YEAR_PATTERN = r\"(\\d{4})\"\n",
    "\n",
    "#     manager_data = \" \".join(data)\n",
    "#     manager_data = re.sub(r\"[^A-Za-z0-9\\s\\-\\(\\).,]+\", \"\", manager_data).strip()\n",
    "\n",
    "#     experience_years = re.findall(EXP_PATTERN, manager_data, re.IGNORECASE)\n",
    "#     dates = re.findall(DATE_PATTERN, manager_data, re.IGNORECASE)[:len(experience_years)]\n",
    "#     years = re.findall(YEAR_PATTERN, manager_data, re.IGNORECASE)[:len(experience_years)]\n",
    "#     names = re.findall(NAME_PATTERN, manager_data, re.IGNORECASE)[:len(experience_years)]\n",
    "    \n",
    "#     managing_since = [f\"{date}, {year}\" for date, year in zip(dates, years)]\n",
    "#     experience_list = [f\"{exp} years\" for exp in experience_years]\n",
    "#     final_list = [\n",
    "#         self._return_manager_data(name=name,since=since,exp=exp)\n",
    "#         for since, exp, name in zip(managing_since, experience_list, names)\n",
    "#     ]\n",
    "    \n",
    "#     return {main_key:final_list}\n",
    "\n",
    "\n",
    "# #NJMF\n",
    "# def _update_manager_data(self,main_key:str,manager_data):\n",
    "#     exp =\"\\\\d{1,2} years\"\n",
    "#     name = \"(?:Mr\\\\.?|Mrs\\\\.?|Ms\\\\.?)\\\\s*([A-Za-z]+\\\\s*[A-Za-z]+)\"\n",
    "#     since = \"(?:since|from)\\\\s*([A-Za-z]+\\\\s*\\\\d{1,2},\\\\s*\\\\d{3,4}|inception)\"\n",
    "    \n",
    "#     manager_data = \" \".join(manager_data) if isinstance(manager_data,list) else manager_data\n",
    "#     manager_data =re.sub(self.REGEX[\"escape\"], \"\", manager_data).strip()\n",
    "#     nsample = re.findall(name, manager_data, re.IGNORECASE)\n",
    "#     esample = re.findall(exp, manager_data, re.IGNORECASE)\n",
    "#     msample = re.findall(since, manager_data, re.IGNORECASE)\n",
    "#     final_list = [self._return_manager_data(since=m,name=n,exp=e)for n, m, e in zip(nsample, msample, esample)]\n",
    "#     return {main_key:final_list}\n",
    "\n",
    "# #sbi\n",
    "# def _extract_manager_data(self, main_key: str, data, pattern: str):\n",
    "#     name = \"(?:Mr\\\\.?|Mrs\\\\.?|Ms\\\\.?)\\\\s*([A-Za-z.]+\\\\s*[A-Za-z]+)\"\n",
    "#     since = \"((?:\\\\(w.e.f\\\\.?)?[A-Za-z]+\\\\s*\\\\d{4}\\\\s*(?:\\\\()?)\"\n",
    "#     exp = \"([0-9]+ years)\"\n",
    "\n",
    "#     final_list = []\n",
    "#     manager_data = \" \".join(data) if isinstance(data,list) else data\n",
    "#     manager_data =re.sub(self.REGEX[\"escape\"], \"\", manager_data).strip()\n",
    "#     n = re.findall(name,manager_data, re.IGNORECASE)\n",
    "#     s = re.findall(since,manager_data, re.IGNORECASE)\n",
    "#     e = re.findall(exp,manager_data, re.IGNORECASE)\n",
    "    \n",
    "#     adjust = lambda target, lst: target[:len(lst)] + ([target[-1]] * abs(len(target) - len(lst)) if lst else [\"\"])\n",
    "#     n,s = adjust(n,e),adjust(s,e)\n",
    "#     for name,since,exp in zip(n,s,e):\n",
    "#         final_list.append(self._return_manager_data(name=name,since=since,exp=exp))\n",
    "#     return {main_key: final_list}\n",
    "\n",
    "# #sbI PASSIVE\n",
    "# def _update_manager_data(self,main_key:str,data):\n",
    "#     final_list = []\n",
    "#     manager_data = \" \".join(data) if isinstance(data, list) else data\n",
    "#     manager_data = re.sub(self.REGEX['escape'], \"\", manager_data).strip()\n",
    "#     pattern_info = self.REGEX[\"manager\"]\n",
    "#     regex_pattern = pattern_info['pattern']\n",
    "#     field_names = pattern_info['fields']\n",
    "#     if matches := re.findall(regex_pattern, manager_data, re.IGNORECASE):\n",
    "#         for match in matches:\n",
    "#             if isinstance(match, str):\n",
    "#                 match = (match,)\n",
    "#             record = {field_names[i]: match[i] if i < len(match) else \"\" for i in range(len(field_names))} #kwargs\n",
    "#             final_list.append(self._return_manager_data(**record))\n",
    "\n",
    "#     return {main_key: final_list}\n",
    "    \n",
    "# # ADITYA\n",
    "# def _update_manager_data(self,main_key:str,manager_data):\n",
    "#     nsample, msample, esample = [], [], []\n",
    "#     nlength = 0\n",
    "#     for key, value in manager_data.items():\n",
    "#         if re.search(r\"\\bfund_manager\\b\", key, re.IGNORECASE):\n",
    "#             nsample = re.findall(self.REGEX[\"manager\"][\"name\"], value, re.IGNORECASE)\n",
    "#             nlength = len(nsample)\n",
    "\n",
    "#         elif re.search(r\"^managing\", key, re.IGNORECASE):\n",
    "#             msample = re.findall(self.REGEX[\"manager\"][\"since\"], value, re.IGNORECASE)\n",
    "            \n",
    "#         elif re.search(r\"^experience\", key, re.IGNORECASE):\n",
    "#             esample = re.findall(self.REGEX[\"manager\"][\"exp\"], value, re.IGNORECASE)\n",
    "#     nlength = len(nsample)\n",
    "#     msample += [\"\"] * (nlength - len(msample))\n",
    "#     esample += [\"\"] * (nlength - len(esample))\n",
    "\n",
    "#     final_list = [\n",
    "#         self._return_manager_data(since=m,name=n,exp=e)\n",
    "#         for n, m, e in zip(nsample, msample, esample)\n",
    "#     ]\n",
    "\n",
    "#     return {main_key:final_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import camelot\n",
    "def get_something(path: str):\n",
    "    pattern = r\"^(REDEMPTION PROCEEDS|FEATURES|ASSET ALLOCATION|FUND MANAGER|SCHEME|Sr\\. No\\.)$\"\n",
    "\n",
    "    with fitz.open(path) as doc:\n",
    "        exists = defaultdict(int)\n",
    "        for pgn, page in enumerate(doc):\n",
    "            page_text = [t.strip() for t in page.get_text().split(\"\\n\")]\n",
    "            for text in page_text:\n",
    "                if re.match(pattern,text):\n",
    "                    exists[pgn]+=1\n",
    "                    \n",
    "    \n",
    "    return [str(pgn+1) for pgn, count in exists.items() if count > 4] #camelot starts pages from 1\n",
    "\n",
    "pages = get_something(sample_path)\n",
    "\n",
    "imp_pages = \",\".join(pages)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = camelot.read_pdf(sample_path,pages=imp_pages, flavor=\"stream\", table_areas= [\"30,50,612,812\"],column_tol = 4, split_text = True) #[\"30,50,612,812\"]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import camelot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def clean_column_name(x):\n",
    "    cleaned = re.sub(r\"[^A-Za-z0-9\\s]\", \"\", str(x), flags=re.IGNORECASE) #Corrected re.sub.\n",
    "    cleaned = \"_\".join(cleaned.split())\n",
    "    return cleaned\n",
    "\n",
    "with pd.ExcelWriter(\"cleaned_tables.xlsx\", engine=\"xlsxwriter\") as writer:\n",
    "    for i, table in enumerate(tables):\n",
    "        df = table.df\n",
    "        df = df.map(lambda x: \" \".join(str(x).split(\"\\n\")).strip())\n",
    "        df.replace(\"\", np.nan, inplace=True)\n",
    "        df.iloc[2:, 0] = df.iloc[2:, 0].ffill()\n",
    "        df = df.iloc[1:, :]\n",
    "        df.columns = df.iloc[0, :].apply(clean_column_name)\n",
    "        # print(df.columns)\n",
    "        df = df.iloc[1:, :]\n",
    "        sheet_name = f\"Table_{i+1}\"\n",
    "\n",
    "        # workbook = writer.book\n",
    "        # worksheet = writer.sheets[sheet_name]\n",
    "\n",
    "        # row = 1\n",
    "        # last_value = None\n",
    "        # start_row = None\n",
    "\n",
    "        # for j, value in enumerate(df.iloc[:, 0], start=1):\n",
    "        #     if pd.notna(value):  # New group found\n",
    "        #         if last_value is not None and start_row is not None:\n",
    "        #             worksheet.merge_range(start_row, 0, row - 1, 0, last_value)  # Merge previous block\n",
    "        #         last_value = value\n",
    "        #         start_row = row\n",
    "        #     row += 1\n",
    "\n",
    "        # # Merge last group\n",
    "        # if last_value is not None and start_row is not None:\n",
    "        #     worksheet.merge_range(start_row, 0, row - 1, 0, last_value)\n",
    "    \n",
    "        \n",
    "\n",
    "print(\"Cleaned tables saved with merged spanning rows!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'LIC MF LARGE CAP FUND': 'LIC MF LARGE CAP FUND',\n",
    " 'LIC MF LARGE & MID CAP FUND': 'LIC MF LARGE& MID CAPFUND',\n",
    " 'LIC MF MULTICAP FUND': 'LIC MF MULTICAP FUND',\n",
    " \"LIC MF MID CAP FUND\": \"LIC MF V't CAP FUND\", #18\n",
    " 'LIC MF SMALLCAP FUND': 'SMALLCAP FUND',\n",
    " 'LIC MF DIVIDEND YIELD FUND': 'LIC MF DIV_DEND YIELD FUND',\n",
    " 'LIC MF VALUE FUND': 'LIC MF VALUE FUND',\n",
    " 'LIC MF FOCUSED FUND': 'LIC MF FOCUSED FUND',\n",
    " 'LIC MF INFRASTRUCTURE FUND': 'LIC MF INFRASTRUCTURE FUND',\n",
    " 'LIC MF MANUFACTURING FUND': 'LIC MF Poin MANGFACTURING FUND',\n",
    " 'LIC MF BANKING & FINANCIAL SERVICES FUND': 'BANKING & FINANC-AL SERVICES FUND',\n",
    " 'LIC MF HEALTHCARE FUND': 'HEALTHCARE FUND',\n",
    " 'LIC MF ELSS TAX SAVER': 'LiC MF EL_SS TAX SAVER',\n",
    " 'LIC MF AGGRESSIVE HYBID FUND': 'LIC MF AGGRESSIVE HYBRiD FUND',\n",
    " 'LIC MF BALANCED ADVANTAGE FUND': 'LIC MF Bâ€™L*NCED ADVANTAGE FUND',\n",
    " 'LIC MF EQUITY SAVINGS FUND': 'LIC MF EQUITY SAVINGS FUND',\n",
    " 'LIC MF CONSERVATIVE HYBRID FUND': 'LIC MF CONSERWATIVE nip?!) FUND',\n",
    " 'LIC MF ARBITRAGE FUND': 'LIC MF ARBITRAGE FUND',\n",
    " 'LIC MF OVERNIGHT FUND': 'LIC MF OVERNIGHT FUND',\n",
    " 'LIC MF LIQUID FUND': 'LIC MF LIQUID FUND',\n",
    " 'LIC MF ULTRA SHORT DURATION FUND': 'LIC MF ULTRA SHORT DURATION FUND',\n",
    " 40: 'LIC MF LOWâ€™ DURATION FUND',\n",
    " 41: 'LIC MF MEDIUM-F2Â°LONG DURATION FUND',\n",
    " 42: 'LIC MF BANK&ONG & PSU FUND',\n",
    " 43: 'LIC MF_. SHORT DURATION FUND',\n",
    " 45: 'LIC MGI FUND',\n",
    " 46: 'LIC MF l HILDRENS FUND',\n",
    " 47: 'BSE SENSEX ETF',\n",
    " 48: 'LIC MF NIFTY 50ETF',\n",
    " 49: 'LIC MF NIFTY 100 ETF',\n",
    " 50: 'LIC MF NIFTY MIDCAP 100 ETF',\n",
    " 51: 'LIC MF NIFTY 8-13 YR G-SECETF',\n",
    " 52: 'LIC MF BSE SENSEX INDEX FUND',\n",
    " 53: 'LIC MF NIFTY 50 INDEX FUND',\n",
    " 54: 'LIC MF NIFTY NEXT 50 INDEX FUND',\n",
    " 55: 'LIC MF G2LD EXCHANGE TRADED FUND',\n",
    " 56: 'LIC MF GSLD ETF',\n",
    " 57: 'LIC MF Large Cap Fund',\n",
    " 58: 'LIC MF Mid cap Fund',\n",
    " 59: 'LIC MF Focused Fund',\n",
    " 60: 'Lic MF ELSs Tax Saver',\n",
    " 61: 'LIC MF Arbitrage Fund',\n",
    " 63: 'LIC MF Nifty Midcap 100 ETF',\n",
    " 64: 'LIC MF Large Cap Fund',\n",
    " 65: 'LIC MF Dividend Yield Fund',\n",
    " 66: 'LIC MEF Healthcare Fund',\n",
    " 67: 'LIC MF Unit Linked Insurance Scheme __LIC MF Overnight Fund',\n",
    " 68: 'LIC MF Short Duration Fund',\n",
    " 69: 'LIC MF BSE Sensex ETF',\n",
    " 70: 'LIC MF BSE Sensex Index Fund',\n",
    " 73: 'LIC MF Healthcare Fund',\n",
    " 74: 'LIC MF Liquid Fund',\n",
    " 75: 'LIC MF Nifty 50 ETF',\n",
    " 76: 'LIC Mutual Fund'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NIPPON DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def via_block(path:str):\n",
    "    pattern = r\"FUNDS AT A GLANCE\"\n",
    "    amc_pattern = \"^(Nippon India|CPSE).*(?=Plan|Next 50|Sensex|Fund|Path|ETF|FOF|EOF|Funds|$)\"\n",
    "    imp_pages = []\n",
    "    with fitz.open(path) as doc:\n",
    "        for pgn, page in enumerate(doc):\n",
    "                page_blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "                sorted_blocks = sorted(page_blocks, key=lambda x: (x[\"bbox\"][1], x[\"bbox\"][0]))\n",
    "                for block_count, block in enumerate(sorted_blocks[:10]):\n",
    "                    if \"lines\" not in block:\n",
    "                        continue\n",
    "                    for line in block[\"lines\"]:\n",
    "                        for span in line[\"spans\"]:\n",
    "                            text = span[\"text\"].strip()\n",
    "                            if re.match(pattern,text):\n",
    "                                imp_pages.append(pgn)\n",
    "                                \n",
    "        amc_fund = defaultdict(list)\n",
    "    \n",
    "        for pgn in imp_pages:\n",
    "            page = doc[pgn]\n",
    "            page_blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "            sorted_blocks = sorted(page_blocks, key=lambda x: (x[\"bbox\"][1], x[\"bbox\"][0]))\n",
    "            for block_count, block in enumerate(sorted_blocks):\n",
    "                if \"lines\" not in block:\n",
    "                    continue\n",
    "                for line in block[\"lines\"]:\n",
    "                    for span in line[\"spans\"]:\n",
    "                        text = span[\"text\"].strip()\n",
    "                        color = span['color']\n",
    "                        if re.match(amc_pattern,text)and color == -1:\n",
    "                            # matches = re.findall(amc_pattern,text)\n",
    "                            amc_fund[pgn].append(text)\n",
    "                            \n",
    "    return imp_pages, dict(amc_fund)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages,amc = via_block(sample_path)\n",
    "pages = list(map(str,[x+1 for x in pages]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_scheme = defaultdict(list)\n",
    "for key, value in amc.items():\n",
    "    # print(key)\n",
    "    set1 = ['Scheme Name']+value[:4]\n",
    "    set2 = ['Scheme Name']+value[4:]\n",
    "    final_scheme[key+1].append(set1)\n",
    "    final_scheme[key+1].append(set2)\n",
    "\n",
    "final_scheme = dict(final_scheme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_pages = \",\".join(pages)\n",
    "tables = camelot.read_pdf(sample_path,pages=imp_pages, flavor=\"lattice\", line_scale = 40)  #table_areas = [\"0,0,580,690\"]            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "with pd.ExcelWriter(\"merged_tables.xlsx\", engine=\"openpyxl\") as writer:\n",
    "    count = 0  # Toggle between 0 and 1\n",
    "    \n",
    "    for i, table in enumerate(tables):\n",
    "        df = table.df\n",
    "        if df.shape[1] < 3:\n",
    "            continue\n",
    "\n",
    "    \n",
    "        df = df.map(lambda x: \" \".join(x.split(\"\\n\")).strip())\n",
    "        df = df.map(lambda x: np.nan if not x.strip() else x)\n",
    "        df.set_index(df.columns[0], inplace=True)\n",
    "\n",
    "        for check in [\"Scheme Name\", \"Market Capitalization\"]:\n",
    "            if check in df.index:\n",
    "                df.drop(check, inplace=True)\n",
    "                \n",
    "        df_cleaned = df[~df.index.isna()]\n",
    "        df_cleaned = df_cleaned[df_cleaned.index != \"\"]\n",
    "        df_cleaned = df_cleaned.reset_index()\n",
    "        df_fill = df_cleaned.ffill(axis=1)\n",
    "\n",
    "\n",
    "        sch_vals = final_scheme[table.page][count]\n",
    "        count = 1 - count  # Toggle between 0 and 1\n",
    "\n",
    "        if len(sch_vals) == 5:\n",
    "            df_fill.loc[-1] = sch_vals \n",
    "            df_fill = df_fill.sort_index().reset_index(drop=True)\n",
    "\n",
    "        # Write to a new sheet\n",
    "        df_fill.to_excel(writer, sheet_name=f\"Table_{i+1}\", index=False)\n",
    "\n",
    "print(\"All tables saved in separate sheets in 'merged_tables.xlsx' ðŸš€\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hello =  {\n",
    "    \"number\": 0,\n",
    "    \"type\": 0,\n",
    "    \"bbox\": (0,0,0,0), #406.72119140625, 439.4930419921875, 565.697265625, 484.5830383300781\n",
    "    \"lines\": [\n",
    "        {\n",
    "            \"spans\": [\n",
    "                {\n",
    "                    \"size\": 30.0,\n",
    "                    \"flags\": 20,\n",
    "                    \"font\": \"Montserrat-Regular\", #set this\n",
    "                    \"color\": -1, #set this\n",
    "                    \"ascender\": 1.0429999828338623,\n",
    "                    \"descender\": -0.2619999945163727,\n",
    "                    \"text\": \"DUMMYDUMMYDUMMYDUMMY\",\n",
    "                    \"origin\": (406.72119140625, 458.26702880859375),\n",
    "                    \"bbox\": (0,0,0,0), #406.72119140625,439.4930419921875,565.697265625,462.9830322265625,\n",
    "                }\n",
    "            ],\n",
    "            \"wmode\": 0,\n",
    "            \"dir\": (1.0, 0.0),\n",
    "            \"bbox\": (0,0,0,0), #406.72119140625,439.4930419921875,565.697265625,462.9830322265625,\n",
    "        },\n",
    "        \n",
    "    ],\n",
    "},  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envPDF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
