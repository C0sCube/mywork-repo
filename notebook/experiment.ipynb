{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pprint, json, math, os, sys\n",
    "# dir_path = \"C:\\\\Users\\\\rando\\\\OneDrive\\\\Documents\\\\mywork-repo\"\n",
    "# fund_path = r\"C:\\Users\\rando\\OneDrive\\Documents\\PDFDrive\\Mar25\"\n",
    "# dry_path = r\"C:\\Users\\rando\\OneDrive\\Documents\\mywork-repo\\notebook\\DryRun.pdf\"\n",
    "\n",
    "dir_path = \"C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\office-work\\\\mywork-repo\"\n",
    "fund_path = r\"C:\\Users\\Kaustubh.keny\\Projects\\PDF\\Apr 25\"\n",
    "dry_path = r\"C:\\Users\\Kaustubh.keny\\Projects\\office-work\\mywork-repo\\notebook\\DryRun.pdf\"\n",
    "sys.path.append(os.path.abspath(dir_path))\n",
    "\n",
    "\n",
    "import fitz, pdfplumber, ocrmypdf, pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "from app.utils import Helper\n",
    "from app.parse_regex import *\n",
    "# from app.parse_table import *\n",
    "\n",
    "dry_path = r'DryRun.pdf'\n",
    "fin_path = r'\\data\\input\\financial_indices.xlsx'\n",
    "mutual_fund = Helper.get_fund_paths(fund_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\PDF\\\\Apr 25\\\\Axis Mutual Fund\\\\1_30-Apr-2025_1_FS.pdf'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutual_fund[\"Axis Mutual Fund\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Helper' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m bboxes = [[\u001b[32m0\u001b[39m, \u001b[32m120\u001b[39m, \u001b[32m180\u001b[39m, \u001b[32m812\u001b[39m],[\u001b[32m180\u001b[39m, \u001b[32m85\u001b[39m, \u001b[32m360\u001b[39m, \u001b[32m812\u001b[39m]] \u001b[38;5;66;03m#[(0, 85, 180, 812),(180, 85, 360, 812),(0,100,270,812),(0,100,350,812)]\u001b[39;00m\n\u001b[32m      8\u001b[39m pages = [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m,\u001b[32m110\u001b[39m)]\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43mHelper\u001b[49m.draw_lines_on_pdf(sample_path, lines, bboxes, pages, dry_path)\n",
      "\u001b[31mNameError\u001b[39m: name 'Helper' is not defined"
     ]
    }
   ],
   "source": [
    "lines = [\n",
    "    ((110, 0), (110, 812)),# Vertical line\n",
    "    ((0, 350), (812, 350)),\n",
    "    ((570, 0), (570, 812))\n",
    "]\n",
    "pages = [12, 14,16]\n",
    "bboxes = [[0, 120, 180, 812],[180, 85, 360, 812]] #[(0, 85, 180, 812),(180, 85, 360, 812),(0,100,270,812),(0,100,350,812)]\n",
    "pages = [i for i in range(1,110)]\n",
    "Helper.draw_lines_on_pdf(sample_path, lines, bboxes, pages, dry_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Bounding box (32.582500000000095, 63.25600000000033, 558.3098000000001, 733.772) not found in table_bbox_parses.\n",
      "Warning: Bounding box (32.582500000000095, 63.25600000000033, 558.3098000000001, 733.772) not found in table_bbox_parses.\n"
     ]
    }
   ],
   "source": [
    "axis_apr = r\"C:\\Users\\Kaustubh.keny\\Projects\\PDF\\Apr 25\\Axis Mutual Fund\\1_30-Apr-25_FS.pdf\"\n",
    "table_parser = TableParser()\n",
    "tables = camelot.read_pdf(axis_apr,flavor=\"hybrid\",pages=\"122-123\")\n",
    "dfs = pd.concat([table.df for table in tables], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.columns = [\"MUTUAL_FUND\",\"MIN_APP\",\"ADD_APP\"]+ [f\"OTHER_{i}\" for i in range(1,(dfs.shape[1]-3)+1)]\n",
    "axis = re.compile(\n",
    "    \"axis.+\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "fdf= dfs\n",
    "dfs.MUTUAL_FUND = table_parser.clean_series(dfs.MUTUAL_FUND,[\"normalize_alphanumeric\"])\n",
    "fdf.MUTUAL_FUND = dfs.MUTUAL_FUND.apply(lambda x: axis.findall(x)[0] if isinstance(x, str) and axis.findall(x) else \"\")\n",
    "fdf = table_parser.clean_dataframe(dfs,[\"newline_to_space\",\"str_to_pd_NA\"])\n",
    "fdf = fdf.dropna(axis=0, how=\"all\").dropna(axis=1, how=\"all\")\n",
    "fdf =table_parser.clean_dataframe(fdf,['NA_to_str'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NIPPON\n",
    "# nip_jan = r\"C:\\Users\\Kaustubh.keny\\Projects\\PDF\\Jan 25\\Nippon India Mutual Fund\\33_31-Jan-25_FS.pdf\"\n",
    "# nip_feb = \"\"\n",
    "# nip_mar = r\"C:\\Users\\Kaustubh.keny\\Projects\\PDF\\Mar 25\\Nippon India Mutual Fund\\33_31-Mar-25_FS.pdf\"\n",
    "# nip_apr = r\"C:\\Users\\Kaustubh.keny\\Projects\\PDF\\Apr 25\\Nippon India Mutual Fund\\33_30-Apr-25_FS.pdf\"\n",
    "# table_parser = TableParser()\n",
    "# tables = camelot.read_pdf(nip_apr,flavor=\"stream\",pages=\"129-140\")\n",
    "# dfs = pd.concat([table.df for table in tables], ignore_index=True)\n",
    "\n",
    "# sr1 = table_parser._get_matching_row_indices(dfs,[\"Nippon.+?Fund\",\"Scheme\\\\s*Name\"],thresh=2)\n",
    "# sr2 = table_parser._get_matching_row_indices(dfs,[\"minimum application\"],thresh=1)\n",
    "# sr2_expanded = {i for idx in sr2 for i in range(idx, idx + 5)}\n",
    "# sr1_expanded = {i for idx in sr1 for i in range(idx, idx + 1)}\n",
    "\n",
    "# all_indices = sr1_expanded | sr2_expanded\n",
    "# valid_indices = sorted(i for i in all_indices if i in dfs.index)\n",
    "\n",
    "# filtered_df = dfs.loc[valid_indices].reset_index()\n",
    "# for idx, rows in filtered_df.iterrows():\n",
    "#     row_val = \" \".join([str(i) for i in rows])\n",
    "#     row_val = SidKimRegex()._normalize_alphanumeric(row_val)\n",
    "#     # print(row_val)\n",
    "#     matches = re.findall(r\"Nippon.+?(?:Funds?|ETF|Path|Saver|active|financial|allocation|tunities|duration|psu debt|advantage|small cap 250)\\s*(?:of Funds?|Fund of Funds?|Funds?|.+?Plan|FoF)?\",row_val, re.IGNORECASE)\n",
    "#     if matches:\n",
    "#         print(idx, len(matches))\n",
    "#         print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BAJAJAJ\n",
    "# jan = r\"C:\\Users\\Kaustubh.keny\\Projects\\PDF\\Jan 25\\Bajaj finserv Mutual Fund\\59_31-Jan-25_FS.pdf\"\n",
    "# feb = r\"C:\\Users\\Kaustubh.keny\\Projects\\PDF\\Feb 25\\Bajaj finserv Mutual Fund\\59_28-Feb-25_FS.pdf\"\n",
    "# mar = r\"C:\\Users\\Kaustubh.keny\\Projects\\PDF\\Mar 25\\Bajaj finserv Mutual Fund\\59_31-Mar-25_FS.pdf\"\n",
    "# apr = r\"C:\\Users\\Kaustubh.keny\\Projects\\PDF\\Apr 25\\Bajaj finserv Mutual Fund\\59_30-Apr-25_FS.pdf\"\n",
    "\n",
    "\n",
    "# table_parser = TableParser()\n",
    "# tables = camelot.read_pdf(apr,flavor=\"hybrid\",pages=\"11-14\")\n",
    "# dfs = pd.concat([table.df for table in tables], ignore_index=True)\n",
    "# sc1 = table_parser.get_matching_col_indices(dfs,[\"Bajaj.+?Fund\",\"SCHEME\\\\s*NAME\"],thresh=20)\n",
    "# sc2 = table_parser.get_matching_col_indices(dfs,[\"Jensen\",\"Standard\\\\s*Deviation\",\"Information\\\\s*ratio\",\"Portfolio\\\\s*Quants\",\"Tracking Error\",\"YTM\",\"Average\\\\s*Maturity\",\"Sharpe\"],thresh=10)\n",
    "# sc3 = table_parser.get_matching_col_indices(dfs,[\"Average\\\\s*Maturity\",\"Modified Duration\",\"Macaulay Duration\",\"YTM\"], thresh=8)\n",
    "# all_cols = sorted(set(sc1)) + [sc2[0],sc2[0]+1, sc3[0],sc3[0]+1]\n",
    "# fdf = dfs.iloc[:, all_cols]\n",
    "# fdf.columns = [\"MUTUAL_FUND\"] + [f\"METRICS_{i}\" for i in range(1, fdf.shape[1])]\n",
    "# bajaj = re.compile(\n",
    "#     r\"(Bajaj?\\\\s*finserv.+?)\",\n",
    "#     re.IGNORECASE\n",
    "# )\n",
    "# fdf.MUTUAL_FUND = table_parser.clean_series(fdf.MUTUAL_FUND,[\"normalize_alphanumeric\"])\n",
    "# fdf.MUTUAL_FUND = fdf.MUTUAL_FUND.apply(lambda x: bajaj.findall(x)[0] if isinstance(x, str) and bajaj.findall(x) else \"\")\n",
    "# fdf = table_parser.clean_dataframe(fdf,[\"newline_to_space\",\"str_to_pd_NA\"])\n",
    "# fdf = fdf.dropna(axis=0, how=\"all\").dropna(axis=1, how=\"all\")\n",
    "# fdf =table_parser.clean_dataframe(fdf,['NA_to_str'])\n",
    "\n",
    "# data = {}\n",
    "# for idx, rows in fdf.iterrows():\n",
    "#     values = list(rows)\n",
    "#     main_scheme_name = str(values[0]).strip() if not pd.isna(values[0]) else \"\"\n",
    "#     if main_scheme_name:\n",
    "#         temp = main_scheme_name\n",
    "#         if temp not in data:\n",
    "#             data[temp] = {\"metrics\": []}\n",
    "#         data[temp][\"metrics\"].append(\" \".join(map(str, values[1:])))\n",
    "    \n",
    "#     if temp:\n",
    "#         data[temp][\"metrics\"].append(\" \".join(map(str, values)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #HDFC\n",
    "# table_parser = TableParser()\n",
    "# tables = camelot.read_pdf(hdfc1,flavor=\"lattice\",pages=\"91-93\")\n",
    "# dfs = pd.concat([table.df for table in tables], ignore_index=True)\n",
    "# sc1 = table_parser._get_matching_col_indices(dfs,[\"HDFC.+?Fund\"],thresh=20)\n",
    "# sc2 = table_parser._get_matching_col_indices(dfs,[\"MINIMUM\\\\s*APPLICATION\\\\s*AMOUNT\",\"Additional\\\\s*Purchase\"], thresh=20)\n",
    "\n",
    "# print(\"Matched columns:\", sc1,sc2)\n",
    "# all_cols = list(set(sc1 + sc2))\n",
    "# fdf = dfs.iloc[:, all_cols]\n",
    "# fdf.columns = [\"MUTUAL_FUND\",\"MIN_ADD\"]\n",
    "# hdfc_pattern = re.compile(\n",
    "#     r\"(HDFC.+?(?:FUNDS?|ETF|PATH|INDEX|SAVER)\\s*(?:OF FUNDS?|FUND OF FUNDS|FOF|.+?PLAN)?)\",\n",
    "#     re.IGNORECASE\n",
    "# )\n",
    "# fdf.MUTUAL_FUND = table_parser._clean_series(fdf.MUTUAL_FUND,[\"normalize_alphanumeric\"])\n",
    "# fdf.MUTUAL_FUND = fdf.MUTUAL_FUND.apply(lambda x: hdfc_pattern.findall(x)[0] if isinstance(x, str) and hdfc_pattern.findall(x) else x)\n",
    "# fdf = table_parser._clean_dataframe(fdf,[\"newline_to_space\",\"str_to_pd_NA\"])\n",
    "# fdf = fdf.dropna(axis=0, how=\"all\").dropna(axis=1, how=\"all\")\n",
    "# fdf =table_parser._clean_dataframe(fdf,['NA_to_str'])\n",
    "\n",
    "# data = {}\n",
    "# for idx, rows in fdf.iterrows():\n",
    "#     values = list(rows)\n",
    "#     main_scheme_name = values[0]\n",
    "#     if main_scheme_name not in data:\n",
    "#         data[main_scheme_name] = {\"min_add\":values[1]}\n",
    "#     else:\n",
    "#         data[main_scheme_name].update({\"min_add_one\":values[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #DSP\n",
    "# jan = r\"C:\\Users\\Kaustubh.keny\\Projects\\PDF\\Jan 25\\DSP Mutual Fund\\8_31-Jan-25_FS.pdf\"\n",
    "# feb = r\"C:\\Users\\Kaustubh.keny\\Projects\\PDF\\Feb 25\\DSP Mutual Fund\\8_28-Feb-25_FS.pdf\"\n",
    "# mar = r\"C:\\Users\\Kaustubh.keny\\Projects\\PDF\\Mar 25\\DSP Mutual Fund\\8_31-Mar-25_FS.pdf\"\n",
    "# apr = r\"C:\\Users\\Kaustubh.keny\\Projects\\PDF\\Apr 25\\DSP Mutual Fund\\8_30-Apr-25_FS.pdf\"\n",
    "\n",
    "# table_parser = TableParser()\n",
    "# tables = camelot.read_pdf(apr,flavor=\"lattice\",pages=\"107-123\")\n",
    "# dfs = pd.concat([table.df for table in tables], ignore_index=True)\n",
    "# sc1 = table_parser._get_matching_col_indices(dfs,[\"DSP.+?Fund\"],thresh=12)\n",
    "# sc2 = table_parser._get_matching_col_indices(dfs,[\"REGULAR\\\\s+PLAN\",\"DIRECT\\\\s+PLAN\"], thresh=12)\n",
    "# sc3 = table_parser._get_matching_col_indices(dfs,[\"Managing this scheme\",\"total work experience\"],thresh=12)\n",
    "# print(\"Matched columns:\", sc1,sc2,sc3)\n",
    "# all_cols = list(set(sc1 + sc2 + sc3))\n",
    "# fdf = dfs.iloc[:, all_cols]\n",
    "# fdf[\"LOAD_STRUCTURE\"] = fdf.iloc[:, -1]\n",
    "# fdf.columns = [\"MUTUAL_FUND\",\"FUND_MANAGER\",\"MIN_ADD\",\"LOAD_STRUCTURE\"]\n",
    "\n",
    "# dsp_pattern = re.compile(\n",
    "#     r\"(DSP.+?(?:FUNDS?|ETF|PATH|INDEX|SAVER)\\s*(?:OF FUNDS?|FUNDs?|FUND OF FUNDS?|FOF|.+?PLAN)?)\",\n",
    "#     re.IGNORECASE\n",
    "# )\n",
    "\n",
    "# fdf.MUTUAL_FUND = table_parser._clean_series(fdf.MUTUAL_FUND,[\"normalize_alphanumeric\"])\n",
    "# fdf.MUTUAL_FUND = fdf.MUTUAL_FUND.apply(lambda x: dsp_pattern.findall(x)[0] if isinstance(x, str) and dsp_pattern.findall(x) else pd.NA)\n",
    "# fdf = table_parser._clean_dataframe(fdf,[\"newline_to_space\",\"str_to_pd_NA\"])\n",
    "# fdf = fdf.dropna(axis=0, how=\"all\").dropna(axis=1, how=\"all\")\n",
    "# fdf =table_parser._clean_dataframe(fdf,['NA_to_str'])\n",
    "\n",
    "# data = {}\n",
    "# for idx, rows in fdf.iterrows():\n",
    "#     values = list(rows)\n",
    "#     main_scheme_name = values[0]\n",
    "#     if main_scheme_name not in data:\n",
    "#         data[main_scheme_name] = {\"fund_manager\":values[1],\"load_structure\":values[3],\"min_add\":values[2]}\n",
    "#     else:\n",
    "#          data[main_scheme_name].update({\"fund_manager_one\":values[1],\"load_structure_one\":values[3],\"min_add_one\":values[2]})\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "def get_proper_fund_names(path: str):\n",
    "    title = {}\n",
    "    pattern = \"(LIFE\\\\sINS.+?)\\\\s*ULIF\"\n",
    "    with fitz.open(path) as doc:\n",
    "        for pgn, page in enumerate(doc):\n",
    "            clip = fitz.Rect(206.87, 45.29, 468.83, 88.14)\n",
    "            pix = page.get_pixmap(clip=clip, dpi=300)\n",
    "            img = Image.open(io.BytesIO(pix.tobytes()))\n",
    "            text = pytesseract.image_to_string(img)\n",
    "            cleaned = re.sub(r\"[^A-Za-z0-9\\s\\.,\\-()]+\", \"\", text).strip()\n",
    "            if matches := re.findall(pattern, cleaned, re.DOTALL):\n",
    "                title[pgn] = \" \".join([_ for _ in matches[0].strip().split(\" \") if _])\n",
    "                print(pgn,matches[0])\n",
    "            # print(f\"[OCR] Page {pgn}: {cleaned}\")\n",
    "            # if cleaned:\n",
    "            #     title[pgn] = cleaned\n",
    "    return title\n",
    "\n",
    "\n",
    "path = r\"C:\\Users\\kaustubh.keny\\Downloads\\ULIP Periodical Disclosure as on 30.06.2025_compressed.pdf\"\n",
    "title = get_proper_fund_names(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proper_fund_names(path: str):\n",
    "    pattern = \"(Scheme\\\\s*Type.+$)\"\n",
    "    title = {} \n",
    "    with fitz.open(path) as doc:\n",
    "        for pgn, page in enumerate(doc):\n",
    "            text = \" \".join(page.get_text(\"text\", clip=(0,0,200,140)).split(\"\\n\"))\n",
    "            text = re.sub(\"[^A-Za-z0-9\\\\s\\\\.,\\\\-\\\\(\\\\)\\\\+\\\\%\\\\:\\\\&]+\", \"\", text).strip()\n",
    "            # print(text)\n",
    "            if matches := re.findall(pattern, text, re.DOTALL):\n",
    "                title[pgn] = \" \".join([_ for _ in matches[0].strip().split(\" \") if _])\n",
    "                print(pgn,matches[0])\n",
    "    return title\n",
    "path = r\"C:\\Users\\kaustubh.keny\\OneDrive - Cogencis Information Services Ltd\\Documents\\2020\\LIC Mutual Fund\\25_30-Sep-20_FS.pdf\"\n",
    "title = get_proper_fund_names(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalize_alphanumeric(text: str) -> str:\n",
    "    if not isinstance(text,str):\n",
    "        return text\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]+\", \" \", str(text))\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip().lower()\n",
    "\n",
    "def extract_clipped_data(input:str, pages:list, bboxes:list):\n",
    "        \n",
    "        document = fitz.open(input)\n",
    "        final_list = []\n",
    "    \n",
    "        for pgn in pages:\n",
    "            page = document[pgn]\n",
    "            \n",
    "            all_blocks = [] #store every data from bboxes\n",
    "            \n",
    "            for bbox in bboxes:\n",
    "                blocks, seen_blocks = [], set()  #store unique blocks based on content and bbox\n",
    "                \n",
    "                page_blocks = page.get_text('dict', clip=bbox)['blocks']\n",
    "                for block in page_blocks:\n",
    "                    if block['type'] == 0 and 'lines' in block: #type 0 means text block\n",
    "                        #hash_key\n",
    "                        block_key = (tuple(block['bbox']), tuple(tuple(line['spans'][0]['text'] for line in block['lines'])))\n",
    "                        if block_key not in seen_blocks:\n",
    "                            seen_blocks.add(block_key)\n",
    "                            blocks.append(block)\n",
    "\n",
    "                sorted_blocks = sorted(blocks, key=lambda x: (x['bbox'][1], x['bbox'][0]))\n",
    "                all_blocks.append(sorted_blocks)\n",
    "\n",
    "            final_list.append({\n",
    "                \"pgn\": pgn,\n",
    "                \"block\": all_blocks #will be list[list,list,..]\n",
    "            })\n",
    "\n",
    "        document.close()\n",
    "        return final_list\n",
    "    \n",
    "def extract_data_relative_line(path: str, line_x: float, side: str):\n",
    "    doc = fitz.open(path)\n",
    "    pages = doc.page_count\n",
    "\n",
    "    final_list = []\n",
    "\n",
    "    for pgn in range(pages):\n",
    "        page = doc[pgn]\n",
    "\n",
    "        blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "        sorted_blocks = sorted(blocks, key=lambda x: (x[\"bbox\"][1], x[\"bbox\"][0]))\n",
    "        extracted_blocks = []\n",
    "\n",
    "        # Keep track of blocks to avoid duplicates\n",
    "        added_blocks = set()\n",
    "\n",
    "        for block in sorted_blocks:\n",
    "            block_id = id(block)  # Unique identifier for the block\n",
    "\n",
    "            for line in block.get(\"lines\", []):\n",
    "                for span in line.get(\"spans\", []):\n",
    "                    origin = span[\"origin\"]\n",
    "                    x0, _ = origin\n",
    "\n",
    "                    # Check the side condition\n",
    "                    if side == \"left\" and x0 < line_x and block_id not in added_blocks:\n",
    "                        extracted_blocks.append(block)\n",
    "                        added_blocks.add(block_id)  # Mark block as added\n",
    "                    elif side == \"right\" and x0 > line_x and block_id not in added_blocks:\n",
    "                        extracted_blocks.append(block)\n",
    "                        added_blocks.add(block_id)  # Mark block as added\n",
    "\n",
    "      \n",
    "        final_list.append({\n",
    "            \"pgn\": pgn,\n",
    "            \"blocks\": extracted_blocks\n",
    "        })\n",
    "\n",
    "    doc.close()\n",
    "\n",
    "    return final_list\n",
    "  \n",
    "def get_clipped_data(input:str, bboxes:list[set], *args):\n",
    "    \n",
    "        document = fitz.open(input)\n",
    "        final_list = []\n",
    "        if args:\n",
    "            pages = list(args)\n",
    "        else:\n",
    "            pages = [i for i in document.page_count]\n",
    "        \n",
    "        for pgn in pages:\n",
    "            page = document[pgn]\n",
    "\n",
    "            blocks = []\n",
    "            for bbox in bboxes:\n",
    "                blocks.extend(page.get_text('dict', clip = bbox)['blocks']) #get all blocks\n",
    "            \n",
    "            filtered_blocks = [block for block in blocks if block['type']== 0 and 'lines' in block]\n",
    "            # sorted_blocks = sorted(filtered_blocks, key= lambda x: (x['bbox'][1], x['bbox'][0]))\n",
    "            \n",
    "             # Extract text from sorted blocks\n",
    "            extracted_text = []\n",
    "            for block in filtered_blocks:\n",
    "                block_text = []\n",
    "                for line in block['lines']:\n",
    "                    line_text = \" \".join(span['text'] for span in line['spans'])\n",
    "                    block_text.append(line_text)\n",
    "                extracted_text.append(\"\\n\".join(block_text))\n",
    "            \n",
    "            final_list.append({\n",
    "            \"pgn\": pgn,\n",
    "            \"block\": filtered_blocks,\n",
    "            \"text\": extracted_text\n",
    "            })\n",
    "            \n",
    "            \n",
    "        document.close()\n",
    "        return final_list\n",
    "    \n",
    "def extract_clipped_text_all_pages(pdf_path, clip_coords):\n",
    "    results = {}\n",
    "    doc = fitz.open(pdf_path)\n",
    "    clip_rect = fitz.Rect(*clip_coords)\n",
    "    try:\n",
    "        for page_number, page in enumerate(doc):\n",
    "            text = page.get_text(\"text\", clip=clip_rect).strip()\n",
    "            results[page_number] = text\n",
    "    finally:\n",
    "        doc.close()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Helper.get_all_pdf_data(mutual_fund[\"Quant Mutual Fund\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
