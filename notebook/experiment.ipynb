{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint, json, math, os, sys\n",
    "import fitz, pdfplumber, ocrmypdf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "dir_path = \"C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\office-work\\\\mywork-repo\"\n",
    "fund_path = \"C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\Jan 25\"\n",
    "\n",
    "# dir_path = \"C:\\\\Users\\\\rando\\\\OneDrive\\\\Documents\\\\mywork-repo\"\n",
    "# fund_path =  \"C:\\\\Users\\\\rando\\\\OneDrive\\\\Documents\\\\Jan 25\"\n",
    "sys.path.append(os.path.abspath(dir_path))\n",
    "\n",
    "from app.utils import Helper\n",
    "from app.parse_regex import *\n",
    "\n",
    "dry_path = r'\\data\\output\\DryRun.pdf'\n",
    "fin_path = r'\\data\\input\\financial_indices.xlsx'\n",
    "mutual_fund = Helper.get_fund_paths(fund_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_clipped_data(input:str, pages:list, bboxes:list):\n",
    "        \n",
    "        document = fitz.open(input)\n",
    "        final_list = []\n",
    "    \n",
    "        for pgn in pages:\n",
    "            page = document[pgn]\n",
    "            \n",
    "            all_blocks = [] #store every data from bboxes\n",
    "            \n",
    "            for bbox in bboxes:\n",
    "                blocks, seen_blocks = [], set()  #store unique blocks based on content and bbox\n",
    "                \n",
    "                page_blocks = page.get_text('dict', clip=bbox)['blocks']\n",
    "                for block in page_blocks:\n",
    "                    if block['type'] == 0 and 'lines' in block: #type 0 means text block\n",
    "                        #hash_key\n",
    "                        block_key = (tuple(block['bbox']), tuple(tuple(line['spans'][0]['text'] for line in block['lines'])))\n",
    "                        if block_key not in seen_blocks:\n",
    "                            seen_blocks.add(block_key)\n",
    "                            blocks.append(block)\n",
    "\n",
    "                sorted_blocks = sorted(blocks, key=lambda x: (x['bbox'][1], x['bbox'][0]))\n",
    "                all_blocks.append(sorted_blocks)\n",
    "\n",
    "            final_list.append({\n",
    "                \"pgn\": pgn,\n",
    "                \"block\": all_blocks #will be list[list,list,..]\n",
    "            })\n",
    "\n",
    "        document.close()\n",
    "        return final_list\n",
    "    \n",
    "def extract_data_relative_line(path: str, line_x: float, side: str):\n",
    "    doc = fitz.open(path)\n",
    "    pages = doc.page_count\n",
    "\n",
    "    final_list = []\n",
    "\n",
    "    for pgn in range(pages):\n",
    "        page = doc[pgn]\n",
    "\n",
    "        blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "        sorted_blocks = sorted(blocks, key=lambda x: (x[\"bbox\"][1], x[\"bbox\"][0]))\n",
    "        extracted_blocks = []\n",
    "\n",
    "        # Keep track of blocks to avoid duplicates\n",
    "        added_blocks = set()\n",
    "\n",
    "        for block in sorted_blocks:\n",
    "            block_id = id(block)  # Unique identifier for the block\n",
    "\n",
    "            for line in block.get(\"lines\", []):\n",
    "                for span in line.get(\"spans\", []):\n",
    "                    origin = span[\"origin\"]\n",
    "                    x0, _ = origin\n",
    "\n",
    "                    # Check the side condition\n",
    "                    if side == \"left\" and x0 < line_x and block_id not in added_blocks:\n",
    "                        extracted_blocks.append(block)\n",
    "                        added_blocks.add(block_id)  # Mark block as added\n",
    "                    elif side == \"right\" and x0 > line_x and block_id not in added_blocks:\n",
    "                        extracted_blocks.append(block)\n",
    "                        added_blocks.add(block_id)  # Mark block as added\n",
    "\n",
    "      \n",
    "        final_list.append({\n",
    "            \"pgn\": pgn,\n",
    "            \"blocks\": extracted_blocks\n",
    "        })\n",
    "\n",
    "    doc.close()\n",
    "\n",
    "    return final_list\n",
    "  \n",
    "def get_clipped_data(input:str, bboxes:list[set], *args):\n",
    "    \n",
    "        document = fitz.open(input)\n",
    "        final_list = []\n",
    "        if args:\n",
    "            pages = list(args)\n",
    "        else:\n",
    "            pages = [i for i in document.page_count]\n",
    "        \n",
    "        for pgn in pages:\n",
    "            page = document[pgn]\n",
    "\n",
    "            blocks = []\n",
    "            for bbox in bboxes:\n",
    "                blocks.extend(page.get_text('dict', clip = bbox)['blocks']) #get all blocks\n",
    "            \n",
    "            filtered_blocks = [block for block in blocks if block['type']== 0 and 'lines' in block]\n",
    "            # sorted_blocks = sorted(filtered_blocks, key= lambda x: (x['bbox'][1], x['bbox'][0]))\n",
    "            \n",
    "             # Extract text from sorted blocks\n",
    "            extracted_text = []\n",
    "            for block in filtered_blocks:\n",
    "                block_text = []\n",
    "                for line in block['lines']:\n",
    "                    line_text = \" \".join(span['text'] for span in line['spans'])\n",
    "                    block_text.append(line_text)\n",
    "                extracted_text.append(\"\\n\".join(block_text))\n",
    "            \n",
    "            final_list.append({\n",
    "            \"pgn\": pgn,\n",
    "            \"block\": filtered_blocks,\n",
    "            \"text\": extracted_text\n",
    "            })\n",
    "            \n",
    "            \n",
    "        document.close()\n",
    "        return final_list\n",
    "    \n",
    "def get_clipped_text(input:str, bboxes:list[set],*args):\n",
    "\n",
    "    document = fitz.open(input)\n",
    "    final_list = []\n",
    "    \n",
    "    if args:\n",
    "        pages = list(args)\n",
    "    else:\n",
    "        pages = [i for i in document.page_count]\n",
    "    \n",
    "    for pgn in pages:\n",
    "        page = document[pgn]\n",
    "        blocks = []\n",
    "        for bbox in bboxes:\n",
    "            blocks = page.get_text('text', clip = bbox).split('\\n') #get all blocks\n",
    "        final_list.append({\n",
    "        \"pgn\": pgn,\n",
    "        \"block\": blocks\n",
    "        })   \n",
    "    document.close()\n",
    "    return final_list\n",
    "\n",
    "def get_proper_fund_names(path: str, pages: list):\n",
    "    doc = fitz.open(path)\n",
    "    title = {}\n",
    "\n",
    "    for pgn in pages:\n",
    "        page = doc[pgn]\n",
    "        blocks = page.get_text(\"dict\")['blocks']\n",
    "        text_all = \" \".join(\n",
    "            span[\"text\"].strip()\n",
    "            for block in blocks[:4]\n",
    "            for line in block.get(\"lines\", [])\n",
    "            for span in line.get(\"spans\", [])\n",
    "            if span[\"text\"].strip()\n",
    "        )\n",
    "\n",
    "        text_all = re.sub(r'[^A-Za-z0-9\\s]+', '', text_all).strip()\n",
    "        matches = re.findall(r\"((?:LIC\\s*MF|BLNCED|LOW)\\s+.*?(?:FUND|ETF|FTF|FOF|PLAN|SAVER))\", text_all, re.IGNORECASE)\n",
    "\n",
    "        title[pgn] = matches[0] if matches else \"\"\n",
    "\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'360 ONE Mutual Fund': 'C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\Jan 25\\\\360 ONE Mutual Fund\\\\18_31-Jan-25_FS.pdf',\n",
       " 'Aditya Birla Sun Life Mutual Fund': 'C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\Jan 25\\\\Aditya Birla Sun Life Mutual Fund\\\\3_31-Jan-25_FS.pdf',\n",
       " 'Axis Mutual Fund': 'C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\Jan 25\\\\Axis Mutual Fund\\\\1_31-Jan-2025_1_FS.pdf',\n",
       " 'Bajaj finserv Mutual Fund': 'C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\Jan 25\\\\Bajaj finserv Mutual Fund\\\\59_31-Jan-25_FS.pdf',\n",
       " 'Bandhan Mutual Fund': 'C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\Jan 25\\\\Bandhan Mutual Fund\\\\16_31-Jan-25_FS.pdf',\n",
       " 'Bank of India Mutual Fund': 'C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\Jan 25\\\\Bank of India Mutual Fund\\\\5_31-Jan-25_FS.pdf',\n",
       " 'Baroda BNP Paribas Mutual Fund': 'C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\Jan 25\\\\Baroda BNP Paribas Mutual Fund\\\\2_31-Jan-25_FS.pdf',\n",
       " 'Canara Robeco Mutual Fund': 'C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\Jan 25\\\\Canara Robeco Mutual Fund\\\\6_31-Jan-25_FS.pdf',\n",
       " 'DSP Mutual Fund': 'C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\Jan 25\\\\DSP Mutual Fund\\\\8_31-Jan-25_FS.pdf',\n",
       " 'Edelweiss Mutual Fund': 'C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\Jan 25\\\\Edelweiss Mutual Fund\\\\9_31-Jan-25_FS.pdf',\n",
       " 'Franklin Templeton Mutual Fund': 'C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\Jan 25\\\\Franklin Templeton Mutual Fund\\\\11_31-Jan-25_FS.pdf',\n",
       " 'Groww Mutual Fund': 'C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\Jan 25\\\\Groww Mutual Fund\\\\20_31-Jan-25_FS.pdf',\n",
       " 'HDFC Mutual Fund': 'C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\Jan 25\\\\HDFC Mutual Fund\\\\12_31-Jan-2025_1_FS.pdf',\n",
       " 'Helios Mutual Fund': 'C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\Jan 25\\\\Helios Mutual Fund\\\\60_31-Jan-25_FS.pdf',\n",
       " 'HSBC Mutual Fund': 'C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\Jan 25\\\\HSBC Mutual Fund\\\\13_31-Jan-25_FS.pdf',\n",
       " 'ICICI Prudential Mutual Fund': 'C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\Jan 25\\\\ICICI Prudential Mutual Fund\\\\14_31-Jan-2025_1_FS.pdf',\n",
       " 'Invesco Mutual Fund': 'C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\Jan 25\\\\Invesco Mutual Fund\\\\21_31-Jan-25_FS.pdf',\n",
       " 'ITI Mutual Fund': 'C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\Jan 25\\\\ITI Mutual Fund\\\\51_31-Jan-25_FS.pdf',\n",
       " 'JM Financial Mutual Fund': 'C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\Jan 25\\\\JM Financial Mutual Fund\\\\22_31-Jan-25_FS.pdf',\n",
       " 'Kotak Mahindra Mutual Fund': 'C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\Jan 25\\\\Kotak Mahindra Mutual Fund\\\\23_31-Jan-25_FS.pdf',\n",
       " 'LIC Mutual Fund': 'C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\Jan 25\\\\LIC Mutual Fund\\\\25_31-Jan-25_FS.pdf',\n",
       " 'Mahindra Manulife Mutual Fund': 'C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\Jan 25\\\\Mahindra Manulife Mutual Fund\\\\26_31-Jan-25_FS.pdf',\n",
       " 'Mirae Asset Mutual Fund': 'C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\Jan 25\\\\Mirae Asset Mutual Fund\\\\27_31-Jan-25_1_FS.pdf',\n",
       " 'Motilal Oswal Mutual Fund': 'C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\Jan 25\\\\Motilal Oswal Mutual Fund\\\\28_31-Jan-2025_1_FS.pdf',\n",
       " 'Navi Mutual Fund': 'C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\Jan 25\\\\Navi Mutual Fund\\\\56_31-Jan-2025_1_FS.pdf',\n",
       " 'Nippon India Mutual Fund': 'C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\Jan 25\\\\Nippon India Mutual Fund\\\\33_31-Jan-25_FS.pdf',\n",
       " 'NJ Mutual Fund': 'C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\Jan 25\\\\NJ Mutual Fund\\\\57_31-Jan-25_FS.pdf',\n",
       " 'Old Bridge Mutual Fund': 'C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\Jan 25\\\\Old Bridge Mutual Fund\\\\95_31-Jan-25_FS.pdf',\n",
       " 'PGIM India Mutual Fund': 'C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\Jan 25\\\\PGIM India Mutual Fund\\\\7_31-Jan-25_FS.pdf',\n",
       " 'PPFAS Mutual Fund': 'C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\Jan 25\\\\PPFAS Mutual Fund\\\\30_31-Jan-25_FS.pdf',\n",
       " 'Quant Mutual Fund': 'C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\Jan 25\\\\Quant Mutual Fund\\\\10_31-Jan-25_FS.pdf',\n",
       " 'Quantum Mutual Fund': 'C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\Jan 25\\\\Quantum Mutual Fund\\\\32_31-Jan-25_FS.pdf',\n",
       " 'Samco Mutual Fund': 'C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\Jan 25\\\\Samco Mutual Fund\\\\58_31-Jan-25_FS.pdf',\n",
       " 'SBI Mutual Fund': 'C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\Jan 25\\\\SBI Mutual Fund\\\\35_31-Jan-2025_1_FS.pdf',\n",
       " 'SBI Mutual Fund Passive': 'C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\Jan 25\\\\SBI Mutual Fund Passive\\\\35_31-Jan-25_FS.pdf',\n",
       " 'Shriram Mutual fund': 'C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\Jan 25\\\\Shriram Mutual fund\\\\36_31-Jan-25_FS.pdf',\n",
       " 'Sundaram Mutual Fund': 'C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\Jan 25\\\\Sundaram Mutual Fund\\\\37_31-Jan-25_FS.pdf',\n",
       " 'Tata Mutual Fund': 'C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\Jan 25\\\\Tata Mutual Fund\\\\38_31-Jan-25_FS.pdf',\n",
       " 'Taurus Mutual Fund': 'C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\Jan 25\\\\Taurus Mutual Fund\\\\39_31-Jan-25_FS.pdf',\n",
       " 'Trust Mutual Fund': 'C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\Jan 25\\\\Trust Mutual Fund\\\\55_31-Jan-25_FS.pdf',\n",
       " 'Union Mutual Fund': 'C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\Jan 25\\\\Union Mutual Fund\\\\40_31-Jan-25_FS.pdf',\n",
       " 'UTI Mutual Fund': 'C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\Jan 25\\\\UTI Mutual Fund\\\\41_31-Jan-25_FS_1.pdf',\n",
       " 'WhiteOak Mutual Fund': 'C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\Jan 25\\\\WhiteOak Mutual Fund\\\\42_31-Jan-25_FS.pdf',\n",
       " 'Zerodha Mutual Fund': 'C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\Jan 25\\\\Zerodha Mutual Fund\\\\71_31-Jan-25_FS.pdf'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutual_fund"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_path = mutual_fund[\"ITI Mutual Fund\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified PDF saved to: C:\\Users\\Kaustubh.keny\\Projects\\office-work\\mywork-repo\\data\\output\\DryRun.pdf\n"
     ]
    }
   ],
   "source": [
    "lines = [\n",
    "    ((180, 0), (180, 812)),# Vertical line\n",
    "    ((0, 40), (812, 40))\n",
    "]\n",
    "pages = [12, 14,16]\n",
    "bboxes = [(390, 105, 596, 812)] #[(0, 85, 180, 812),(180, 85, 360, 812),(0,100,270,812),(0,100,350,812)]\n",
    "pages = [i for i in range(1,110)]\n",
    "Helper.draw_lines_on_pdf(sample_path, lines, bboxes, pages, dir_path +dry_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proper_fund_names(path: str):\n",
    "    pattern = \"(Invesco India.*?(?:Fund(?:of Funds?)?|ETF|FOF|Path))\"\n",
    "\n",
    "    title = {}   \n",
    "    with fitz.open(path) as doc:\n",
    "        for pgn, page in enumerate(doc):\n",
    "            text = \" \".join(page.get_text(\"text\", clip=(180, 0, 590, 40)).split(\"\\n\"))\n",
    "            text = re.sub(\"[^A-Za-z0-9\\\\s\\\\-\\\\(\\\\).,]+\", \"\", text).strip()\n",
    "            # print(pgn,text)\n",
    "            if matches := re.findall(pattern, text, re.DOTALL):\n",
    "                title[pgn] = matches[0]\n",
    "                print(matches[0])\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = get_proper_fund_names(sample_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r\"(360 ONE.*)$\" 0,0,520,50\n",
    "# \"(Aditya Birla.*(?:Plan\\\\*?\\\\#?\\\\'?|Sensex|Fund|Path|ETF|FOF\\\\*?|Scheme|EOF|Funds\\\\*?|Yojna)?)$\" 0,0,470,25\n",
    "# \"(Bajaj.*(?:Fund|Path|ETF|FOF|EOF|Growth))$\" 0,0,470,50\n",
    "# \"(Bank of India.*?(?:Plan|Funds?|ETF|FOF|FTF))\"  0,55,280,105\n",
    "# \"(Baroda BNP.*?(?:Fund|Path|ETF|FTF|FOF|Index|Fund of Fund))\" 0,0,220,120\n",
    "# \"CANARA.*?\\\\)\" 0,0,400,55\n",
    "# \"((?:DSP|Bharat).*?(?:Fund\\\\s*(?:of Fund)?|FUND|ETF|FTF|FOF))\" 0,0,500,40\n",
    "# \"(Edelweiss\\s*.+?(?:Fund|Path|ETF|FOF|Path))\" 0,0,150,100\n",
    "# \"((?:Franklin|Templeton).*?(?:Fund\\\\s*(?:of Funds)?|Plan))\" 0,0,470,80\n",
    "# \"(GROWW.*?FUND)\" 0,0,470,60\n",
    "# \"(HDFC.*?(?:FUND|Fund\\\\s*(?:of Funds?)?|ETF\\\\s*(?:Fund of Funds?)?))\" 0,0,400,60\n",
    "# \"(HSBC.*?(?:FUND|Fund\\\\s*(?:of Funds?)?|ETF\\\\s*(?:Fund of Funds?)?))\" 0,0,600,45\n",
    "# \"(Helios.*)\" 0,0,600,40\n",
    "# \"((?:ICICI|BHARAT).*)\" 0,0,490,30\n",
    "# \"(Invesco India.*?(?:Fund(?:of Funds?)?|ETF|FOF|Path))\" 180,0,590,40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Helper.get_all_pdf_data(sample_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_and_highlight(path: str):\n",
    "        output_path = path.replace(\".pdf\", \"_hltd.pdf\")\n",
    "        \n",
    "        with fitz.open(path) as doc:\n",
    "            page_count = doc.page_count\n",
    "            indices = Helper._get_financial_indices(r\"C:\\Users\\Kaustubh.keny\\Projects\\office-work\\mywork-repo\\data\\input\\financial_indices.xlsx\")\n",
    "            data = [{\"title\": \"\", \"highlights\": 0, \"detect_idx\": []} for _ in range(page_count)]\n",
    "\n",
    "            for dpgn, page in enumerate(doc):\n",
    "                page_blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "                sorted_blocks = sorted(page_blocks, key=lambda x: (x[\"bbox\"][1], x[\"bbox\"][0]))\n",
    "\n",
    "                for block_count, block in enumerate(sorted_blocks):\n",
    "                    if \"lines\" not in block:\n",
    "                        continue\n",
    "                    for line in block[\"lines\"]:\n",
    "                        for span in line[\"spans\"]:\n",
    "                            text = span[\"text\"].strip().lower()\n",
    "                            \n",
    "                            for indice in indices:\n",
    "                                pattern = rf\"\\b{re.escape(indice)}\\b\"\n",
    "                                if re.search(pattern, text):\n",
    "                                    if indice not in data[dpgn]['detect_idx']:\n",
    "                                        data[dpgn]['detect_idx'].append(indice)\n",
    "                                        data[dpgn]['highlights'] += 1\n",
    "                                    page.add_highlight_annot(fitz.Rect(span[\"bbox\"]))\n",
    "                                    break\n",
    "\n",
    "            doc.save(output_path)\n",
    "        return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import camelot\n",
    "def get_something(path: str):\n",
    "    pattern = r\"^(REDEMPTION PROCEEDS|FEATURES|ASSET ALLOCATION|FUND MANAGER|SCHEME|Sr\\. No\\.)$\"\n",
    "\n",
    "    with fitz.open(path) as doc:\n",
    "        exists = defaultdict(int)\n",
    "        for pgn, page in enumerate(doc):\n",
    "            page_text = [t.strip() for t in page.get_text().split(\"\\n\")]\n",
    "            for text in page_text:\n",
    "                if re.match(pattern,text):\n",
    "                    exists[pgn]+=1\n",
    "                    \n",
    "    \n",
    "    return [str(pgn+1) for pgn, count in exists.items() if count > 4] #camelot starts pages from 1\n",
    "\n",
    "pages = get_something(sample_path)\n",
    "\n",
    "imp_pages = \",\".join(pages)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = camelot.read_pdf(sample_path,pages=imp_pages, flavor=\"stream\", table_areas= [\"30,50,612,812\"],column_tol = 4, split_text = True) #[\"30,50,612,812\"]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import camelot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def clean_column_name(x):\n",
    "    cleaned = re.sub(r\"[^A-Za-z0-9\\s]\", \"\", str(x), flags=re.IGNORECASE) #Corrected re.sub.\n",
    "    cleaned = \"_\".join(cleaned.split())\n",
    "    return cleaned\n",
    "\n",
    "with pd.ExcelWriter(\"cleaned_tables.xlsx\", engine=\"xlsxwriter\") as writer:\n",
    "    for i, table in enumerate(tables):\n",
    "        df = table.df\n",
    "        df = df.map(lambda x: \" \".join(str(x).split(\"\\n\")).strip())\n",
    "        df.replace(\"\", np.nan, inplace=True)\n",
    "        df.iloc[2:, 0] = df.iloc[2:, 0].ffill()\n",
    "        df = df.iloc[1:, :]\n",
    "        df.columns = df.iloc[0, :].apply(clean_column_name)\n",
    "        # print(df.columns)\n",
    "        df = df.iloc[1:, :]\n",
    "        sheet_name = f\"Table_{i+1}\"\n",
    "\n",
    "        # workbook = writer.book\n",
    "        # worksheet = writer.sheets[sheet_name]\n",
    "\n",
    "        # row = 1\n",
    "        # last_value = None\n",
    "        # start_row = None\n",
    "\n",
    "        # for j, value in enumerate(df.iloc[:, 0], start=1):\n",
    "        #     if pd.notna(value):  # New group found\n",
    "        #         if last_value is not None and start_row is not None:\n",
    "        #             worksheet.merge_range(start_row, 0, row - 1, 0, last_value)  # Merge previous block\n",
    "        #         last_value = value\n",
    "        #         start_row = row\n",
    "        #     row += 1\n",
    "\n",
    "        # # Merge last group\n",
    "        # if last_value is not None and start_row is not None:\n",
    "        #     worksheet.merge_range(start_row, 0, row - 1, 0, last_value)\n",
    "    \n",
    "        \n",
    "\n",
    "print(\"Cleaned tables saved with merged spanning rows!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {14: 'LIC MF LARGE CAP FUND',\n",
    "#  15: 'LIC MF LARGE& MID CAPFUND',\n",
    "#  17: 'LIC MF MULTICAP FUND',\n",
    "#  18: \"LIC MF V't CAP FUND\",\n",
    "#  19: 'SMALLCAP FUND',\n",
    "#  21: 'LIC MF DIV_DEND YIELD FUND',\n",
    "#  22: 'LIC MF VALUE FUND',\n",
    "#  23: 'LIC MF FOCUSED FUND',\n",
    "#  24: 'LIC MF INFRASTRUCTURE FUND',\n",
    "#  25: 'LIC MF Poin MANGFACTURING FUND',\n",
    "#  26: 'BANKING & FINANC-AL SERVICES FUND',\n",
    "#  27: 'HEALTHCARE FUND',\n",
    "#  28: 'LiC MF EL_SS TAX SAVER',\n",
    "#  29: 'LIC MF AGGRESSIVE HYBRiD FUND',\n",
    "#  30: 'LIC MF Bâ€™L*NCED ADVANTAGE FUND',\n",
    "#  31: 'LIC MF EQUITY SAVINGS FUND',\n",
    "#  32: 'LIC MF CONSERWATIVE nip?!) FUND',\n",
    "#  33: 'LIC MF ARBITRAGE FUND',\n",
    "#  36: 'LIC MF OVERNIGHT FUND',\n",
    "#  37: 'LIC MF LIQUID FUND',\n",
    "#  38: 'LIC MF ULTRA SHORT DURATION FUND',\n",
    "#  40: 'LIC MF LOWâ€™ DURATION FUND',\n",
    "#  41: 'LIC MF MEDIUM-F2Â°LONG DURATION FUND',\n",
    "#  42: 'LIC MF BANK&ONG & PSU FUND',\n",
    "#  43: 'LIC MF_. SHORT DURATION FUND',\n",
    "#  45: 'LIC MGI FUND',\n",
    "#  46: 'LIC MF l HILDRENS FUND',\n",
    "#  47: 'BSE SENSEX ETF',\n",
    "#  48: 'LIC MF NIFTY 50ETF',\n",
    "#  49: 'LIC MF NIFTY 100 ETF',\n",
    "#  50: 'LIC MF NIFTY MIDCAP 100 ETF',\n",
    "#  51: 'LIC MF NIFTY 8-13 YR G-SECETF',\n",
    "#  52: 'LIC MF BSE SENSEX INDEX FUND',\n",
    "#  53: 'LIC MF NIFTY 50 INDEX FUND',\n",
    "#  54: 'LIC MF NIFTY NEXT 50 INDEX FUND',\n",
    "#  55: 'LIC MF G2LD EXCHANGE TRADED FUND',\n",
    "#  56: 'LIC MF GSLD ETF',\n",
    "#  57: 'LIC MF Large Cap Fund',\n",
    "#  58: 'LIC MF Mid cap Fund',\n",
    "#  59: 'LIC MF Focused Fund',\n",
    "#  60: 'Lic MF ELSs Tax Saver',\n",
    "#  61: 'LIC MF Arbitrage Fund',\n",
    "#  63: 'LIC MF Nifty Midcap 100 ETF',\n",
    "#  64: 'LIC MF Large Cap Fund',\n",
    "#  65: 'LIC MF Dividend Yield Fund',\n",
    "#  66: 'LIC MEF Healthcare Fund',\n",
    "#  67: 'LIC MF Unit Linked Insurance Scheme __LIC MF Overnight Fund',\n",
    "#  68: 'LIC MF Short Duration Fund',\n",
    "#  69: 'LIC MF BSE Sensex ETF',\n",
    "#  70: 'LIC MF BSE Sensex Index Fund',\n",
    "#  73: 'LIC MF Healthcare Fund',\n",
    "#  74: 'LIC MF Liquid Fund',\n",
    "#  75: 'LIC MF Nifty 50 ETF',\n",
    "#  76: 'LIC Mutual Fund'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NIPPON DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def via_block(path:str):\n",
    "    pattern = r\"FUNDS AT A GLANCE\"\n",
    "    amc_pattern = \"^(Nippon India|CPSE).*(?=Plan|Next 50|Sensex|Fund|Path|ETF|FOF|EOF|Funds|$)\"\n",
    "    imp_pages = []\n",
    "    with fitz.open(path) as doc:\n",
    "        for pgn, page in enumerate(doc):\n",
    "                page_blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "                sorted_blocks = sorted(page_blocks, key=lambda x: (x[\"bbox\"][1], x[\"bbox\"][0]))\n",
    "                for block_count, block in enumerate(sorted_blocks[:10]):\n",
    "                    if \"lines\" not in block:\n",
    "                        continue\n",
    "                    for line in block[\"lines\"]:\n",
    "                        for span in line[\"spans\"]:\n",
    "                            text = span[\"text\"].strip()\n",
    "                            if re.match(pattern,text):\n",
    "                                imp_pages.append(pgn)\n",
    "                                \n",
    "        amc_fund = defaultdict(list)\n",
    "    \n",
    "        for pgn in imp_pages:\n",
    "            page = doc[pgn]\n",
    "            page_blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "            sorted_blocks = sorted(page_blocks, key=lambda x: (x[\"bbox\"][1], x[\"bbox\"][0]))\n",
    "            for block_count, block in enumerate(sorted_blocks):\n",
    "                if \"lines\" not in block:\n",
    "                    continue\n",
    "                for line in block[\"lines\"]:\n",
    "                    for span in line[\"spans\"]:\n",
    "                        text = span[\"text\"].strip()\n",
    "                        color = span['color']\n",
    "                        if re.match(amc_pattern,text)and color == -1:\n",
    "                            # matches = re.findall(amc_pattern,text)\n",
    "                            amc_fund[pgn].append(text)\n",
    "                            \n",
    "    return imp_pages, dict(amc_fund)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages,amc = via_block(sample_path)\n",
    "pages = list(map(str,[x+1 for x in pages]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_scheme = defaultdict(list)\n",
    "for key, value in amc.items():\n",
    "    # print(key)\n",
    "    set1 = ['Scheme Name']+value[:4]\n",
    "    set2 = ['Scheme Name']+value[4:]\n",
    "    final_scheme[key+1].append(set1)\n",
    "    final_scheme[key+1].append(set2)\n",
    "\n",
    "final_scheme = dict(final_scheme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_pages = \",\".join(pages)\n",
    "tables = camelot.read_pdf(sample_path,pages=imp_pages, flavor=\"lattice\", line_scale = 40)  #table_areas = [\"0,0,580,690\"]            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "with pd.ExcelWriter(\"merged_tables.xlsx\", engine=\"openpyxl\") as writer:\n",
    "    count = 0  # Toggle between 0 and 1\n",
    "    \n",
    "    for i, table in enumerate(tables):\n",
    "        df = table.df\n",
    "        if df.shape[1] < 3:\n",
    "            continue\n",
    "\n",
    "    \n",
    "        df = df.map(lambda x: \" \".join(x.split(\"\\n\")).strip())\n",
    "        df = df.map(lambda x: np.nan if not x.strip() else x)\n",
    "        df.set_index(df.columns[0], inplace=True)\n",
    "\n",
    "        for check in [\"Scheme Name\", \"Market Capitalization\"]:\n",
    "            if check in df.index:\n",
    "                df.drop(check, inplace=True)\n",
    "                \n",
    "        df_cleaned = df[~df.index.isna()]\n",
    "        df_cleaned = df_cleaned[df_cleaned.index != \"\"]\n",
    "        df_cleaned = df_cleaned.reset_index()\n",
    "        df_fill = df_cleaned.ffill(axis=1)\n",
    "\n",
    "\n",
    "        sch_vals = final_scheme[table.page][count]\n",
    "        count = 1 - count  # Toggle between 0 and 1\n",
    "\n",
    "        if len(sch_vals) == 5:\n",
    "            df_fill.loc[-1] = sch_vals \n",
    "            df_fill = df_fill.sort_index().reset_index(drop=True)\n",
    "\n",
    "        # Write to a new sheet\n",
    "        df_fill.to_excel(writer, sheet_name=f\"Table_{i+1}\", index=False)\n",
    "\n",
    "print(\"All tables saved in separate sheets in 'merged_tables.xlsx' ðŸš€\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hello =  {\n",
    "    \"number\": 0,\n",
    "    \"type\": 0,\n",
    "    \"bbox\": (0,0,0,0), #406.72119140625, 439.4930419921875, 565.697265625, 484.5830383300781\n",
    "    \"lines\": [\n",
    "        {\n",
    "            \"spans\": [\n",
    "                {\n",
    "                    \"size\": 30.0,\n",
    "                    \"flags\": 20,\n",
    "                    \"font\": \"Montserrat-Regular\", #set this\n",
    "                    \"color\": -1, #set this\n",
    "                    \"ascender\": 1.0429999828338623,\n",
    "                    \"descender\": -0.2619999945163727,\n",
    "                    \"text\": \"DUMMYDUMMYDUMMYDUMMY\",\n",
    "                    \"origin\": (406.72119140625, 458.26702880859375),\n",
    "                    \"bbox\": (0,0,0,0), #406.72119140625,439.4930419921875,565.697265625,462.9830322265625,\n",
    "                }\n",
    "            ],\n",
    "            \"wmode\": 0,\n",
    "            \"dir\": (1.0, 0.0),\n",
    "            \"bbox\": (0,0,0,0), #406.72119140625,439.4930419921875,565.697265625,462.9830322265625,\n",
    "        },\n",
    "        \n",
    "    ],\n",
    "},\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
