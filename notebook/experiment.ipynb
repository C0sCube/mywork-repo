{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint, json, math, os, sys\n",
    "import fitz, pdfplumber, ocrmypdf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "dir_path = \"C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\office-work\\\\mywork-repo\"\n",
    "fund_path = \"C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\Feb 25\"\n",
    "\n",
    "# dir_path = \"C:\\\\Users\\\\rando\\\\OneDrive\\\\Documents\\\\mywork-repo\"\n",
    "# fund_path =  \"C:\\\\Users\\\\rando\\\\OneDrive\\\\Documents\\\\Feb25\"\n",
    "sys.path.append(os.path.abspath(dir_path))\n",
    "\n",
    "from app.utils import Helper\n",
    "from app.parse_regex import *\n",
    "\n",
    "dry_path = r'\\data\\output\\DryRun.pdf'\n",
    "fin_path = r'\\data\\input\\financial_indices.xlsx'\n",
    "mutual_fund = Helper.get_fund_paths(fund_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_clipped_data(input:str, pages:list, bboxes:list):\n",
    "        \n",
    "        document = fitz.open(input)\n",
    "        final_list = []\n",
    "    \n",
    "        for pgn in pages:\n",
    "            page = document[pgn]\n",
    "            \n",
    "            all_blocks = [] #store every data from bboxes\n",
    "            \n",
    "            for bbox in bboxes:\n",
    "                blocks, seen_blocks = [], set()  #store unique blocks based on content and bbox\n",
    "                \n",
    "                page_blocks = page.get_text('dict', clip=bbox)['blocks']\n",
    "                for block in page_blocks:\n",
    "                    if block['type'] == 0 and 'lines' in block: #type 0 means text block\n",
    "                        #hash_key\n",
    "                        block_key = (tuple(block['bbox']), tuple(tuple(line['spans'][0]['text'] for line in block['lines'])))\n",
    "                        if block_key not in seen_blocks:\n",
    "                            seen_blocks.add(block_key)\n",
    "                            blocks.append(block)\n",
    "\n",
    "                sorted_blocks = sorted(blocks, key=lambda x: (x['bbox'][1], x['bbox'][0]))\n",
    "                all_blocks.append(sorted_blocks)\n",
    "\n",
    "            final_list.append({\n",
    "                \"pgn\": pgn,\n",
    "                \"block\": all_blocks #will be list[list,list,..]\n",
    "            })\n",
    "\n",
    "        document.close()\n",
    "        return final_list\n",
    "    \n",
    "def extract_data_relative_line(path: str, line_x: float, side: str):\n",
    "    doc = fitz.open(path)\n",
    "    pages = doc.page_count\n",
    "\n",
    "    final_list = []\n",
    "\n",
    "    for pgn in range(pages):\n",
    "        page = doc[pgn]\n",
    "\n",
    "        blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "        sorted_blocks = sorted(blocks, key=lambda x: (x[\"bbox\"][1], x[\"bbox\"][0]))\n",
    "        extracted_blocks = []\n",
    "\n",
    "        # Keep track of blocks to avoid duplicates\n",
    "        added_blocks = set()\n",
    "\n",
    "        for block in sorted_blocks:\n",
    "            block_id = id(block)  # Unique identifier for the block\n",
    "\n",
    "            for line in block.get(\"lines\", []):\n",
    "                for span in line.get(\"spans\", []):\n",
    "                    origin = span[\"origin\"]\n",
    "                    x0, _ = origin\n",
    "\n",
    "                    # Check the side condition\n",
    "                    if side == \"left\" and x0 < line_x and block_id not in added_blocks:\n",
    "                        extracted_blocks.append(block)\n",
    "                        added_blocks.add(block_id)  # Mark block as added\n",
    "                    elif side == \"right\" and x0 > line_x and block_id not in added_blocks:\n",
    "                        extracted_blocks.append(block)\n",
    "                        added_blocks.add(block_id)  # Mark block as added\n",
    "\n",
    "      \n",
    "        final_list.append({\n",
    "            \"pgn\": pgn,\n",
    "            \"blocks\": extracted_blocks\n",
    "        })\n",
    "\n",
    "    doc.close()\n",
    "\n",
    "    return final_list\n",
    "  \n",
    "def get_clipped_data(input:str, bboxes:list[set], *args):\n",
    "    \n",
    "        document = fitz.open(input)\n",
    "        final_list = []\n",
    "        if args:\n",
    "            pages = list(args)\n",
    "        else:\n",
    "            pages = [i for i in document.page_count]\n",
    "        \n",
    "        for pgn in pages:\n",
    "            page = document[pgn]\n",
    "\n",
    "            blocks = []\n",
    "            for bbox in bboxes:\n",
    "                blocks.extend(page.get_text('dict', clip = bbox)['blocks']) #get all blocks\n",
    "            \n",
    "            filtered_blocks = [block for block in blocks if block['type']== 0 and 'lines' in block]\n",
    "            # sorted_blocks = sorted(filtered_blocks, key= lambda x: (x['bbox'][1], x['bbox'][0]))\n",
    "            \n",
    "             # Extract text from sorted blocks\n",
    "            extracted_text = []\n",
    "            for block in filtered_blocks:\n",
    "                block_text = []\n",
    "                for line in block['lines']:\n",
    "                    line_text = \" \".join(span['text'] for span in line['spans'])\n",
    "                    block_text.append(line_text)\n",
    "                extracted_text.append(\"\\n\".join(block_text))\n",
    "            \n",
    "            final_list.append({\n",
    "            \"pgn\": pgn,\n",
    "            \"block\": filtered_blocks,\n",
    "            \"text\": extracted_text\n",
    "            })\n",
    "            \n",
    "            \n",
    "        document.close()\n",
    "        return final_list\n",
    "    \n",
    "def get_clipped_text(input:str, bboxes:list[set],*args):\n",
    "\n",
    "    document = fitz.open(input)\n",
    "    final_list = []\n",
    "    \n",
    "    if args:\n",
    "        pages = list(args)\n",
    "    else:\n",
    "        pages = [i for i in document.page_count]\n",
    "    \n",
    "    for pgn in pages:\n",
    "        page = document[pgn]\n",
    "        blocks = []\n",
    "        for bbox in bboxes:\n",
    "            blocks = page.get_text('text', clip = bbox).split('\\n') #get all blocks\n",
    "        final_list.append({\n",
    "        \"pgn\": pgn,\n",
    "        \"block\": blocks\n",
    "        })   \n",
    "    document.close()\n",
    "    return final_list\n",
    "\n",
    "def get_proper_fund_names(path: str, pages: list):\n",
    "    doc = fitz.open(path)\n",
    "    title = {}\n",
    "\n",
    "    for pgn in pages:\n",
    "        page = doc[pgn]\n",
    "        blocks = page.get_text(\"dict\")['blocks']\n",
    "        text_all = \" \".join(\n",
    "            span[\"text\"].strip()\n",
    "            for block in blocks[:4]\n",
    "            for line in block.get(\"lines\", [])\n",
    "            for span in line.get(\"spans\", [])\n",
    "            if span[\"text\"].strip()\n",
    "        )\n",
    "\n",
    "        text_all = re.sub(r'[^A-Za-z0-9\\s]+', '', text_all).strip()\n",
    "        matches = re.findall(r\"((?:LIC\\s*MF|BLNCED|LOW)\\s+.*?(?:FUND|ETF|FTF|FOF|PLAN|SAVER))\", text_all, re.IGNORECASE)\n",
    "\n",
    "        title[pgn] = matches[0] if matches else \"\"\n",
    "\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_path = mutual_fund[\"Quant Mutual Fund\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified PDF saved to: C:\\Users\\Kaustubh.keny\\Projects\\office-work\\mywork-repo\\data\\output\\DryRun.pdf\n"
     ]
    }
   ],
   "source": [
    "lines = [\n",
    "    ((75, 0), (75, 812)),# Vertical line\n",
    "    ((0, 145), (812, 145))\n",
    "]\n",
    "pages = [12, 14,16]\n",
    "bboxes = [[435, 17, 610, 103],] #[(0, 85, 180, 812),(180, 85, 360, 812),(0,100,270,812),(0,100,350,812)]\n",
    "pages = [i for i in range(1,110)]\n",
    "Helper.draw_lines_on_pdf(sample_path, lines, bboxes, pages, dir_path +dry_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_clipped_text_all_pages(pdf_path, clip_coords):\n",
    "    results = {}\n",
    "    doc = fitz.open(pdf_path)\n",
    "    clip_rect = fitz.Rect(*clip_coords)\n",
    "    try:\n",
    "        for page_number, page in enumerate(doc):\n",
    "            text = page.get_text(\"text\", clip=clip_rect).strip()\n",
    "            results[page_number] = text\n",
    "    finally:\n",
    "        doc.close()\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clip_coords = (435, 17, 610, 100)  # (x0, y0, x1, y1)\n",
    "\n",
    "clipped_texts = extract_clipped_text_all_pages(sample_path, clip_coords)\n",
    "\n",
    "for page_num, text in clipped_texts.items():\n",
    "    print(f\"Page {page_num + 1}:\\n{text}\\n{'-'*40}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proper_fund_names(path: str):\n",
    "    pattern = \"((?:ICICI|BHARAT).*?)\\\\s*\\\\(\"\n",
    "    title = {}   \n",
    "    with fitz.open(path) as doc:\n",
    "        for pgn, page in enumerate(doc):\n",
    "            text = \" \".join(page.get_text(\"text\", clip=( 0,\n",
    "          0,\n",
    "          490,\n",
    "          60)).split(\"\\n\"))\n",
    "            text = re.sub(\"[^A-Za-z0-9\\\\s\\\\-\\\\(\\\\).,]+|\\u2028\", \"\", text).strip()\n",
    "            # print(text)\n",
    "            if matches := re.findall(pattern, text, re.DOTALL):\n",
    "                title[pgn] = \" \".join([_ for _ in matches[0].strip().split(\" \") if _])\n",
    "                print(matches[0],pgn)\n",
    "    return title\n",
    "title = get_proper_fund_names(sample_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r\"(360 ONE.*)$\" 0,0,520,50\n",
    "# \"(Aditya Birla.*(?:Plan\\\\*?\\\\#?\\\\'?|Sensex|Fund|Path|ETF|FOF\\\\*?|Scheme|EOF|Funds\\\\*?|Yojna)?)$\" 0,0,470,25\n",
    "# \"(Bajaj.*(?:Fund|Path|ETF|FOF|EOF|Growth))$\" 0,0,470,50\n",
    "# \"(Bank of India.*?(?:Plan|Funds?|ETF|FOF|FTF))\"  0,55,280,105\n",
    "# \"(Baroda BNP.*?(?:Fund|Path|ETF|FTF|FOF|Index|Fund of Fund))\" 0,0,220,120\n",
    "# \"CANARA.*?\\\\)\" 0,0,400,55\n",
    "# \"((?:DSP|Bharat).*?(?:Fund\\\\s*(?:of Fund)?|FUND|ETF|FTF|FOF))\" 0,0,500,40\n",
    "# \"(Edelweiss\\s*.+?(?:Fund|Path|ETF|FOF|Path))\" 0,0,150,100\n",
    "# \"((?:Franklin|Templeton).*?(?:Fund\\\\s*(?:of Funds)?|Plan))\" 0,0,470,80\n",
    "# \"(GROWW.*?FUND)\" 0,0,470,60\n",
    "# \"(HDFC.*?(?:FUND|Fund\\\\s*(?:of Funds?)?|ETF\\\\s*(?:Fund of Funds?)?))\" 0,0,400,60\n",
    "# \"(HSBC.*?(?:FUND|Fund\\\\s*(?:of Funds?)?|ETF\\\\s*(?:Fund of Funds?)?))\" 0,0,600,45\n",
    "# \"(Helios.*)\" 0,0,600,40\n",
    "# \"((?:ICICI|BHARAT).*)\" 0,0,490,30\n",
    "# \"(Invesco India.*?(?:Fund(?:of Funds?)?|ETF|FOF|Path))\" 180,0,590,40\n",
    "# \"(ITI.*?(?:Fund(?:of Funds?)?|ETF|FOF|Path))\" 0,30,300,100\n",
    "# \"(JM.*?(?:Fund(?:of Funds?)?|ETF|FOF|Path))\" 0,0,400,50\n",
    "# \"(KOTAK.*?(?:FUND\\\\s*(?:OF FUNDS?\\\\s*|-\\\\s*\\\\w+)?|ETF|FOF|PATH|FTF))\" 0,0,450,70\n",
    "# \"((?:LI?i?C|BSE|BANK|SMALL|HEALTH).*?(?:FUND|Path|ETF|FTF|EOF|FOF|PLAN|SAVER|FUND\\s*OF\\s*FUND))\" 0,0,400,100\n",
    "# \"\\\\b(Mahindra.*?(?:Fund|ETF|EOF|FOF|FTF|Path|FO))\\\\b\" 170,50,450,80\n",
    "# r'MIRAE ASSET .*?\\b(?:ETF|EOF|FOF|FTF|FUND|FUND OF FUND|INDEX FUND)\\b' 0,0,560,140\n",
    "# \"(NJ.*?(?:FUND\\\\s*(?:OF FUNDS?\\\\s*|-\\\\s*\\\\w+)?|ETF|FOF|PATH|FTF))\" 0,30,480,70\n",
    "# \"([A-Z0-9\\\\s\\\\-]+\\\\s*PGIM INDIA)\" 0,0,300,80\n",
    "# \"(Samco.*?Fund)\" 0,30,600,100\n",
    "# SBI PASSIVE r\"((?:SBI|i\\s*_|S35).*$)\" 0, 0, 400, 50 \n",
    "# SBI NORMAL \"(SBI.*?(?:Fund\\\\s*(?:of funds?|.*?Plan)?|ETF|FTF|FOF))\" 160, 640, 550, 812\n",
    "# r\"(Union\\s*[A-Za-z\\s]+?(?:FUND|PATH|ETF|FOF))\" 0,0,180,150 \n",
    "# \"(Sundaram.*?(?:Fund\\\\s*(?:of funds?|.*?Plan|Series)?|ETF|FTF|FOF))\" 0,0,400,40\n",
    "# \"((?:Tata|Treasury|TATA).*?(?:Funds?(?:.*?Plan)?|ETF|FOF|EOF|Plan.*?(?:days)?))\" 0,0,540,40\n",
    "# \"((?:TAURUS|Taurus).*?(?:FUND|Fund))\" 0,0,480,40\n",
    "# \"(TRUSTMF.*?(?:FUND|Fund))\" 0,0,340,45\n",
    "# \"(quant.*?(?:FUND|Fund))\" 0,0,480,80\n",
    "# \"(UTI.*?(?:FUND(?:\\\\s*OF FUND|.+?DURATION)?|PLAN|ETF|FTF|FOF|Fund|ETF(?: FUND OF FUND)?)(?:\\\\s*\\\\(\\\\s*Erst.+?\\\\))?)\" 0,0,460,35\n",
    "# \"(WhiteOak.*?(?:Fund))\" 0,0,410,60\n",
    "# \"(Zerodha.*?(?:Fund|ETF(?:\\\\s*FoF)?|FoF))\" 0,60,500,180\n",
    "# \"(Motilal.*?(?:Fund\\\\s*(?:o?O?f Funds?(?:.+?Aggressive|.+?Conservative)?)?|ETF(?:.+? of Funds?)?|FoF))\" 0,0,600,60\n",
    "\n",
    "# \"(SHRIRAM.*?(?:FUNDS?|ETF|Fo?O?F|PLANS?))\" 0,0,480,70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Helper.get_all_pdf_data(sample_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with fitz.open(sample_path) as doc:\n",
    "    indices = [\"Fund Manager\", \"Co-Fund Manager\", \"Date of Allotment\", \"Benchmark Index\", \"Minimum Application\", \"Additional Purchase\", \"Entry Load\", \"Exit Load\", \"Portfolio Turnover\", \"Net AUM\", \"Monthly Average AUM\", \"Std Dev\", \"Sharpe Ratio\", \"Portfolio Beta\", \"R Squared\", \"Treynor\", \"YTM\", \"Macaulay\", \"Residual Maturity\", \"NAV\"]\n",
    "    output_path = sample_path.replace(\".pdf\", \"_hltd.pdf\")\n",
    "    highlight_count,found_indices = 0,[]\n",
    "    for pgn, page in enumerate(doc):\n",
    "        blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "        for block in blocks:\n",
    "            if \"lines\" not in block:\n",
    "                continue\n",
    "            for line in block[\"lines\"]:\n",
    "                for span in line[\"spans\"]:\n",
    "                    text = span[\"text\"].strip().lower()\n",
    "                    text =  re.sub(\"[^\\\\w\\\\s]\", \"\", text)\n",
    "                    for indice in indices:\n",
    "                        if re.search(rf\"\\b{re.escape(indice)}\\b\", text,re.IGNORECASE):\n",
    "                            if indice not in found_indices:\n",
    "                                found_indices.append(indice)\n",
    "                                highlight_count += 1\n",
    "                            page.add_highlight_annot(fitz.Rect(span[\"bbox\"]))\n",
    "                            break\n",
    "\n",
    "    doc.save(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#canara\n",
    "# def _update_manager_data(self,main_key:str,manager_data):\n",
    "#     name = r\"(?:Mr\\.?|Mrs\\.?|Ms\\.?)\\s+([A-Z][a-z]+\\s[A-Z][a-z]+)\"\n",
    "#     exp = r\"\\b\\d+\\s*Years?\\b\"\n",
    "#     since = r\"\\bSince\\s*([0-9]+\\s*-\\s*[A-Za-z]+\\.?\\s*-\\s*[0-9]+)\\b\"\n",
    "#     nsample, msample, esample = [], [], []\n",
    "#     nlength = 0\n",
    "#     value = \" \".join(manager_data.values())\n",
    "#     nsample = re.findall(name, value, re.IGNORECASE)\n",
    "#     esample = re.findall(exp, value, re.IGNORECASE)\n",
    "#     msample = re.findall(since, value, re.IGNORECASE)\n",
    "    \n",
    "#     nlength = len(nsample)\n",
    "#     msample += [\"\"] * (nlength - len(msample))\n",
    "#     esample += [\"\"] * (nlength - len(esample))\n",
    "    \n",
    "#     final_list = [self._return_manager_data(since=m,name=n,exp=e)for n, m, e in zip(nsample, msample, esample)]\n",
    "#     return {main_key:final_list}\n",
    "\n",
    "# # hdfc\n",
    "# def _extract_manager_data(self, main_key: str, data, pattern:str):\n",
    "#     DATE_PATTERN = r\"([A-Za-z]+\\s*\\d+),\"\n",
    "#     NAME_PATTERN = r\"([A-Za-z]+\\s[A-Za-z]+)\"\n",
    "#     EXP_PATTERN = r\"over (\\d+ years)\"\n",
    "#     YEAR_PATTERN = r\"(\\d{4})\"\n",
    "\n",
    "#     manager_data = \" \".join(data)\n",
    "#     manager_data = re.sub(r\"[^A-Za-z0-9\\s\\-\\(\\).,]+\", \"\", manager_data).strip()\n",
    "\n",
    "#     experience_years = re.findall(EXP_PATTERN, manager_data, re.IGNORECASE)\n",
    "#     dates = re.findall(DATE_PATTERN, manager_data, re.IGNORECASE)[:len(experience_years)]\n",
    "#     years = re.findall(YEAR_PATTERN, manager_data, re.IGNORECASE)[:len(experience_years)]\n",
    "#     names = re.findall(NAME_PATTERN, manager_data, re.IGNORECASE)[:len(experience_years)]\n",
    "    \n",
    "#     managing_since = [f\"{date}, {year}\" for date, year in zip(dates, years)]\n",
    "#     experience_list = [f\"{exp} years\" for exp in experience_years]\n",
    "#     final_list = [\n",
    "#         self._return_manager_data(name=name,since=since,exp=exp)\n",
    "#         for since, exp, name in zip(managing_since, experience_list, names)\n",
    "#     ]\n",
    "    \n",
    "#     return {main_key:final_list}\n",
    "\n",
    "\n",
    "# #NJMF\n",
    "# def _update_manager_data(self,main_key:str,manager_data):\n",
    "#     exp =\"\\\\d{1,2} years\"\n",
    "#     name = \"(?:Mr\\\\.?|Mrs\\\\.?|Ms\\\\.?)\\\\s*([A-Za-z]+\\\\s*[A-Za-z]+)\"\n",
    "#     since = \"(?:since|from)\\\\s*([A-Za-z]+\\\\s*\\\\d{1,2},\\\\s*\\\\d{3,4}|inception)\"\n",
    "    \n",
    "#     manager_data = \" \".join(manager_data) if isinstance(manager_data,list) else manager_data\n",
    "#     manager_data =re.sub(self.REGEX[\"escape\"], \"\", manager_data).strip()\n",
    "#     nsample = re.findall(name, manager_data, re.IGNORECASE)\n",
    "#     esample = re.findall(exp, manager_data, re.IGNORECASE)\n",
    "#     msample = re.findall(since, manager_data, re.IGNORECASE)\n",
    "#     final_list = [self._return_manager_data(since=m,name=n,exp=e)for n, m, e in zip(nsample, msample, esample)]\n",
    "#     return {main_key:final_list}\n",
    "\n",
    "# #sbi\n",
    "# def _extract_manager_data(self, main_key: str, data, pattern: str):\n",
    "#     name = \"(?:Mr\\\\.?|Mrs\\\\.?|Ms\\\\.?)\\\\s*([A-Za-z.]+\\\\s*[A-Za-z]+)\"\n",
    "#     since = \"((?:\\\\(w.e.f\\\\.?)?[A-Za-z]+\\\\s*\\\\d{4}\\\\s*(?:\\\\()?)\"\n",
    "#     exp = \"([0-9]+ years)\"\n",
    "\n",
    "#     final_list = []\n",
    "#     manager_data = \" \".join(data) if isinstance(data,list) else data\n",
    "#     manager_data =re.sub(self.REGEX[\"escape\"], \"\", manager_data).strip()\n",
    "#     n = re.findall(name,manager_data, re.IGNORECASE)\n",
    "#     s = re.findall(since,manager_data, re.IGNORECASE)\n",
    "#     e = re.findall(exp,manager_data, re.IGNORECASE)\n",
    "    \n",
    "#     adjust = lambda target, lst: target[:len(lst)] + ([target[-1]] * abs(len(target) - len(lst)) if lst else [\"\"])\n",
    "#     n,s = adjust(n,e),adjust(s,e)\n",
    "#     for name,since,exp in zip(n,s,e):\n",
    "#         final_list.append(self._return_manager_data(name=name,since=since,exp=exp))\n",
    "#     return {main_key: final_list}\n",
    "\n",
    "# #sbI PASSIVE\n",
    "# def _update_manager_data(self,main_key:str,data):\n",
    "#     final_list = []\n",
    "#     manager_data = \" \".join(data) if isinstance(data, list) else data\n",
    "#     manager_data = re.sub(self.REGEX['escape'], \"\", manager_data).strip()\n",
    "#     pattern_info = self.REGEX[\"manager\"]\n",
    "#     regex_pattern = pattern_info['pattern']\n",
    "#     field_names = pattern_info['fields']\n",
    "#     if matches := re.findall(regex_pattern, manager_data, re.IGNORECASE):\n",
    "#         for match in matches:\n",
    "#             if isinstance(match, str):\n",
    "#                 match = (match,)\n",
    "#             record = {field_names[i]: match[i] if i < len(match) else \"\" for i in range(len(field_names))} #kwargs\n",
    "#             final_list.append(self._return_manager_data(**record))\n",
    "\n",
    "#     return {main_key: final_list}\n",
    "    \n",
    "# # ADITYA\n",
    "# def _update_manager_data(self,main_key:str,manager_data):\n",
    "#     nsample, msample, esample = [], [], []\n",
    "#     nlength = 0\n",
    "#     for key, value in manager_data.items():\n",
    "#         if re.search(r\"\\bfund_manager\\b\", key, re.IGNORECASE):\n",
    "#             nsample = re.findall(self.REGEX[\"manager\"][\"name\"], value, re.IGNORECASE)\n",
    "#             nlength = len(nsample)\n",
    "\n",
    "#         elif re.search(r\"^managing\", key, re.IGNORECASE):\n",
    "#             msample = re.findall(self.REGEX[\"manager\"][\"since\"], value, re.IGNORECASE)\n",
    "            \n",
    "#         elif re.search(r\"^experience\", key, re.IGNORECASE):\n",
    "#             esample = re.findall(self.REGEX[\"manager\"][\"exp\"], value, re.IGNORECASE)\n",
    "#     nlength = len(nsample)\n",
    "#     msample += [\"\"] * (nlength - len(msample))\n",
    "#     esample += [\"\"] * (nlength - len(esample))\n",
    "\n",
    "#     final_list = [\n",
    "#         self._return_manager_data(since=m,name=n,exp=e)\n",
    "#         for n, m, e in zip(nsample, msample, esample)\n",
    "#     ]\n",
    "\n",
    "#     return {main_key:final_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import camelot\n",
    "def get_something(path: str):\n",
    "    pattern = r\"^(REDEMPTION PROCEEDS|FEATURES|ASSET ALLOCATION|FUND MANAGER|SCHEME|Sr\\. No\\.)$\"\n",
    "\n",
    "    with fitz.open(path) as doc:\n",
    "        exists = defaultdict(int)\n",
    "        for pgn, page in enumerate(doc):\n",
    "            page_text = [t.strip() for t in page.get_text().split(\"\\n\")]\n",
    "            for text in page_text:\n",
    "                if re.match(pattern,text):\n",
    "                    exists[pgn]+=1\n",
    "                    \n",
    "    \n",
    "    return [str(pgn+1) for pgn, count in exists.items() if count > 4] #camelot starts pages from 1\n",
    "\n",
    "pages = get_something(sample_path)\n",
    "\n",
    "imp_pages = \",\".join(pages)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = camelot.read_pdf(sample_path,pages=imp_pages, flavor=\"stream\", table_areas= [\"30,50,612,812\"],column_tol = 4, split_text = True) #[\"30,50,612,812\"]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import camelot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def clean_column_name(x):\n",
    "    cleaned = re.sub(r\"[^A-Za-z0-9\\s]\", \"\", str(x), flags=re.IGNORECASE) #Corrected re.sub.\n",
    "    cleaned = \"_\".join(cleaned.split())\n",
    "    return cleaned\n",
    "\n",
    "with pd.ExcelWriter(\"cleaned_tables.xlsx\", engine=\"xlsxwriter\") as writer:\n",
    "    for i, table in enumerate(tables):\n",
    "        df = table.df\n",
    "        df = df.map(lambda x: \" \".join(str(x).split(\"\\n\")).strip())\n",
    "        df.replace(\"\", np.nan, inplace=True)\n",
    "        df.iloc[2:, 0] = df.iloc[2:, 0].ffill()\n",
    "        df = df.iloc[1:, :]\n",
    "        df.columns = df.iloc[0, :].apply(clean_column_name)\n",
    "        # print(df.columns)\n",
    "        df = df.iloc[1:, :]\n",
    "        sheet_name = f\"Table_{i+1}\"\n",
    "\n",
    "        # workbook = writer.book\n",
    "        # worksheet = writer.sheets[sheet_name]\n",
    "\n",
    "        # row = 1\n",
    "        # last_value = None\n",
    "        # start_row = None\n",
    "\n",
    "        # for j, value in enumerate(df.iloc[:, 0], start=1):\n",
    "        #     if pd.notna(value):  # New group found\n",
    "        #         if last_value is not None and start_row is not None:\n",
    "        #             worksheet.merge_range(start_row, 0, row - 1, 0, last_value)  # Merge previous block\n",
    "        #         last_value = value\n",
    "        #         start_row = row\n",
    "        #     row += 1\n",
    "\n",
    "        # # Merge last group\n",
    "        # if last_value is not None and start_row is not None:\n",
    "        #     worksheet.merge_range(start_row, 0, row - 1, 0, last_value)\n",
    "    \n",
    "        \n",
    "\n",
    "print(\"Cleaned tables saved with merged spanning rows!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {14: 'LIC MF LARGE CAP FUND',\n",
    "#  15: 'LIC MF LARGE& MID CAPFUND',\n",
    "#  17: 'LIC MF MULTICAP FUND',\n",
    "#  18: \"LIC MF V't CAP FUND\",\n",
    "#  19: 'SMALLCAP FUND',\n",
    "#  21: 'LIC MF DIV_DEND YIELD FUND',\n",
    "#  22: 'LIC MF VALUE FUND',\n",
    "#  23: 'LIC MF FOCUSED FUND',\n",
    "#  24: 'LIC MF INFRASTRUCTURE FUND',\n",
    "#  25: 'LIC MF Poin MANGFACTURING FUND',\n",
    "#  26: 'BANKING & FINANC-AL SERVICES FUND',\n",
    "#  27: 'HEALTHCARE FUND',\n",
    "#  28: 'LiC MF EL_SS TAX SAVER',\n",
    "#  29: 'LIC MF AGGRESSIVE HYBRiD FUND',\n",
    "#  30: 'LIC MF B’L*NCED ADVANTAGE FUND',\n",
    "#  31: 'LIC MF EQUITY SAVINGS FUND',\n",
    "#  32: 'LIC MF CONSERWATIVE nip?!) FUND',\n",
    "#  33: 'LIC MF ARBITRAGE FUND',\n",
    "#  36: 'LIC MF OVERNIGHT FUND',\n",
    "#  37: 'LIC MF LIQUID FUND',\n",
    "#  38: 'LIC MF ULTRA SHORT DURATION FUND',\n",
    "#  40: 'LIC MF LOW’ DURATION FUND',\n",
    "#  41: 'LIC MF MEDIUM-F2°LONG DURATION FUND',\n",
    "#  42: 'LIC MF BANK&ONG & PSU FUND',\n",
    "#  43: 'LIC MF_. SHORT DURATION FUND',\n",
    "#  45: 'LIC MGI FUND',\n",
    "#  46: 'LIC MF l HILDRENS FUND',\n",
    "#  47: 'BSE SENSEX ETF',\n",
    "#  48: 'LIC MF NIFTY 50ETF',\n",
    "#  49: 'LIC MF NIFTY 100 ETF',\n",
    "#  50: 'LIC MF NIFTY MIDCAP 100 ETF',\n",
    "#  51: 'LIC MF NIFTY 8-13 YR G-SECETF',\n",
    "#  52: 'LIC MF BSE SENSEX INDEX FUND',\n",
    "#  53: 'LIC MF NIFTY 50 INDEX FUND',\n",
    "#  54: 'LIC MF NIFTY NEXT 50 INDEX FUND',\n",
    "#  55: 'LIC MF G2LD EXCHANGE TRADED FUND',\n",
    "#  56: 'LIC MF GSLD ETF',\n",
    "#  57: 'LIC MF Large Cap Fund',\n",
    "#  58: 'LIC MF Mid cap Fund',\n",
    "#  59: 'LIC MF Focused Fund',\n",
    "#  60: 'Lic MF ELSs Tax Saver',\n",
    "#  61: 'LIC MF Arbitrage Fund',\n",
    "#  63: 'LIC MF Nifty Midcap 100 ETF',\n",
    "#  64: 'LIC MF Large Cap Fund',\n",
    "#  65: 'LIC MF Dividend Yield Fund',\n",
    "#  66: 'LIC MEF Healthcare Fund',\n",
    "#  67: 'LIC MF Unit Linked Insurance Scheme __LIC MF Overnight Fund',\n",
    "#  68: 'LIC MF Short Duration Fund',\n",
    "#  69: 'LIC MF BSE Sensex ETF',\n",
    "#  70: 'LIC MF BSE Sensex Index Fund',\n",
    "#  73: 'LIC MF Healthcare Fund',\n",
    "#  74: 'LIC MF Liquid Fund',\n",
    "#  75: 'LIC MF Nifty 50 ETF',\n",
    "#  76: 'LIC Mutual Fund'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NIPPON DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def via_block(path:str):\n",
    "    pattern = r\"FUNDS AT A GLANCE\"\n",
    "    amc_pattern = \"^(Nippon India|CPSE).*(?=Plan|Next 50|Sensex|Fund|Path|ETF|FOF|EOF|Funds|$)\"\n",
    "    imp_pages = []\n",
    "    with fitz.open(path) as doc:\n",
    "        for pgn, page in enumerate(doc):\n",
    "                page_blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "                sorted_blocks = sorted(page_blocks, key=lambda x: (x[\"bbox\"][1], x[\"bbox\"][0]))\n",
    "                for block_count, block in enumerate(sorted_blocks[:10]):\n",
    "                    if \"lines\" not in block:\n",
    "                        continue\n",
    "                    for line in block[\"lines\"]:\n",
    "                        for span in line[\"spans\"]:\n",
    "                            text = span[\"text\"].strip()\n",
    "                            if re.match(pattern,text):\n",
    "                                imp_pages.append(pgn)\n",
    "                                \n",
    "        amc_fund = defaultdict(list)\n",
    "    \n",
    "        for pgn in imp_pages:\n",
    "            page = doc[pgn]\n",
    "            page_blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "            sorted_blocks = sorted(page_blocks, key=lambda x: (x[\"bbox\"][1], x[\"bbox\"][0]))\n",
    "            for block_count, block in enumerate(sorted_blocks):\n",
    "                if \"lines\" not in block:\n",
    "                    continue\n",
    "                for line in block[\"lines\"]:\n",
    "                    for span in line[\"spans\"]:\n",
    "                        text = span[\"text\"].strip()\n",
    "                        color = span['color']\n",
    "                        if re.match(amc_pattern,text)and color == -1:\n",
    "                            # matches = re.findall(amc_pattern,text)\n",
    "                            amc_fund[pgn].append(text)\n",
    "                            \n",
    "    return imp_pages, dict(amc_fund)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages,amc = via_block(sample_path)\n",
    "pages = list(map(str,[x+1 for x in pages]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_scheme = defaultdict(list)\n",
    "for key, value in amc.items():\n",
    "    # print(key)\n",
    "    set1 = ['Scheme Name']+value[:4]\n",
    "    set2 = ['Scheme Name']+value[4:]\n",
    "    final_scheme[key+1].append(set1)\n",
    "    final_scheme[key+1].append(set2)\n",
    "\n",
    "final_scheme = dict(final_scheme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_pages = \",\".join(pages)\n",
    "tables = camelot.read_pdf(sample_path,pages=imp_pages, flavor=\"lattice\", line_scale = 40)  #table_areas = [\"0,0,580,690\"]            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "with pd.ExcelWriter(\"merged_tables.xlsx\", engine=\"openpyxl\") as writer:\n",
    "    count = 0  # Toggle between 0 and 1\n",
    "    \n",
    "    for i, table in enumerate(tables):\n",
    "        df = table.df\n",
    "        if df.shape[1] < 3:\n",
    "            continue\n",
    "\n",
    "    \n",
    "        df = df.map(lambda x: \" \".join(x.split(\"\\n\")).strip())\n",
    "        df = df.map(lambda x: np.nan if not x.strip() else x)\n",
    "        df.set_index(df.columns[0], inplace=True)\n",
    "\n",
    "        for check in [\"Scheme Name\", \"Market Capitalization\"]:\n",
    "            if check in df.index:\n",
    "                df.drop(check, inplace=True)\n",
    "                \n",
    "        df_cleaned = df[~df.index.isna()]\n",
    "        df_cleaned = df_cleaned[df_cleaned.index != \"\"]\n",
    "        df_cleaned = df_cleaned.reset_index()\n",
    "        df_fill = df_cleaned.ffill(axis=1)\n",
    "\n",
    "\n",
    "        sch_vals = final_scheme[table.page][count]\n",
    "        count = 1 - count  # Toggle between 0 and 1\n",
    "\n",
    "        if len(sch_vals) == 5:\n",
    "            df_fill.loc[-1] = sch_vals \n",
    "            df_fill = df_fill.sort_index().reset_index(drop=True)\n",
    "\n",
    "        # Write to a new sheet\n",
    "        df_fill.to_excel(writer, sheet_name=f\"Table_{i+1}\", index=False)\n",
    "\n",
    "print(\"All tables saved in separate sheets in 'merged_tables.xlsx' 🚀\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hello =  {\n",
    "    \"number\": 0,\n",
    "    \"type\": 0,\n",
    "    \"bbox\": (0,0,0,0), #406.72119140625, 439.4930419921875, 565.697265625, 484.5830383300781\n",
    "    \"lines\": [\n",
    "        {\n",
    "            \"spans\": [\n",
    "                {\n",
    "                    \"size\": 30.0,\n",
    "                    \"flags\": 20,\n",
    "                    \"font\": \"Montserrat-Regular\", #set this\n",
    "                    \"color\": -1, #set this\n",
    "                    \"ascender\": 1.0429999828338623,\n",
    "                    \"descender\": -0.2619999945163727,\n",
    "                    \"text\": \"DUMMYDUMMYDUMMYDUMMY\",\n",
    "                    \"origin\": (406.72119140625, 458.26702880859375),\n",
    "                    \"bbox\": (0,0,0,0), #406.72119140625,439.4930419921875,565.697265625,462.9830322265625,\n",
    "                }\n",
    "            ],\n",
    "            \"wmode\": 0,\n",
    "            \"dir\": (1.0, 0.0),\n",
    "            \"bbox\": (0,0,0,0), #406.72119140625,439.4930419921875,565.697265625,462.9830322265625,\n",
    "        },\n",
    "        \n",
    "    ],\n",
    "},  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
