{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Config already loaded. Skipping re-initialization.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pprint, json, math, os, sys\n",
    "# dir_path = \"C:\\\\Users\\\\rando\\\\OneDrive\\\\Documents\\\\mywork-repo\"\n",
    "# fund_path = r\"C:\\Users\\rando\\OneDrive\\Documents\\PDFDrive\\Mar25\"\n",
    "# dry_path = r\"C:\\Users\\rando\\OneDrive\\Documents\\mywork-repo\\notebook\\DryRun.pdf\"\n",
    "\n",
    "dir_path = \"C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\office-work\\\\mywork-repo\"\n",
    "fund_path = \"C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\Mar 25\"\n",
    "dry_path = r\"C:\\Users\\Kaustubh.keny\\Projects\\office-work\\mywork-repo\\notebook\\DryRun.pdf\"\n",
    "sys.path.append(os.path.abspath(dir_path))\n",
    "from app.config_loader import load_config_once\n",
    "conf = load_config_once()\n",
    "\n",
    "\n",
    "import fitz, pdfplumber, ocrmypdf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "from app.utils import Helper\n",
    "from app.parse_regex import *\n",
    "\n",
    "dry_path = r'DryRun.pdf'\n",
    "fin_path = r'\\data\\input\\financial_indices.xlsx'\n",
    "mutual_fund = Helper.get_fund_paths(fund_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_clipped_data(input:str, pages:list, bboxes:list):\n",
    "        \n",
    "        document = fitz.open(input)\n",
    "        final_list = []\n",
    "    \n",
    "        for pgn in pages:\n",
    "            page = document[pgn]\n",
    "            \n",
    "            all_blocks = [] #store every data from bboxes\n",
    "            \n",
    "            for bbox in bboxes:\n",
    "                blocks, seen_blocks = [], set()  #store unique blocks based on content and bbox\n",
    "                \n",
    "                page_blocks = page.get_text('dict', clip=bbox)['blocks']\n",
    "                for block in page_blocks:\n",
    "                    if block['type'] == 0 and 'lines' in block: #type 0 means text block\n",
    "                        #hash_key\n",
    "                        block_key = (tuple(block['bbox']), tuple(tuple(line['spans'][0]['text'] for line in block['lines'])))\n",
    "                        if block_key not in seen_blocks:\n",
    "                            seen_blocks.add(block_key)\n",
    "                            blocks.append(block)\n",
    "\n",
    "                sorted_blocks = sorted(blocks, key=lambda x: (x['bbox'][1], x['bbox'][0]))\n",
    "                all_blocks.append(sorted_blocks)\n",
    "\n",
    "            final_list.append({\n",
    "                \"pgn\": pgn,\n",
    "                \"block\": all_blocks #will be list[list,list,..]\n",
    "            })\n",
    "\n",
    "        document.close()\n",
    "        return final_list\n",
    "    \n",
    "def extract_data_relative_line(path: str, line_x: float, side: str):\n",
    "    doc = fitz.open(path)\n",
    "    pages = doc.page_count\n",
    "\n",
    "    final_list = []\n",
    "\n",
    "    for pgn in range(pages):\n",
    "        page = doc[pgn]\n",
    "\n",
    "        blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "        sorted_blocks = sorted(blocks, key=lambda x: (x[\"bbox\"][1], x[\"bbox\"][0]))\n",
    "        extracted_blocks = []\n",
    "\n",
    "        # Keep track of blocks to avoid duplicates\n",
    "        added_blocks = set()\n",
    "\n",
    "        for block in sorted_blocks:\n",
    "            block_id = id(block)  # Unique identifier for the block\n",
    "\n",
    "            for line in block.get(\"lines\", []):\n",
    "                for span in line.get(\"spans\", []):\n",
    "                    origin = span[\"origin\"]\n",
    "                    x0, _ = origin\n",
    "\n",
    "                    # Check the side condition\n",
    "                    if side == \"left\" and x0 < line_x and block_id not in added_blocks:\n",
    "                        extracted_blocks.append(block)\n",
    "                        added_blocks.add(block_id)  # Mark block as added\n",
    "                    elif side == \"right\" and x0 > line_x and block_id not in added_blocks:\n",
    "                        extracted_blocks.append(block)\n",
    "                        added_blocks.add(block_id)  # Mark block as added\n",
    "\n",
    "      \n",
    "        final_list.append({\n",
    "            \"pgn\": pgn,\n",
    "            \"blocks\": extracted_blocks\n",
    "        })\n",
    "\n",
    "    doc.close()\n",
    "\n",
    "    return final_list\n",
    "  \n",
    "def get_clipped_data(input:str, bboxes:list[set], *args):\n",
    "    \n",
    "        document = fitz.open(input)\n",
    "        final_list = []\n",
    "        if args:\n",
    "            pages = list(args)\n",
    "        else:\n",
    "            pages = [i for i in document.page_count]\n",
    "        \n",
    "        for pgn in pages:\n",
    "            page = document[pgn]\n",
    "\n",
    "            blocks = []\n",
    "            for bbox in bboxes:\n",
    "                blocks.extend(page.get_text('dict', clip = bbox)['blocks']) #get all blocks\n",
    "            \n",
    "            filtered_blocks = [block for block in blocks if block['type']== 0 and 'lines' in block]\n",
    "            # sorted_blocks = sorted(filtered_blocks, key= lambda x: (x['bbox'][1], x['bbox'][0]))\n",
    "            \n",
    "             # Extract text from sorted blocks\n",
    "            extracted_text = []\n",
    "            for block in filtered_blocks:\n",
    "                block_text = []\n",
    "                for line in block['lines']:\n",
    "                    line_text = \" \".join(span['text'] for span in line['spans'])\n",
    "                    block_text.append(line_text)\n",
    "                extracted_text.append(\"\\n\".join(block_text))\n",
    "            \n",
    "            final_list.append({\n",
    "            \"pgn\": pgn,\n",
    "            \"block\": filtered_blocks,\n",
    "            \"text\": extracted_text\n",
    "            })\n",
    "            \n",
    "            \n",
    "        document.close()\n",
    "        return final_list\n",
    "    \n",
    "def extract_clipped_text_all_pages(pdf_path, clip_coords):\n",
    "    results = {}\n",
    "    doc = fitz.open(pdf_path)\n",
    "    clip_rect = fitz.Rect(*clip_coords)\n",
    "    try:\n",
    "        for page_number, page in enumerate(doc):\n",
    "            text = page.get_text(\"text\", clip=clip_rect).strip()\n",
    "            results[page_number] = text\n",
    "    finally:\n",
    "        doc.close()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample_path = mutual_fund[\"Edelweiss Mutual Fund\"]\n",
    "rp = r\"\\pdfs\\unifi1747199772321.pdf\"\n",
    "rp = r\"\\pdfs\\unifi1747373013796.pdf\"\n",
    "rp = r\"\\pdfs\\unifi1747199156275.pdf\"\n",
    "dsp1 = \"\\\\pdfs\\\\May SID\\\\DSP Mutual Fund\\\\8_17160_May-2025_1748246407_SID.pdf\"\n",
    "tata1 = r\"\\pdfs\\May SID\\Tata Mutual Fund\\38_17082_May-2025_1746423571_SID.pdf\"\n",
    "nip1 = \"\\\\pdfs\\\\May SID\\\\Nippon India Mutual Fund\\\\33_17142_May-2025_1747631690_SID.pdf\"\n",
    "sbi1 = \"\\\\pdfs\\\\May SID\\\\SBI Mutual Fund\\\\35_17126_May-2025_1747282059_SID.pdf\",\n",
    "samc1 = \"\\\\pdfs\\\\May SID\\\\Samco Mutual Fund\\\\58_17166_May-2025_1748246684_SID.pdf\"\n",
    "sample_path = dir_path + samc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified PDF saved to: DryRun.pdf\n"
     ]
    }
   ],
   "source": [
    "lines = [\n",
    "    ((260, 0), (260, 812)),# Vertical line\n",
    "    ((0, 450), (812, 450))\n",
    "]\n",
    "pages = [12, 14,16]\n",
    "bboxes = [[0, 120, 180, 812],[180, 85, 360, 812]] #[(0, 85, 180, 812),(180, 85, 360, 812),(0,100,270,812),(0,100,350,812)]\n",
    "pages = [i for i in range(1,110)]\n",
    "Helper.draw_lines_on_pdf(sample_path, lines, bboxes, pages, dry_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clip_coords = (484, 50,600, 100)  # (x0, y0, x1, y1)\n",
    "\n",
    "clipped_texts = extract_clipped_text_all_pages(sample_path, clip_coords)\n",
    "\n",
    "for page_num, text in clipped_texts.items():\n",
    "    print(f\"Page {page_num}:\\n{text}\\n{'-'*40}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _detect_table_start_by_keywords(df: pd.DataFrame, keywords: list):\n",
    "#     normalized_keywords = [re.sub(r\"\\s+\", \" \", kw.strip().lower()) for kw in keywords]\n",
    "\n",
    "#     for i in range(df.shape[0]):\n",
    "#         for j in range(df.shape[1]):\n",
    "#             cell = str(df.iat[i, j]).strip().lower()\n",
    "#             cell = re.sub(r\"\\s+\", \" \", cell)\n",
    "#             if any(kw in cell for kw in normalized_keywords):\n",
    "#                 return i, j\n",
    "#     return 0, 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Helper.get_all_pdf_data(sample_path)s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proper_fund_names(path: str):\n",
    "    pattern = \"(HDFC.*?(?:FUND|Fund|ETF|FO?o?F)\\\\s*(?:of Funds?|.+?Plan|Fund of Funds?|Fund)?)\"\n",
    "    title = {}   \n",
    "    with fitz.open(path) as doc:\n",
    "        for pgn, page in enumerate(doc):\n",
    "            text = \" \".join(page.get_text(\"text\", clip=(0, 0, 400, 60)).split(\"\\n\"))\n",
    "            text = re.sub(\"[^A-Za-z0-9\\\\s\\\\-\\\\(\\\\).,]+|\\u2028\", \"\", text).strip()\n",
    "            print(text)\n",
    "            if matches := re.findall(pattern, text, re.DOTALL):\n",
    "                title[pgn] = \" \".join([_ for _ in matches[0].strip().split(\" \") if _])\n",
    "                print(pgn,matches[0])\n",
    "    return title\n",
    "title = get_proper_fund_names(sample_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os ,json \n",
    "import pandas as pd\n",
    "\n",
    "full_name = set()\n",
    "split_name = set()\n",
    "MANAGER_REGEX = FundRegex().MANAGER_STOP_WORDS\n",
    "for a,b,files in os.walk(r\"C:\\Users\\Kaustubh.keny\\Projects\\office-work\\mywork-repo\\sql_learn\\json\\MAR25DATA\"):\n",
    "    for paths in files:\n",
    "        sample_path = os.path.join(os.getcwd(),\"..\",\"sql_learn\",\"json\",\"MAR25DATA\",paths)\n",
    "        # print(sample_path)\n",
    "        try:\n",
    "            with open(sample_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                data = json.load(file)\n",
    "                for k,scheme in data.items():\n",
    "                    # print(k)\n",
    "                    if \"fund_manager\" in scheme:\n",
    "                        for entry in scheme['fund_manager']:\n",
    "                            # print(entry['name'])\n",
    "                            name = entry['name']\n",
    "                            for regex_ in MANAGER_REGEX:\n",
    "                                name = re.sub(f\"\\\\b{regex_}\\\\b|[^A-Za-z\\\\s0-9]+\",\"\",name, re.IGNORECASE)\n",
    "                                name = re.sub(r\"\\s+\",\" \",name)\n",
    "                            full_name.add(name)\n",
    "                            printthis = name if name.strip() else \"EMPTY\"\n",
    "                            # print(f\"<<{printthis}>>\")\n",
    "                            if printthis == \"EMPTY\":\n",
    "                                print(k,printthis,entry['name'])\n",
    "                            # split_name.add(name.split(' '))\n",
    "                    # print(scheme.keys())\n",
    "        except Exception as e:\n",
    "            print(f\"NEVER MIND {e}\")\n",
    "    # print(files)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
