{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Loaded config with output_folder = None\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pprint, json, math, os, sys\n",
    "# dir_path = \"C:\\\\Users\\\\rando\\\\OneDrive\\\\Documents\\\\mywork-repo\"\n",
    "# fund_path = r\"C:\\Users\\rando\\OneDrive\\Documents\\PDFDrive\\Mar25\"\n",
    "# dry_path = r\"C:\\Users\\rando\\OneDrive\\Documents\\mywork-repo\\notebook\\DryRun.pdf\"\n",
    "\n",
    "dir_path = \"C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\office-work\\\\mywork-repo\"\n",
    "fund_path = \"C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\Mar 25\"\n",
    "dry_path = r\"C:\\Users\\Kaustubh.keny\\Projects\\office-work\\mywork-repo\\notebook\\DryRun.pdf\"\n",
    "sys.path.append(os.path.abspath(dir_path))\n",
    "from app.config_loader import load_config_once\n",
    "conf = load_config_once()\n",
    "\n",
    "\n",
    "import fitz, pdfplumber, ocrmypdf,camelot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "from app.utils import Helper\n",
    "from app.parse_regex import *\n",
    "from app.parse_table import *\n",
    "\n",
    "dry_path = r'DryRun.pdf'\n",
    "fin_path = r'\\data\\input\\financial_indices.xlsx'\n",
    "mutual_fund = Helper.get_fund_paths(fund_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m bboxes \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m120\u001b[39m, \u001b[38;5;241m180\u001b[39m, \u001b[38;5;241m812\u001b[39m],[\u001b[38;5;241m180\u001b[39m, \u001b[38;5;241m85\u001b[39m, \u001b[38;5;241m360\u001b[39m, \u001b[38;5;241m812\u001b[39m]] \u001b[38;5;66;03m#[(0, 85, 180, 812),(180, 85, 360, 812),(0,100,270,812),(0,100,350,812)]\u001b[39;00m\n\u001b[0;32m      8\u001b[0m pages \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m110\u001b[39m)]\n\u001b[1;32m----> 9\u001b[0m Helper\u001b[38;5;241m.\u001b[39mdraw_lines_on_pdf(\u001b[43msample_path\u001b[49m, lines, bboxes, pages, dry_path)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sample_path' is not defined"
     ]
    }
   ],
   "source": [
    "lines = [\n",
    "    ((110, 0), (110, 812)),# Vertical line\n",
    "    ((0, 350), (812, 350)),\n",
    "    ((570, 0), (570, 812))\n",
    "]\n",
    "pages = [12, 14,16]\n",
    "bboxes = [[0, 120, 180, 812],[180, 85, 360, 812]] #[(0, 85, 180, 812),(180, 85, 360, 812),(0,100,270,812),(0,100,350,812)]\n",
    "pages = [i for i in range(1,110)]\n",
    "Helper.draw_lines_on_pdf(sample_path, lines, bboxes, pages, dry_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NIPPON\n",
    "# nip_jan = r\"C:\\Users\\Kaustubh.keny\\Projects\\PDF\\Jan 25\\Nippon India Mutual Fund\\33_31-Jan-25_FS.pdf\"\n",
    "# nip_feb = \"\"\n",
    "# nip_mar = r\"C:\\Users\\Kaustubh.keny\\Projects\\PDF\\Mar 25\\Nippon India Mutual Fund\\33_31-Mar-25_FS.pdf\"\n",
    "# nip_apr = r\"C:\\Users\\Kaustubh.keny\\Projects\\PDF\\Apr 25\\Nippon India Mutual Fund\\33_30-Apr-25_FS.pdf\"\n",
    "# table_parser = TableParser()\n",
    "# tables = camelot.read_pdf(nip_apr,flavor=\"stream\",pages=\"129-140\")\n",
    "# dfs = pd.concat([table.df for table in tables], ignore_index=True)\n",
    "\n",
    "# sr1 = table_parser._get_matching_row_indices(dfs,[\"Nippon.+?Fund\",\"Scheme\\\\s*Name\"],thresh=2)\n",
    "# sr2 = table_parser._get_matching_row_indices(dfs,[\"minimum application\"],thresh=1)\n",
    "# sr2_expanded = {i for idx in sr2 for i in range(idx, idx + 5)}\n",
    "# sr1_expanded = {i for idx in sr1 for i in range(idx, idx + 1)}\n",
    "\n",
    "# all_indices = sr1_expanded | sr2_expanded\n",
    "# valid_indices = sorted(i for i in all_indices if i in dfs.index)\n",
    "\n",
    "# filtered_df = dfs.loc[valid_indices].reset_index()\n",
    "# for idx, rows in filtered_df.iterrows():\n",
    "#     row_val = \" \".join([str(i) for i in rows])\n",
    "#     row_val = SidKimRegex()._normalize_alphanumeric(row_val)\n",
    "#     # print(row_val)\n",
    "#     matches = re.findall(r\"Nippon.+?(?:Funds?|ETF|Path|Saver|active|financial|allocation|tunities|duration|psu debt|advantage|small cap 250)\\s*(?:of Funds?|Fund of Funds?|Funds?|.+?Plan|FoF)?\",row_val, re.IGNORECASE)\n",
    "#     if matches:\n",
    "#         print(idx, len(matches))\n",
    "#         print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BAJAJAJ\n",
    "jan = r\"C:\\Users\\Kaustubh.keny\\Projects\\PDF\\Jan 25\\Bajaj finserv Mutual Fund\\59_31-Jan-25_FS.pdf\"\n",
    "feb = r\"C:\\Users\\Kaustubh.keny\\Projects\\PDF\\Feb 25\\Bajaj finserv Mutual Fund\\59_28-Feb-25_FS.pdf\"\n",
    "mar = r\"C:\\Users\\Kaustubh.keny\\Projects\\PDF\\Mar 25\\Bajaj finserv Mutual Fund\\59_31-Mar-25_FS.pdf\"\n",
    "apr = r\"C:\\Users\\Kaustubh.keny\\Projects\\PDF\\Apr 25\\Bajaj finserv Mutual Fund\\59_30-Apr-25_FS.pdf\"\n",
    "\n",
    "\n",
    "table_parser = TableParser()\n",
    "tables = camelot.read_pdf(feb,flavor=\"lattice\",pages=\"14-17\")\n",
    "dfs = pd.concat([table.df for table in tables], ignore_index=True)\n",
    "sc1 = table_parser._get_matching_col_indices(dfs,[\"Bajaj.+?Fund\",\"SCHEME\\\\s*NAME\"],thresh=20)\n",
    "sc2 = table_parser._get_matching_col_indices(dfs,[\"Jensen\",\"Standard\\\\s*Deviation\",\"Information\\\\s*ratio\",\"Portfolio\\\\s*Quants\",\"Tracking Error\",\"YTM\",\"Average\\\\s*Maturity\",\"Sharpe\"],thresh=10)\n",
    "# sc2 = table_parser._get_matching_col_indices(dfs,[\"Debt\\\\s*Quant\",\"Modified\\\\s*Duration\",\"Macaulay\",\"YTM\",\"Average\\\\s*Maturity\",\"Sharpe\"],thresh=20)\n",
    "all_cols = sorted(set(sc1)) + list(range(sc2[0], dfs.shape[1]))\n",
    "fdf = dfs.iloc[:, all_cols]\n",
    "fdf.columns = [\"MUTUAL_FUND\"] + [f\"METRICS_{i}\" for i in range(1, fdf.shape[1])]\n",
    "hdfc_pattern = re.compile(\n",
    "    r\"(Baj.+?(?:FUNDS?|ETF|PATH|INDEX|SAVER)\\s*(?:OF FUNDS?|FUND|FUND OF FUNDS|FOF|.+?PLAN|.+?GROWTH)?)\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "fdf.MUTUAL_FUND = table_parser._clean_series(fdf.MUTUAL_FUND,[\"normalize_alphanumeric\"])\n",
    "fdf.MUTUAL_FUND = fdf.MUTUAL_FUND.apply(lambda x: hdfc_pattern.findall(x)[0] if isinstance(x, str) and hdfc_pattern.findall(x) else \"\")\n",
    "fdf = table_parser._clean_dataframe(fdf,[\"newline_to_space\",\"str_to_pd_NA\"])\n",
    "fdf = fdf.dropna(axis=0, how=\"all\").dropna(axis=1, how=\"all\")\n",
    "fdf =table_parser._clean_dataframe(fdf,['NA_to_str'])\n",
    "\n",
    "data = {}\n",
    "for idx, rows in fdf.iterrows():\n",
    "    values = list(rows)\n",
    "    main_scheme_name = str(values[0]).strip() if not pd.isna(values[0]) else \"\"\n",
    "    if main_scheme_name:\n",
    "        temp = main_scheme_name\n",
    "        if temp not in data:\n",
    "            data[temp] = {\"metrics\": []}\n",
    "        data[temp][\"metrics\"].append(\" \".join(map(str, values[1:])))\n",
    "    \n",
    "    if temp:\n",
    "        data[temp][\"metrics\"].append(\" \".join(map(str, values)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HDFC\n",
    "table_parser = TableParser()\n",
    "tables = camelot.read_pdf(hdfc1,flavor=\"lattice\",pages=\"91-93\")\n",
    "dfs = pd.concat([table.df for table in tables], ignore_index=True)\n",
    "sc1 = table_parser._get_matching_col_indices(dfs,[\"HDFC.+?Fund\"],thresh=20)\n",
    "sc2 = table_parser._get_matching_col_indices(dfs,[\"MINIMUM\\\\s*APPLICATION\\\\s*AMOUNT\",\"Additional\\\\s*Purchase\"], thresh=20)\n",
    "\n",
    "print(\"Matched columns:\", sc1,sc2)\n",
    "all_cols = list(set(sc1 + sc2))\n",
    "fdf = dfs.iloc[:, all_cols]\n",
    "fdf.columns = [\"MUTUAL_FUND\",\"MIN_ADD\"]\n",
    "hdfc_pattern = re.compile(\n",
    "    r\"(HDFC.+?(?:FUNDS?|ETF|PATH|INDEX|SAVER)\\s*(?:OF FUNDS?|FUND OF FUNDS|FOF|.+?PLAN)?)\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "fdf.MUTUAL_FUND = table_parser._clean_series(fdf.MUTUAL_FUND,[\"normalize_alphanumeric\"])\n",
    "fdf.MUTUAL_FUND = fdf.MUTUAL_FUND.apply(lambda x: hdfc_pattern.findall(x)[0] if isinstance(x, str) and hdfc_pattern.findall(x) else x)\n",
    "fdf = table_parser._clean_dataframe(fdf,[\"newline_to_space\",\"str_to_pd_NA\"])\n",
    "fdf = fdf.dropna(axis=0, how=\"all\").dropna(axis=1, how=\"all\")\n",
    "fdf =table_parser._clean_dataframe(fdf,['NA_to_str'])\n",
    "\n",
    "data = {}\n",
    "for idx, rows in fdf.iterrows():\n",
    "    values = list(rows)\n",
    "    main_scheme_name = values[0]\n",
    "    if main_scheme_name not in data:\n",
    "        data[main_scheme_name] = {\"min_add\":values[1]}\n",
    "    else:\n",
    "        data[main_scheme_name].update({\"min_add_one\":values[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DSP\n",
    "jan = r\"C:\\Users\\Kaustubh.keny\\Projects\\PDF\\Jan 25\\DSP Mutual Fund\\8_31-Jan-25_FS.pdf\"\n",
    "feb = r\"C:\\Users\\Kaustubh.keny\\Projects\\PDF\\Feb 25\\DSP Mutual Fund\\8_28-Feb-25_FS.pdf\"\n",
    "mar = r\"C:\\Users\\Kaustubh.keny\\Projects\\PDF\\Mar 25\\DSP Mutual Fund\\8_31-Mar-25_FS.pdf\"\n",
    "apr = r\"C:\\Users\\Kaustubh.keny\\Projects\\PDF\\Apr 25\\DSP Mutual Fund\\8_30-Apr-25_FS.pdf\"\n",
    "\n",
    "table_parser = TableParser()\n",
    "tables = camelot.read_pdf(apr,flavor=\"lattice\",pages=\"107-123\")\n",
    "dfs = pd.concat([table.df for table in tables], ignore_index=True)\n",
    "sc1 = table_parser._get_matching_col_indices(dfs,[\"DSP.+?Fund\"],thresh=12)\n",
    "sc2 = table_parser._get_matching_col_indices(dfs,[\"REGULAR\\\\s+PLAN\",\"DIRECT\\\\s+PLAN\"], thresh=12)\n",
    "sc3 = table_parser._get_matching_col_indices(dfs,[\"Managing this scheme\",\"total work experience\"],thresh=12)\n",
    "print(\"Matched columns:\", sc1,sc2,sc3)\n",
    "all_cols = list(set(sc1 + sc2 + sc3))\n",
    "fdf = dfs.iloc[:, all_cols]\n",
    "fdf[\"LOAD_STRUCTURE\"] = fdf.iloc[:, -1]\n",
    "fdf.columns = [\"MUTUAL_FUND\",\"FUND_MANAGER\",\"MIN_ADD\",\"LOAD_STRUCTURE\"]\n",
    "\n",
    "dsp_pattern = re.compile(\n",
    "    r\"(DSP.+?(?:FUNDS?|ETF|PATH|INDEX|SAVER)\\s*(?:OF FUNDS?|FUNDs?|FUND OF FUNDS?|FOF|.+?PLAN)?)\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "fdf.MUTUAL_FUND = table_parser._clean_series(fdf.MUTUAL_FUND,[\"normalize_alphanumeric\"])\n",
    "fdf.MUTUAL_FUND = fdf.MUTUAL_FUND.apply(lambda x: dsp_pattern.findall(x)[0] if isinstance(x, str) and dsp_pattern.findall(x) else pd.NA)\n",
    "fdf = table_parser._clean_dataframe(fdf,[\"newline_to_space\",\"str_to_pd_NA\"])\n",
    "fdf = fdf.dropna(axis=0, how=\"all\").dropna(axis=1, how=\"all\")\n",
    "fdf =table_parser._clean_dataframe(fdf,['NA_to_str'])\n",
    "\n",
    "data = {}\n",
    "for idx, rows in fdf.iterrows():\n",
    "    values = list(rows)\n",
    "    main_scheme_name = values[0]\n",
    "    if main_scheme_name not in data:\n",
    "        data[main_scheme_name] = {\"fund_manager\":values[1],\"load_structure\":values[3],\"min_add\":values[2]}\n",
    "    else:\n",
    "         data[main_scheme_name].update({\"fund_manager_one\":values[1],\"load_structure_one\":values[3],\"min_add_one\":values[2]})\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dsp flexi cap fund', 'dsp large mid cap fund ', 'dsp large cap fund ', 'dsp mid cap fund', 'dsp india t i g e r fund ', 'dsp small cap fund', 'dsp focused fund ', 'dsp natural resources and new energy fund', 'dsp elss tax saver fund', 'dsp healthcare fund', 'dsp quant fund ', 'dsp value fund', 'dsp banking financial services fund', 'dsp multicap fund', 'dsp business cycle fund', 'dsp 10y g sec fund', 'dsp bond fund', 'dsp banking psu debt fund', 'dsp credit risk fund ', 'dsp gilt fund ', 'dsp savings fund', 'dsp low duration fund', 'dsp short term fund', 'dsp strategic bond fund', 'dsp ultra short fund', 'dsp corporate bond fund', 'dsp floater fund', 'dsp overnight fund', 'dsp liquidity fund', 'dsp arbitrage fund', 'dsp dynamic asset allocation fund ', 'dsp aggressive hybrid fund ', 'dsp equity savings fund', 'dsp regular savings fund', 'dsp multi asset allocation fund', 'dsp nifty 50 equal weight index fund', 'dsp nifty next 50 index fund', 'dsp nifty 50 index fund', 'dsp nifty midcap 150 quality 50 index fund', 'dsp nifty sdl plus g sec jun 2028 30 70 index fund', 'dsp crisil sdl plus g sec apr 2033 50 50 index fund', 'dsp nifty sdl plus g sec sep 2027 50 50 index fund', 'dsp nifty smallcap250 quality 50 index fund', 'dsp nifty bank index fund', 'dsp nifty top 10 equal weight index fund', 'dsp bse sensex next 30 index fund', 'dsp nifty private bank index fund', 'dsp nifty 1d rate liquid etf', 'dsp nifty 50 equal weight etf', 'dsp nifty 50 etf', 'dsp nifty midcap 150 quality 50 etf', 'dsp silver etf', 'dsp nifty bank etf', 'dsp gold etf', 'dsp nifty it etf', 'dsp nifty psu bank etf', 'dsp nifty private bank etf', 'dsp bse sensex etf', 'dsp nifty healthcare etf', 'dsp bse liquid rate etf', 'dsp nifty top 10 equal weight etf', 'dsp bse sensex next 30 etf', 'dsp us flexible equity fund of fund', 'dsp global clean energy fund of fund', 'dsp world gold fund of fund', 'dsp us treasury fund of fund', 'dsp world mining fund of fund', 'dsp income plus arbitrage fund of fund', 'dsp global innovation fund of fund', 'dsp gold etf fund', '', 'dsp dynamic asset allocation fund']\n"
     ]
    }
   ],
   "source": [
    "print(list(data.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dsp_apr.json\",\"w\") as file:\n",
    "    json.dump(data,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proper_fund_names(path: str):\n",
    "    pattern = \"(HDFC.*?(?:FUND|Fund|ETF|FO?o?F)\\\\s*(?:of Funds?|.+?Plan|Fund of Funds?|Fund)?)\"\n",
    "    title = {}   \n",
    "    with fitz.open(path) as doc:\n",
    "        for pgn, page in enumerate(doc):\n",
    "            text = \" \".join(page.get_text(\"text\", clip=(0, 0, 400, 60)).split(\"\\n\"))\n",
    "            text = re.sub(\"[^A-Za-z0-9\\\\s\\\\-\\\\(\\\\).,]+|\\u2028\", \"\", text).strip()\n",
    "            print(text)\n",
    "            if matches := re.findall(pattern, text, re.DOTALL):\n",
    "                title[pgn] = \" \".join([_ for _ in matches[0].strip().split(\" \") if _])\n",
    "                print(pgn,matches[0])\n",
    "    return title\n",
    "title = get_proper_fund_names(sample_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os ,json \n",
    "import pandas as pd\n",
    "\n",
    "full_name = set()\n",
    "split_name = set()\n",
    "MANAGER_REGEX = FundRegex().MANAGER_STOP_WORDS\n",
    "for a,b,files in os.walk(r\"C:\\Users\\Kaustubh.keny\\Projects\\office-work\\mywork-repo\\sql_learn\\json\\MAR25DATA\"):\n",
    "    for paths in files:\n",
    "        sample_path = os.path.join(os.getcwd(),\"..\",\"sql_learn\",\"json\",\"MAR25DATA\",paths)\n",
    "        # print(sample_path)\n",
    "        try:\n",
    "            with open(sample_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                data = json.load(file)\n",
    "                for k,scheme in data.items():\n",
    "                    # print(k)\n",
    "                    if \"fund_manager\" in scheme:\n",
    "                        for entry in scheme['fund_manager']:\n",
    "                            # print(entry['name'])\n",
    "                            name = entry['name']\n",
    "                            for regex_ in MANAGER_REGEX:\n",
    "                                name = re.sub(f\"\\\\b{regex_}\\\\b|[^A-Za-z\\\\s0-9]+\",\"\",name, re.IGNORECASE)\n",
    "                                name = re.sub(r\"\\s+\",\" \",name)\n",
    "                            full_name.add(name)\n",
    "                            printthis = name if name.strip() else \"EMPTY\"\n",
    "                            # print(f\"<<{printthis}>>\")\n",
    "                            if printthis == \"EMPTY\":\n",
    "                                print(k,printthis,entry['name'])\n",
    "                            # split_name.add(name.split(' '))\n",
    "                    # print(scheme.keys())\n",
    "        except Exception as e:\n",
    "            print(f\"NEVER MIND {e}\")\n",
    "    # print(files)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalize_alphanumeric(text: str) -> str:\n",
    "    if not isinstance(text,str):\n",
    "        return text\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]+\", \" \", str(text))\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip().lower()\n",
    "\n",
    "def extract_clipped_data(input:str, pages:list, bboxes:list):\n",
    "        \n",
    "        document = fitz.open(input)\n",
    "        final_list = []\n",
    "    \n",
    "        for pgn in pages:\n",
    "            page = document[pgn]\n",
    "            \n",
    "            all_blocks = [] #store every data from bboxes\n",
    "            \n",
    "            for bbox in bboxes:\n",
    "                blocks, seen_blocks = [], set()  #store unique blocks based on content and bbox\n",
    "                \n",
    "                page_blocks = page.get_text('dict', clip=bbox)['blocks']\n",
    "                for block in page_blocks:\n",
    "                    if block['type'] == 0 and 'lines' in block: #type 0 means text block\n",
    "                        #hash_key\n",
    "                        block_key = (tuple(block['bbox']), tuple(tuple(line['spans'][0]['text'] for line in block['lines'])))\n",
    "                        if block_key not in seen_blocks:\n",
    "                            seen_blocks.add(block_key)\n",
    "                            blocks.append(block)\n",
    "\n",
    "                sorted_blocks = sorted(blocks, key=lambda x: (x['bbox'][1], x['bbox'][0]))\n",
    "                all_blocks.append(sorted_blocks)\n",
    "\n",
    "            final_list.append({\n",
    "                \"pgn\": pgn,\n",
    "                \"block\": all_blocks #will be list[list,list,..]\n",
    "            })\n",
    "\n",
    "        document.close()\n",
    "        return final_list\n",
    "    \n",
    "def extract_data_relative_line(path: str, line_x: float, side: str):\n",
    "    doc = fitz.open(path)\n",
    "    pages = doc.page_count\n",
    "\n",
    "    final_list = []\n",
    "\n",
    "    for pgn in range(pages):\n",
    "        page = doc[pgn]\n",
    "\n",
    "        blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "        sorted_blocks = sorted(blocks, key=lambda x: (x[\"bbox\"][1], x[\"bbox\"][0]))\n",
    "        extracted_blocks = []\n",
    "\n",
    "        # Keep track of blocks to avoid duplicates\n",
    "        added_blocks = set()\n",
    "\n",
    "        for block in sorted_blocks:\n",
    "            block_id = id(block)  # Unique identifier for the block\n",
    "\n",
    "            for line in block.get(\"lines\", []):\n",
    "                for span in line.get(\"spans\", []):\n",
    "                    origin = span[\"origin\"]\n",
    "                    x0, _ = origin\n",
    "\n",
    "                    # Check the side condition\n",
    "                    if side == \"left\" and x0 < line_x and block_id not in added_blocks:\n",
    "                        extracted_blocks.append(block)\n",
    "                        added_blocks.add(block_id)  # Mark block as added\n",
    "                    elif side == \"right\" and x0 > line_x and block_id not in added_blocks:\n",
    "                        extracted_blocks.append(block)\n",
    "                        added_blocks.add(block_id)  # Mark block as added\n",
    "\n",
    "      \n",
    "        final_list.append({\n",
    "            \"pgn\": pgn,\n",
    "            \"blocks\": extracted_blocks\n",
    "        })\n",
    "\n",
    "    doc.close()\n",
    "\n",
    "    return final_list\n",
    "  \n",
    "def get_clipped_data(input:str, bboxes:list[set], *args):\n",
    "    \n",
    "        document = fitz.open(input)\n",
    "        final_list = []\n",
    "        if args:\n",
    "            pages = list(args)\n",
    "        else:\n",
    "            pages = [i for i in document.page_count]\n",
    "        \n",
    "        for pgn in pages:\n",
    "            page = document[pgn]\n",
    "\n",
    "            blocks = []\n",
    "            for bbox in bboxes:\n",
    "                blocks.extend(page.get_text('dict', clip = bbox)['blocks']) #get all blocks\n",
    "            \n",
    "            filtered_blocks = [block for block in blocks if block['type']== 0 and 'lines' in block]\n",
    "            # sorted_blocks = sorted(filtered_blocks, key= lambda x: (x['bbox'][1], x['bbox'][0]))\n",
    "            \n",
    "             # Extract text from sorted blocks\n",
    "            extracted_text = []\n",
    "            for block in filtered_blocks:\n",
    "                block_text = []\n",
    "                for line in block['lines']:\n",
    "                    line_text = \" \".join(span['text'] for span in line['spans'])\n",
    "                    block_text.append(line_text)\n",
    "                extracted_text.append(\"\\n\".join(block_text))\n",
    "            \n",
    "            final_list.append({\n",
    "            \"pgn\": pgn,\n",
    "            \"block\": filtered_blocks,\n",
    "            \"text\": extracted_text\n",
    "            })\n",
    "            \n",
    "            \n",
    "        document.close()\n",
    "        return final_list\n",
    "    \n",
    "def extract_clipped_text_all_pages(pdf_path, clip_coords):\n",
    "    results = {}\n",
    "    doc = fitz.open(pdf_path)\n",
    "    clip_rect = fitz.Rect(*clip_coords)\n",
    "    try:\n",
    "        for page_number, page in enumerate(doc):\n",
    "            text = page.get_text(\"text\", clip=clip_rect).strip()\n",
    "            results[page_number] = text\n",
    "    finally:\n",
    "        doc.close()\n",
    "    return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
