{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded config with output_folder = None\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pprint, json, math, os, sys\n",
    "dir_path = \"C:\\\\Users\\\\rando\\\\OneDrive\\\\Documents\\\\mywork-repo\"\n",
    "fund_path = r\"C:\\Users\\rando\\OneDrive\\Documents\\PDFDrive\\Mar25\"\n",
    "dry_path = r\"C:\\Users\\rando\\OneDrive\\Documents\\mywork-repo\\notebook\\DryRun.pdf\"\n",
    "\n",
    "# dir_path = \"C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\office-work\\\\mywork-repo\"\n",
    "# fund_path = \"C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\Mar 25\"\n",
    "# dry_path = r\"C:\\Users\\Kaustubh.keny\\Projects\\office-work\\mywork-repo\\notebook\\DryRun.pdf\"\n",
    "sys.path.append(os.path.abspath(dir_path))\n",
    "from app.config_loader import load_config_once\n",
    "conf = load_config_once()\n",
    "\n",
    "\n",
    "import fitz, pdfplumber, ocrmypdf,camelot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "from app.utils import Helper\n",
    "from app.parse_regex import *\n",
    "from app.parse_table import *\n",
    "\n",
    "dry_path = r'DryRun.pdf'\n",
    "fin_path = r'\\data\\input\\financial_indices.xlsx'\n",
    "mutual_fund = Helper.get_fund_paths(fund_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_path = mutual_fund[\"Dsp Mutual Fund\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "789.1199951171875 595.4400024414062\n"
     ]
    }
   ],
   "source": [
    "with fitz.open(sample_path) as pdf:\n",
    "    page = pdf[0]\n",
    "    print(page.rect.height, page.rect.width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified PDF saved to: DryRun.pdf\n"
     ]
    }
   ],
   "source": [
    "lines = [\n",
    "    ((110, 0), (110, 812)),# Vertical line\n",
    "    ((0, 395), (812, 395)),\n",
    "    # ((395, 0), (395, 812))\n",
    "]\n",
    "pages = [12, 14,16]\n",
    "bboxes = [] #[(0, 85, 180, 812),(180, 85, 360, 812),(0,100,270,812),(0,100,350,812)]\n",
    "pages = [i for i in range(1,110)]\n",
    "Helper.draw_lines_on_pdf(sample_path, lines, bboxes, pages, dry_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NIPPON\n",
    "# nip_jan = r\"C:\\Users\\Kaustubh.keny\\Projects\\PDF\\Jan 25\\Nippon India Mutual Fund\\33_31-Jan-25_FS.pdf\"\n",
    "# nip_feb = \"\"\n",
    "# nip_mar = r\"C:\\Users\\Kaustubh.keny\\Projects\\PDF\\Mar 25\\Nippon India Mutual Fund\\33_31-Mar-25_FS.pdf\"\n",
    "# nip_apr = r\"C:\\Users\\Kaustubh.keny\\Projects\\PDF\\Apr 25\\Nippon India Mutual Fund\\33_30-Apr-25_FS.pdf\"\n",
    "# table_parser = TableParser()\n",
    "# tables = camelot.read_pdf(nip_apr,flavor=\"stream\",pages=\"129-140\")\n",
    "# dfs = pd.concat([table.df for table in tables], ignore_index=True)\n",
    "\n",
    "# sr1 = table_parser._get_matching_row_indices(dfs,[\"Nippon.+?Fund\",\"Scheme\\\\s*Name\"],thresh=2)\n",
    "# sr2 = table_parser._get_matching_row_indices(dfs,[\"minimum application\"],thresh=1)\n",
    "# sr2_expanded = {i for idx in sr2 for i in range(idx, idx + 5)}\n",
    "# sr1_expanded = {i for idx in sr1 for i in range(idx, idx + 1)}\n",
    "\n",
    "# all_indices = sr1_expanded | sr2_expanded\n",
    "# valid_indices = sorted(i for i in all_indices if i in dfs.index)\n",
    "\n",
    "# filtered_df = dfs.loc[valid_indices].reset_index()\n",
    "# for idx, rows in filtered_df.iterrows():\n",
    "#     row_val = \" \".join([str(i) for i in rows])\n",
    "#     row_val = SidKimRegex()._normalize_alphanumeric(row_val)\n",
    "#     # print(row_val)\n",
    "#     matches = re.findall(r\"Nippon.+?(?:Funds?|ETF|Path|Saver|active|financial|allocation|tunities|duration|psu debt|advantage|small cap 250)\\s*(?:of Funds?|Fund of Funds?|Funds?|.+?Plan|FoF)?\",row_val, re.IGNORECASE)\n",
    "#     if matches:\n",
    "#         print(idx, len(matches))\n",
    "#         print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BAJAJAJ\n",
    "jan = r\"C:\\Users\\Kaustubh.keny\\Projects\\PDF\\Jan 25\\Bajaj finserv Mutual Fund\\59_31-Jan-25_FS.pdf\"\n",
    "feb = r\"C:\\Users\\Kaustubh.keny\\Projects\\PDF\\Feb 25\\Bajaj finserv Mutual Fund\\59_28-Feb-25_FS.pdf\"\n",
    "mar = r\"C:\\Users\\Kaustubh.keny\\Projects\\PDF\\Mar 25\\Bajaj finserv Mutual Fund\\59_31-Mar-25_FS.pdf\"\n",
    "apr = r\"C:\\Users\\Kaustubh.keny\\Projects\\PDF\\Apr 25\\Bajaj finserv Mutual Fund\\59_30-Apr-25_FS.pdf\"\n",
    "\n",
    "\n",
    "table_parser = TableParser()\n",
    "tables = camelot.read_pdf(feb,flavor=\"lattice\",pages=\"14-17\")\n",
    "dfs = pd.concat([table.df for table in tables], ignore_index=True)\n",
    "sc1 = table_parser._get_matching_col_indices(dfs,[\"Bajaj.+?Fund\",\"SCHEME\\\\s*NAME\"],thresh=20)\n",
    "sc2 = table_parser._get_matching_col_indices(dfs,[\"Jensen\",\"Standard\\\\s*Deviation\",\"Information\\\\s*ratio\",\"Portfolio\\\\s*Quants\",\"Tracking Error\",\"YTM\",\"Average\\\\s*Maturity\",\"Sharpe\"],thresh=10)\n",
    "# sc2 = table_parser._get_matching_col_indices(dfs,[\"Debt\\\\s*Quant\",\"Modified\\\\s*Duration\",\"Macaulay\",\"YTM\",\"Average\\\\s*Maturity\",\"Sharpe\"],thresh=20)\n",
    "all_cols = sorted(set(sc1)) + list(range(sc2[0], dfs.shape[1]))\n",
    "fdf = dfs.iloc[:, all_cols]\n",
    "fdf.columns = [\"MUTUAL_FUND\"] + [f\"METRICS_{i}\" for i in range(1, fdf.shape[1])]\n",
    "hdfc_pattern = re.compile(\n",
    "    r\"(Baj.+?(?:FUNDS?|ETF|PATH|INDEX|SAVER)\\s*(?:OF FUNDS?|FUND|FUND OF FUNDS|FOF|.+?PLAN|.+?GROWTH)?)\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "fdf.MUTUAL_FUND = table_parser._clean_series(fdf.MUTUAL_FUND,[\"normalize_alphanumeric\"])\n",
    "fdf.MUTUAL_FUND = fdf.MUTUAL_FUND.apply(lambda x: hdfc_pattern.findall(x)[0] if isinstance(x, str) and hdfc_pattern.findall(x) else \"\")\n",
    "fdf = table_parser._clean_dataframe(fdf,[\"newline_to_space\",\"str_to_pd_NA\"])\n",
    "fdf = fdf.dropna(axis=0, how=\"all\").dropna(axis=1, how=\"all\")\n",
    "fdf =table_parser._clean_dataframe(fdf,['NA_to_str'])\n",
    "\n",
    "data = {}\n",
    "for idx, rows in fdf.iterrows():\n",
    "    values = list(rows)\n",
    "    main_scheme_name = str(values[0]).strip() if not pd.isna(values[0]) else \"\"\n",
    "    if main_scheme_name:\n",
    "        temp = main_scheme_name\n",
    "        if temp not in data:\n",
    "            data[temp] = {\"metrics\": []}\n",
    "        data[temp][\"metrics\"].append(\" \".join(map(str, values[1:])))\n",
    "    \n",
    "    if temp:\n",
    "        data[temp][\"metrics\"].append(\" \".join(map(str, values)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HDFC\n",
    "table_parser = TableParser()\n",
    "tables = camelot.read_pdf(hdfc1,flavor=\"lattice\",pages=\"91-93\")\n",
    "dfs = pd.concat([table.df for table in tables], ignore_index=True)\n",
    "sc1 = table_parser._get_matching_col_indices(dfs,[\"HDFC.+?Fund\"],thresh=20)\n",
    "sc2 = table_parser._get_matching_col_indices(dfs,[\"MINIMUM\\\\s*APPLICATION\\\\s*AMOUNT\",\"Additional\\\\s*Purchase\"], thresh=20)\n",
    "\n",
    "print(\"Matched columns:\", sc1,sc2)\n",
    "all_cols = list(set(sc1 + sc2))\n",
    "fdf = dfs.iloc[:, all_cols]\n",
    "fdf.columns = [\"MUTUAL_FUND\",\"MIN_ADD\"]\n",
    "hdfc_pattern = re.compile(\n",
    "    r\"(HDFC.+?(?:FUNDS?|ETF|PATH|INDEX|SAVER)\\s*(?:OF FUNDS?|FUND OF FUNDS|FOF|.+?PLAN)?)\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "fdf.MUTUAL_FUND = table_parser._clean_series(fdf.MUTUAL_FUND,[\"normalize_alphanumeric\"])\n",
    "fdf.MUTUAL_FUND = fdf.MUTUAL_FUND.apply(lambda x: hdfc_pattern.findall(x)[0] if isinstance(x, str) and hdfc_pattern.findall(x) else x)\n",
    "fdf = table_parser._clean_dataframe(fdf,[\"newline_to_space\",\"str_to_pd_NA\"])\n",
    "fdf = fdf.dropna(axis=0, how=\"all\").dropna(axis=1, how=\"all\")\n",
    "fdf =table_parser._clean_dataframe(fdf,['NA_to_str'])\n",
    "\n",
    "data = {}\n",
    "for idx, rows in fdf.iterrows():\n",
    "    values = list(rows)\n",
    "    main_scheme_name = values[0]\n",
    "    if main_scheme_name not in data:\n",
    "        data[main_scheme_name] = {\"min_add\":values[1]}\n",
    "    else:\n",
    "        data[main_scheme_name].update({\"min_add_one\":values[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DSP\n",
    "jan = r\"C:\\Users\\Kaustubh.keny\\Projects\\PDF\\Jan 25\\DSP Mutual Fund\\8_31-Jan-25_FS.pdf\"\n",
    "feb = r\"C:\\Users\\Kaustubh.keny\\Projects\\PDF\\Feb 25\\DSP Mutual Fund\\8_28-Feb-25_FS.pdf\"\n",
    "mar = r\"C:\\Users\\Kaustubh.keny\\Projects\\PDF\\Mar 25\\DSP Mutual Fund\\8_31-Mar-25_FS.pdf\"\n",
    "apr = r\"C:\\Users\\Kaustubh.keny\\Projects\\PDF\\Apr 25\\DSP Mutual Fund\\8_30-Apr-25_FS.pdf\"\n",
    "\n",
    "table_parser = TableParser()\n",
    "tables = camelot.read_pdf(apr,flavor=\"lattice\",pages=\"107-123\")\n",
    "dfs = pd.concat([table.df for table in tables], ignore_index=True)\n",
    "sc1 = table_parser._get_matching_col_indices(dfs,[\"DSP.+?Fund\"],thresh=12)\n",
    "sc2 = table_parser._get_matching_col_indices(dfs,[\"REGULAR\\\\s+PLAN\",\"DIRECT\\\\s+PLAN\"], thresh=12)\n",
    "sc3 = table_parser._get_matching_col_indices(dfs,[\"Managing this scheme\",\"total work experience\"],thresh=12)\n",
    "print(\"Matched columns:\", sc1,sc2,sc3)\n",
    "all_cols = list(set(sc1 + sc2 + sc3))\n",
    "fdf = dfs.iloc[:, all_cols]\n",
    "fdf[\"LOAD_STRUCTURE\"] = fdf.iloc[:, -1]\n",
    "fdf.columns = [\"MUTUAL_FUND\",\"FUND_MANAGER\",\"MIN_ADD\",\"LOAD_STRUCTURE\"]\n",
    "\n",
    "dsp_pattern = re.compile(\n",
    "    r\"(DSP.+?(?:FUNDS?|ETF|PATH|INDEX|SAVER)\\s*(?:OF FUNDS?|FUNDs?|FUND OF FUNDS?|FOF|.+?PLAN)?)\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "fdf.MUTUAL_FUND = table_parser._clean_series(fdf.MUTUAL_FUND,[\"normalize_alphanumeric\"])\n",
    "fdf.MUTUAL_FUND = fdf.MUTUAL_FUND.apply(lambda x: dsp_pattern.findall(x)[0] if isinstance(x, str) and dsp_pattern.findall(x) else pd.NA)\n",
    "fdf = table_parser._clean_dataframe(fdf,[\"newline_to_space\",\"str_to_pd_NA\"])\n",
    "fdf = fdf.dropna(axis=0, how=\"all\").dropna(axis=1, how=\"all\")\n",
    "fdf =table_parser._clean_dataframe(fdf,['NA_to_str'])\n",
    "\n",
    "data = {}\n",
    "for idx, rows in fdf.iterrows():\n",
    "    values = list(rows)\n",
    "    main_scheme_name = values[0]\n",
    "    if main_scheme_name not in data:\n",
    "        data[main_scheme_name] = {\"fund_manager\":values[1],\"load_structure\":values[3],\"min_add\":values[2]}\n",
    "    else:\n",
    "         data[main_scheme_name].update({\"fund_manager_one\":values[1],\"load_structure_one\":values[3],\"min_add_one\":values[2]})\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 Bottom : DSP US Treasury Fund of Fund DSP Dynamic Asset Allocation Fund DSP Aggressive Hybrid Fund DSP Equity Bond Fund\n",
      "Page 2 Bottom : DSP Banking Financial Services Fund\n",
      "Page 3 Top : DSP Flexi Cap Fund DSP Equity Fund\n",
      "Page 4 Top : DSP Top 100 Equity Fund\n",
      "Page 5 Top : DSP Equity Opportunities Fund\n",
      "Page 6 Top : DSP India T.I.G.E.R. Fund\n",
      "Page 7 Top : DSP Mid Cap Fund\n",
      "Page 8 Top : DSP ELSS Tax Saver Fund DSP Tax Saver Fund\n",
      "Page 9 Top : DSP Healthcare Fund\n",
      "Page 10 Top : DSP Quant Fund\n",
      "Page 11 Top : DSP VALUE FUND\n",
      "Page 12 Top : DSP Small Cap Fund\n",
      "Page 13 Top : DSP Focus Fund\n",
      "Page 14 Top : DSP Multicap Fund\n",
      "Page 15 Top : DSP Business Cycle Fund\n",
      "Page 16 Top : DSP Natural Resources and New Energy Fund\n",
      "Page 17 Top : DSP World Gold Fund of Fund\n",
      "Page 18 Top : DSP World Mining Fund of Fund DSP World Mining Fund\n",
      "Page 19 Top : DSP Global Clean Energy Fund of Fund DSP World Energy Fund\n",
      "Page 20 Top : DSP US Flexible Equity Fund of Fund DSP US Flexible Equity Fund\n",
      "Page 21 Top : DSP Income Plus Arbitrage Fund of Fund DSP Global Allocation Fund of Fund\n",
      "Page 22 Top : DSP Global Innovation Fund of Fund\n",
      "Page 23 Top : DSP Gold ETF\n",
      "Page 24 Top : DSP US Treasury Fund of Fund\n",
      "Page 25 Top : DSP Dynamic Asset Allocation Fund\n",
      "Page 26 Top : DSP Aggressive Hybrid Fund DSP Equity Bond Fund\n",
      "Page 27 Top : DSP Equity Savings Fund\n",
      "Page 28 Top : DSP Nifty 50 Equal Weight Index Fund DSP Equal Nifty 50 Fund\n",
      "Page 29 Top : DSP Nifty 50 Equal Weight ETF\n",
      "Page 30 Top : DSP NIFTY 50 ETF\n",
      "Page 31 Top : DSP NIFTY MIDCAP 150 QUALITY 50 ETF\n",
      "Page 32 Top : DSP Silver ETF\n",
      "Page 32 Bottom : DSP Gold ETF\n",
      "Page 33 Top : DSP Nifty Bank ETF\n",
      "Page 34 Top : DSP Nifty IT ETF\n",
      "Page 35 Top : DSP Nifty PSU Bank ETF\n",
      "Page 36 Top : DSP Nifty Private Bank ETF\n",
      "Page 37 Top : DSP BSE Sensex ETF\n",
      "Page 38 Top : DSP Nifty Healthcare ETF\n",
      "Page 39 Top : DSP NIFTY 1D Rate Liquid ETF DSP Liquid ETF\n",
      "Page 40 Top : DSP BSE Liquid Rate ETF\n",
      "Page 41 Top : DSP Nifty Top 10 Equal Weight ETF\n",
      "Page 42 Top : DSP BSE Sensex Next 30 ETF\n",
      "Page 43 Top : DSP Nifty Top 10 Equal Weight Index Fund\n",
      "Page 45 Top : DSP Nifty Next 50 Index Fund\n",
      "Page 46 Top : DSP Nifty 50 Index Fund\n",
      "Page 47 Top : DSP Nifty Bank Index Fund\n",
      "Page 48 Top : DSP BSE SENSEX Next 30 Index Fund\n",
      "Page 49 Top : DSP Nifty Private Bank Index Fund\n",
      "Page 50 Top : DSP Arbitrage Fund\n",
      "Page 51 Top : DSP Regular Savings Fund\n",
      "Page 52 Top : DSP Liquidity Fund\n",
      "Page 53 Top : DSP Ultra Short Fund\n",
      "Page 54 Top : DSP Floater Fund\n",
      "Page 55 Top : DSP Nifty SDL Plus G-Sec Jun 2028 3070 Index Fund\n",
      "Page 56 Top : DSP CRISIL SDL Plus G-Sec Apr 2033 5050 Index Fund\n",
      "Page 58 Top : DSP Nifty Smallcap250 Quality 50 Index Fund\n",
      "Page 59 Top : DSP Savings Fund\n",
      "Page 60 Top : DSP Gilt Fund DSP Government Securities Fund\n",
      "Page 61 Top : DSP Short Term Fund\n",
      "Page 62 Top : DSP Banking PSU Debt Fund\n",
      "Page 63 Top : DSP Credit Risk Fund\n",
      "Page 64 Top : DSP Strategic Bond Fund\n",
      "Page 65 Top : DSP Bond Fund\n",
      "Page 66 Top : DSP Low Duration Fund\n",
      "Page 67 Top : DSP 10Y G-Sec Fund\n",
      "Page 68 Top : DSP Corporate Bond Fund\n",
      "Page 68 Bottom : DSP Overnight Fund\n",
      "Page 69 Top : DSP Multi Asset Allocation Fund\n",
      "Page 70 Top : DSP Banking Financial Services Fund\n",
      "Page 75 Bottom : DSP Bond Fund DSP Bond Fund\n",
      "Page 76 Bottom : DSP Equity Savings Fund DSP Equity Savings Fund\n",
      "Page 79 Bottom : DSP Silver ETF\n",
      "Page 80 Bottom : DSP NIFTY IT ETF DSP NIFTY IT ETF\n",
      "Page 85 Bottom : DSP Focus Fund\n",
      "Page 87 Bottom : DSP Short Term Fund\n",
      "Page 88 Bottom : DSP Strategic Bond Fund\n",
      "Page 89 Bottom : DSP Gilt Fund DSP Government Securities Fund DSP Gilt Fund\n",
      "Page 90 Bottom : DSP Nifty 50 Index Fund\n",
      "Page 91 Bottom : DSP Quant Fund DSP Quant Fund\n",
      "Page 98 Bottom : DSP Value Fund DSP Multi Asset Allocation Fund DSP Banking Financial Services Fund\n",
      "Page 99 Bottom : DSP Strategic Bond Fund DSP Credit Risk Fund\n",
      "Page 100 Bottom : DSP 10Y G-Sec Fund\n",
      "Page 109 Bottom : DSP Bond Fund\n",
      "Page 110 Bottom : DSP Savings Fund\n",
      "Page 123 Bottom : DSP ELSS Tax Saver Fund DSP Tax Saver Fund\n",
      "Page 126 Bottom : DSP Banking PSU Debt Fund\n",
      "Page 127 Bottom : DSP Nifty 50 ETF\n",
      "Page 128 Bottom : DSP Nifty PSU Bank ETF\n",
      "Page 129 Bottom : DSP BSE Liquid Rate ETF\n",
      "Page 132 Top : DSP Savings Fund\n",
      "Page 132 Bottom : DSP Banking PSU Debt Fund\n",
      "Page 133 Top : DSP Floater Fund\n",
      "Page 133 Bottom : DSP Bond Fund\n",
      "Page 134 Top : DSP Gilt Fund DSP Government Securities Fund\n",
      "Page 134 Bottom : DSP NIFTY 1D Rate Liquid ETF\n",
      "Page 135 Top : DSP Crisil SDL Plus G-Sec Apr 2033 5050 Index Fund\n",
      "Page 139 Bottom : DSP Aggressive Hybrid Fund DSP Equity Bond Fund DSP ELSS Tax Saver Fund DSP Tax Saver Fund DSP Tax Saver Fund DSP ELSS Tax Saver Fund DSP Nifty 50 Equal Weight Index Fund DSP Nifty Next 50 Index Fund DSP Nifty 50 Index Fund DSP Nifty SDL Plus G-Sec Sep 2027 5050 Index Fund DSP CRISIL SDL Plus G-Sec Apr 2033 5050 Index Fund\n"
     ]
    }
   ],
   "source": [
    "def get_proper_fund_names(path: str):\n",
    "    pattern = r\"((?:DSP|Bharat).*?(?:Fund\\s*(?:of Fund)?|FUND|ETF|FTF|FOF))\"\n",
    "    title = {}\n",
    "\n",
    "    with fitz.open(path) as doc:\n",
    "        for pgn, page in enumerate(doc):\n",
    "            height = page.rect.height\n",
    "\n",
    "            # Top half\n",
    "            clip_top = (0, 0, 500, 40)\n",
    "            text_top = \" \".join(page.get_text(\"text\", clip=clip_top).split())\n",
    "            text_top = re.sub(r\"[^A-Za-z0-9\\s\\-\\(\\).,]+|\\u2028\", \"\", text_top)\n",
    "            top_matches = re.findall(pattern, text_top, re.DOTALL)\n",
    "            top_match = \" \".join(top_matches).strip()\n",
    "            norm_top = \" \".join(top_match.split())\n",
    "\n",
    "            # Bottom half\n",
    "            clip_bottom = (0, height / 2, 500, height / 2 + 40)\n",
    "            text_bottom = \" \".join(page.get_text(\"text\", clip=clip_bottom).split())\n",
    "            text_bottom = re.sub(r\"[^A-Za-z0-9\\s\\-\\(\\).,]+|\\u2028\", \"\", text_bottom)\n",
    "            bottom_matches = re.findall(pattern, text_bottom, re.DOTALL)\n",
    "            bottom_match = \" \".join(bottom_matches).strip()\n",
    "            norm_bottom = \" \".join(bottom_match.split())\n",
    "\n",
    "            # Print and collect top\n",
    "            if norm_top:\n",
    "                print(f\"Page {pgn} Top : {norm_top}\")\n",
    "                title[f\"{pgn}_0\"] = norm_top\n",
    "\n",
    "            # Print and collect bottom if different\n",
    "            if norm_bottom and norm_bottom != norm_top:\n",
    "                print(f\"Page {pgn} Bottom : {norm_bottom}\")\n",
    "                title[f\"{pgn}_1\"] = norm_bottom\n",
    "                title[str(pgn)] = [norm_top, norm_bottom]\n",
    "\n",
    "    return title\n",
    "title = get_proper_fund_names(sample_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1_1': 'DSP US Treasury Fund of Fund DSP Dynamic Asset Allocation Fund DSP Aggressive Hybrid Fund DSP Equity Bond Fund',\n",
       " '1': ['',\n",
       "  'DSP US Treasury Fund of Fund DSP Dynamic Asset Allocation Fund DSP Aggressive Hybrid Fund DSP Equity Bond Fund'],\n",
       " '2_1': 'DSP Banking Financial Services Fund',\n",
       " '2': ['', 'DSP Banking Financial Services Fund'],\n",
       " '3_0': 'DSP Flexi Cap Fund DSP Equity Fund',\n",
       " '4_0': 'DSP Top 100 Equity Fund',\n",
       " '5_0': 'DSP Equity Opportunities Fund',\n",
       " '6_0': 'DSP India T.I.G.E.R. Fund',\n",
       " '7_0': 'DSP Mid Cap Fund',\n",
       " '8_0': 'DSP ELSS Tax Saver Fund DSP Tax Saver Fund',\n",
       " '9_0': 'DSP Healthcare Fund',\n",
       " '10_0': 'DSP Quant Fund',\n",
       " '11_0': 'DSP VALUE FUND',\n",
       " '12_0': 'DSP Small Cap Fund',\n",
       " '13_0': 'DSP Focus Fund',\n",
       " '14_0': 'DSP Multicap Fund',\n",
       " '15_0': 'DSP Business Cycle Fund',\n",
       " '16_0': 'DSP Natural Resources and New Energy Fund',\n",
       " '17_0': 'DSP World Gold Fund of Fund',\n",
       " '18_0': 'DSP World Mining Fund of Fund DSP World Mining Fund',\n",
       " '19_0': 'DSP Global Clean Energy Fund of Fund DSP World Energy Fund',\n",
       " '20_0': 'DSP US Flexible Equity Fund of Fund DSP US Flexible Equity Fund',\n",
       " '21_0': 'DSP Income Plus Arbitrage Fund of Fund DSP Global Allocation Fund of Fund',\n",
       " '22_0': 'DSP Global Innovation Fund of Fund',\n",
       " '23_0': 'DSP Gold ETF',\n",
       " '24_0': 'DSP US Treasury Fund of Fund',\n",
       " '25_0': 'DSP Dynamic Asset Allocation Fund',\n",
       " '26_0': 'DSP Aggressive Hybrid Fund DSP Equity Bond Fund',\n",
       " '27_0': 'DSP Equity Savings Fund',\n",
       " '28_0': 'DSP Nifty 50 Equal Weight Index Fund DSP Equal Nifty 50 Fund',\n",
       " '29_0': 'DSP Nifty 50 Equal Weight ETF',\n",
       " '30_0': 'DSP NIFTY 50 ETF',\n",
       " '31_0': 'DSP NIFTY MIDCAP 150 QUALITY 50 ETF',\n",
       " '32_0': 'DSP Silver ETF',\n",
       " '32_1': 'DSP Gold ETF',\n",
       " '32': ['DSP Silver ETF', 'DSP Gold ETF'],\n",
       " '33_0': 'DSP Nifty Bank ETF',\n",
       " '34_0': 'DSP Nifty IT ETF',\n",
       " '35_0': 'DSP Nifty PSU Bank ETF',\n",
       " '36_0': 'DSP Nifty Private Bank ETF',\n",
       " '37_0': 'DSP BSE Sensex ETF',\n",
       " '38_0': 'DSP Nifty Healthcare ETF',\n",
       " '39_0': 'DSP NIFTY 1D Rate Liquid ETF DSP Liquid ETF',\n",
       " '40_0': 'DSP BSE Liquid Rate ETF',\n",
       " '41_0': 'DSP Nifty Top 10 Equal Weight ETF',\n",
       " '42_0': 'DSP BSE Sensex Next 30 ETF',\n",
       " '43_0': 'DSP Nifty Top 10 Equal Weight Index Fund',\n",
       " '45_0': 'DSP Nifty Next 50 Index Fund',\n",
       " '46_0': 'DSP Nifty 50 Index Fund',\n",
       " '47_0': 'DSP Nifty Bank Index Fund',\n",
       " '48_0': 'DSP BSE SENSEX Next 30 Index Fund',\n",
       " '49_0': 'DSP Nifty Private Bank Index Fund',\n",
       " '50_0': 'DSP Arbitrage Fund',\n",
       " '51_0': 'DSP Regular Savings Fund',\n",
       " '52_0': 'DSP Liquidity Fund',\n",
       " '53_0': 'DSP Ultra Short Fund',\n",
       " '54_0': 'DSP Floater Fund',\n",
       " '55_0': 'DSP Nifty SDL Plus G-Sec Jun 2028 3070 Index Fund',\n",
       " '56_0': 'DSP CRISIL SDL Plus G-Sec Apr 2033 5050 Index Fund',\n",
       " '58_0': 'DSP Nifty Smallcap250 Quality 50 Index Fund',\n",
       " '59_0': 'DSP Savings Fund',\n",
       " '60_0': 'DSP Gilt Fund DSP Government Securities Fund',\n",
       " '61_0': 'DSP Short Term Fund',\n",
       " '62_0': 'DSP Banking PSU Debt Fund',\n",
       " '63_0': 'DSP Credit Risk Fund',\n",
       " '64_0': 'DSP Strategic Bond Fund',\n",
       " '65_0': 'DSP Bond Fund',\n",
       " '66_0': 'DSP Low Duration Fund',\n",
       " '67_0': 'DSP 10Y G-Sec Fund',\n",
       " '68_0': 'DSP Corporate Bond Fund',\n",
       " '68_1': 'DSP Overnight Fund',\n",
       " '68': ['DSP Corporate Bond Fund', 'DSP Overnight Fund'],\n",
       " '69_0': 'DSP Multi Asset Allocation Fund',\n",
       " '70_0': 'DSP Banking Financial Services Fund',\n",
       " '75_1': 'DSP Bond Fund DSP Bond Fund',\n",
       " '75': ['', 'DSP Bond Fund DSP Bond Fund'],\n",
       " '76_1': 'DSP Equity Savings Fund DSP Equity Savings Fund',\n",
       " '76': ['', 'DSP Equity Savings Fund DSP Equity Savings Fund'],\n",
       " '79_1': 'DSP Silver ETF',\n",
       " '79': ['', 'DSP Silver ETF'],\n",
       " '80_1': 'DSP NIFTY IT ETF DSP NIFTY IT ETF',\n",
       " '80': ['', 'DSP NIFTY IT ETF DSP NIFTY IT ETF'],\n",
       " '85_1': 'DSP Focus Fund',\n",
       " '85': ['', 'DSP Focus Fund'],\n",
       " '87_1': 'DSP Short Term Fund',\n",
       " '87': ['', 'DSP Short Term Fund'],\n",
       " '88_1': 'DSP Strategic Bond Fund',\n",
       " '88': ['', 'DSP Strategic Bond Fund'],\n",
       " '89_1': 'DSP Gilt Fund DSP Government Securities Fund DSP Gilt Fund',\n",
       " '89': ['', 'DSP Gilt Fund DSP Government Securities Fund DSP Gilt Fund'],\n",
       " '90_1': 'DSP Nifty 50 Index Fund',\n",
       " '90': ['', 'DSP Nifty 50 Index Fund'],\n",
       " '91_1': 'DSP Quant Fund DSP Quant Fund',\n",
       " '91': ['', 'DSP Quant Fund DSP Quant Fund'],\n",
       " '98_1': 'DSP Value Fund DSP Multi Asset Allocation Fund DSP Banking Financial Services Fund',\n",
       " '98': ['',\n",
       "  'DSP Value Fund DSP Multi Asset Allocation Fund DSP Banking Financial Services Fund'],\n",
       " '99_1': 'DSP Strategic Bond Fund DSP Credit Risk Fund',\n",
       " '99': ['', 'DSP Strategic Bond Fund DSP Credit Risk Fund'],\n",
       " '100_1': 'DSP 10Y G-Sec Fund',\n",
       " '100': ['', 'DSP 10Y G-Sec Fund'],\n",
       " '109_1': 'DSP Bond Fund',\n",
       " '109': ['', 'DSP Bond Fund'],\n",
       " '110_1': 'DSP Savings Fund',\n",
       " '110': ['', 'DSP Savings Fund'],\n",
       " '123_1': 'DSP ELSS Tax Saver Fund DSP Tax Saver Fund',\n",
       " '123': ['', 'DSP ELSS Tax Saver Fund DSP Tax Saver Fund'],\n",
       " '126_1': 'DSP Banking PSU Debt Fund',\n",
       " '126': ['', 'DSP Banking PSU Debt Fund'],\n",
       " '127_1': 'DSP Nifty 50 ETF',\n",
       " '127': ['', 'DSP Nifty 50 ETF'],\n",
       " '128_1': 'DSP Nifty PSU Bank ETF',\n",
       " '128': ['', 'DSP Nifty PSU Bank ETF'],\n",
       " '129_1': 'DSP BSE Liquid Rate ETF',\n",
       " '129': ['', 'DSP BSE Liquid Rate ETF'],\n",
       " '132_0': 'DSP Savings Fund',\n",
       " '132_1': 'DSP Banking PSU Debt Fund',\n",
       " '132': ['DSP Savings Fund', 'DSP Banking PSU Debt Fund'],\n",
       " '133_0': 'DSP Floater Fund',\n",
       " '133_1': 'DSP Bond Fund',\n",
       " '133': ['DSP Floater Fund', 'DSP Bond Fund'],\n",
       " '134_0': 'DSP Gilt Fund DSP Government Securities Fund',\n",
       " '134_1': 'DSP NIFTY 1D Rate Liquid ETF',\n",
       " '134': ['DSP Gilt Fund DSP Government Securities Fund',\n",
       "  'DSP NIFTY 1D Rate Liquid ETF'],\n",
       " '135_0': 'DSP Crisil SDL Plus G-Sec Apr 2033 5050 Index Fund',\n",
       " '139_1': 'DSP Aggressive Hybrid Fund DSP Equity Bond Fund DSP ELSS Tax Saver Fund DSP Tax Saver Fund DSP Tax Saver Fund DSP ELSS Tax Saver Fund DSP Nifty 50 Equal Weight Index Fund DSP Nifty Next 50 Index Fund DSP Nifty 50 Index Fund DSP Nifty SDL Plus G-Sec Sep 2027 5050 Index Fund DSP CRISIL SDL Plus G-Sec Apr 2033 5050 Index Fund',\n",
       " '139': ['',\n",
       "  'DSP Aggressive Hybrid Fund DSP Equity Bond Fund DSP ELSS Tax Saver Fund DSP Tax Saver Fund DSP Tax Saver Fund DSP ELSS Tax Saver Fund DSP Nifty 50 Equal Weight Index Fund DSP Nifty Next 50 Index Fund DSP Nifty 50 Index Fund DSP Nifty SDL Plus G-Sec Sep 2027 5050 Index Fund DSP CRISIL SDL Plus G-Sec Apr 2033 5050 Index Fund']}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proper_fund_names(path: str):\n",
    "    pattern = \"(HDFC.*?(?:FUND|Fund|ETF|FO?o?F)\\\\s*(?:of Funds?|.+?Plan|Fund of Funds?|Fund)?)\"\n",
    "    title = {}   \n",
    "    with fitz.open(path) as doc:\n",
    "        for pgn, page in enumerate(doc):\n",
    "            text = \" \".join(page.get_text(\"text\", clip=(0, 0, 400, 60)).split(\"\\n\"))\n",
    "            text = re.sub(\"[^A-Za-z0-9\\\\s\\\\-\\\\(\\\\).,]+|\\u2028\", \"\", text).strip()\n",
    "            print(text)\n",
    "            if matches := re.findall(pattern, text, re.DOTALL):\n",
    "                title[pgn] = \" \".join([_ for _ in matches[0].strip().split(\" \") if _])\n",
    "                print(pgn,matches[0])\n",
    "    return title\n",
    "title = get_proper_fund_names(sample_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os ,json \n",
    "import pandas as pd\n",
    "\n",
    "full_name = set()\n",
    "split_name = set()\n",
    "MANAGER_REGEX = FundRegex().MANAGER_STOP_WORDS\n",
    "for a,b,files in os.walk(r\"C:\\Users\\Kaustubh.keny\\Projects\\office-work\\mywork-repo\\sql_learn\\json\\MAR25DATA\"):\n",
    "    for paths in files:\n",
    "        sample_path = os.path.join(os.getcwd(),\"..\",\"sql_learn\",\"json\",\"MAR25DATA\",paths)\n",
    "        # print(sample_path)\n",
    "        try:\n",
    "            with open(sample_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                data = json.load(file)\n",
    "                for k,scheme in data.items():\n",
    "                    # print(k)\n",
    "                    if \"fund_manager\" in scheme:\n",
    "                        for entry in scheme['fund_manager']:\n",
    "                            # print(entry['name'])\n",
    "                            name = entry['name']\n",
    "                            for regex_ in MANAGER_REGEX:\n",
    "                                name = re.sub(f\"\\\\b{regex_}\\\\b|[^A-Za-z\\\\s0-9]+\",\"\",name, re.IGNORECASE)\n",
    "                                name = re.sub(r\"\\s+\",\" \",name)\n",
    "                            full_name.add(name)\n",
    "                            printthis = name if name.strip() else \"EMPTY\"\n",
    "                            # print(f\"<<{printthis}>>\")\n",
    "                            if printthis == \"EMPTY\":\n",
    "                                print(k,printthis,entry['name'])\n",
    "                            # split_name.add(name.split(' '))\n",
    "                    # print(scheme.keys())\n",
    "        except Exception as e:\n",
    "            print(f\"NEVER MIND {e}\")\n",
    "    # print(files)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalize_alphanumeric(text: str) -> str:\n",
    "    if not isinstance(text,str):\n",
    "        return text\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]+\", \" \", str(text))\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip().lower()\n",
    "\n",
    "def extract_clipped_data(input:str, pages:list, bboxes:list):\n",
    "        \n",
    "        document = fitz.open(input)\n",
    "        final_list = []\n",
    "    \n",
    "        for pgn in pages:\n",
    "            page = document[pgn]\n",
    "            \n",
    "            all_blocks = [] #store every data from bboxes\n",
    "            \n",
    "            for bbox in bboxes:\n",
    "                blocks, seen_blocks = [], set()  #store unique blocks based on content and bbox\n",
    "                \n",
    "                page_blocks = page.get_text('dict', clip=bbox)['blocks']\n",
    "                for block in page_blocks:\n",
    "                    if block['type'] == 0 and 'lines' in block: #type 0 means text block\n",
    "                        #hash_key\n",
    "                        block_key = (tuple(block['bbox']), tuple(tuple(line['spans'][0]['text'] for line in block['lines'])))\n",
    "                        if block_key not in seen_blocks:\n",
    "                            seen_blocks.add(block_key)\n",
    "                            blocks.append(block)\n",
    "\n",
    "                sorted_blocks = sorted(blocks, key=lambda x: (x['bbox'][1], x['bbox'][0]))\n",
    "                all_blocks.append(sorted_blocks)\n",
    "\n",
    "            final_list.append({\n",
    "                \"pgn\": pgn,\n",
    "                \"block\": all_blocks #will be list[list,list,..]\n",
    "            })\n",
    "\n",
    "        document.close()\n",
    "        return final_list\n",
    "    \n",
    "def extract_data_relative_line(path: str, line_x: float, side: str):\n",
    "    doc = fitz.open(path)\n",
    "    pages = doc.page_count\n",
    "\n",
    "    final_list = []\n",
    "\n",
    "    for pgn in range(pages):\n",
    "        page = doc[pgn]\n",
    "\n",
    "        blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "        sorted_blocks = sorted(blocks, key=lambda x: (x[\"bbox\"][1], x[\"bbox\"][0]))\n",
    "        extracted_blocks = []\n",
    "\n",
    "        # Keep track of blocks to avoid duplicates\n",
    "        added_blocks = set()\n",
    "\n",
    "        for block in sorted_blocks:\n",
    "            block_id = id(block)  # Unique identifier for the block\n",
    "\n",
    "            for line in block.get(\"lines\", []):\n",
    "                for span in line.get(\"spans\", []):\n",
    "                    origin = span[\"origin\"]\n",
    "                    x0, _ = origin\n",
    "\n",
    "                    # Check the side condition\n",
    "                    if side == \"left\" and x0 < line_x and block_id not in added_blocks:\n",
    "                        extracted_blocks.append(block)\n",
    "                        added_blocks.add(block_id)  # Mark block as added\n",
    "                    elif side == \"right\" and x0 > line_x and block_id not in added_blocks:\n",
    "                        extracted_blocks.append(block)\n",
    "                        added_blocks.add(block_id)  # Mark block as added\n",
    "\n",
    "      \n",
    "        final_list.append({\n",
    "            \"pgn\": pgn,\n",
    "            \"blocks\": extracted_blocks\n",
    "        })\n",
    "\n",
    "    doc.close()\n",
    "\n",
    "    return final_list\n",
    "  \n",
    "def get_clipped_data(input:str, bboxes:list[set], *args):\n",
    "    \n",
    "        document = fitz.open(input)\n",
    "        final_list = []\n",
    "        if args:\n",
    "            pages = list(args)\n",
    "        else:\n",
    "            pages = [i for i in document.page_count]\n",
    "        \n",
    "        for pgn in pages:\n",
    "            page = document[pgn]\n",
    "\n",
    "            blocks = []\n",
    "            for bbox in bboxes:\n",
    "                blocks.extend(page.get_text('dict', clip = bbox)['blocks']) #get all blocks\n",
    "            \n",
    "            filtered_blocks = [block for block in blocks if block['type']== 0 and 'lines' in block]\n",
    "            # sorted_blocks = sorted(filtered_blocks, key= lambda x: (x['bbox'][1], x['bbox'][0]))\n",
    "            \n",
    "             # Extract text from sorted blocks\n",
    "            extracted_text = []\n",
    "            for block in filtered_blocks:\n",
    "                block_text = []\n",
    "                for line in block['lines']:\n",
    "                    line_text = \" \".join(span['text'] for span in line['spans'])\n",
    "                    block_text.append(line_text)\n",
    "                extracted_text.append(\"\\n\".join(block_text))\n",
    "            \n",
    "            final_list.append({\n",
    "            \"pgn\": pgn,\n",
    "            \"block\": filtered_blocks,\n",
    "            \"text\": extracted_text\n",
    "            })\n",
    "            \n",
    "            \n",
    "        document.close()\n",
    "        return final_list\n",
    "    \n",
    "def extract_clipped_text_all_pages(pdf_path, clip_coords):\n",
    "    results = {}\n",
    "    doc = fitz.open(pdf_path)\n",
    "    clip_rect = fitz.Rect(*clip_coords)\n",
    "    try:\n",
    "        for page_number, page in enumerate(doc):\n",
    "            text = page.get_text(\"text\", clip=clip_rect).strip()\n",
    "            results[page_number] = text\n",
    "    finally:\n",
    "        doc.close()\n",
    "    return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdf-extractor-env",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
