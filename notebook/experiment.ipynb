{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded config with output_folder = None\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pprint, json, math, os, sys\n",
    "dir_path = \"C:\\\\Users\\\\rando\\\\OneDrive\\\\Documents\\\\mywork-repo\"\n",
    "fund_path = r\"C:\\Users\\rando\\OneDrive\\Documents\\PDFDrive\\Mar25\"\n",
    "dry_path = r\"C:\\Users\\rando\\OneDrive\\Documents\\mywork-repo\\notebook\\DryRun.pdf\"\n",
    "\n",
    "# dir_path = \"C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\office-work\\\\mywork-repo\"\n",
    "# fund_path = \"C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\Mar 25\"\n",
    "# dry_path = r\"C:\\Users\\Kaustubh.keny\\Projects\\office-work\\mywork-repo\\notebook\\DryRun.pdf\"\n",
    "sys.path.append(os.path.abspath(dir_path))\n",
    "from app.config_loader import load_config_once\n",
    "conf = load_config_once()\n",
    "\n",
    "\n",
    "import fitz, pdfplumber, ocrmypdf,camelot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "from app.utils import Helper\n",
    "from app.parse_regex import *\n",
    "from app.parse_table import *\n",
    "\n",
    "dry_path = r'DryRun.pdf'\n",
    "fin_path = r'\\data\\input\\financial_indices.xlsx'\n",
    "mutual_fund = Helper.get_fund_paths(fund_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalize_alphanumeric(text: str) -> str:\n",
    "    if not isinstance(text,str):\n",
    "        return text\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]+\", \" \", str(text))\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip().lower()\n",
    "\n",
    "def extract_clipped_data(input:str, pages:list, bboxes:list):\n",
    "        \n",
    "        document = fitz.open(input)\n",
    "        final_list = []\n",
    "    \n",
    "        for pgn in pages:\n",
    "            page = document[pgn]\n",
    "            \n",
    "            all_blocks = [] #store every data from bboxes\n",
    "            \n",
    "            for bbox in bboxes:\n",
    "                blocks, seen_blocks = [], set()  #store unique blocks based on content and bbox\n",
    "                \n",
    "                page_blocks = page.get_text('dict', clip=bbox)['blocks']\n",
    "                for block in page_blocks:\n",
    "                    if block['type'] == 0 and 'lines' in block: #type 0 means text block\n",
    "                        #hash_key\n",
    "                        block_key = (tuple(block['bbox']), tuple(tuple(line['spans'][0]['text'] for line in block['lines'])))\n",
    "                        if block_key not in seen_blocks:\n",
    "                            seen_blocks.add(block_key)\n",
    "                            blocks.append(block)\n",
    "\n",
    "                sorted_blocks = sorted(blocks, key=lambda x: (x['bbox'][1], x['bbox'][0]))\n",
    "                all_blocks.append(sorted_blocks)\n",
    "\n",
    "            final_list.append({\n",
    "                \"pgn\": pgn,\n",
    "                \"block\": all_blocks #will be list[list,list,..]\n",
    "            })\n",
    "\n",
    "        document.close()\n",
    "        return final_list\n",
    "    \n",
    "def extract_data_relative_line(path: str, line_x: float, side: str):\n",
    "    doc = fitz.open(path)\n",
    "    pages = doc.page_count\n",
    "\n",
    "    final_list = []\n",
    "\n",
    "    for pgn in range(pages):\n",
    "        page = doc[pgn]\n",
    "\n",
    "        blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "        sorted_blocks = sorted(blocks, key=lambda x: (x[\"bbox\"][1], x[\"bbox\"][0]))\n",
    "        extracted_blocks = []\n",
    "\n",
    "        # Keep track of blocks to avoid duplicates\n",
    "        added_blocks = set()\n",
    "\n",
    "        for block in sorted_blocks:\n",
    "            block_id = id(block)  # Unique identifier for the block\n",
    "\n",
    "            for line in block.get(\"lines\", []):\n",
    "                for span in line.get(\"spans\", []):\n",
    "                    origin = span[\"origin\"]\n",
    "                    x0, _ = origin\n",
    "\n",
    "                    # Check the side condition\n",
    "                    if side == \"left\" and x0 < line_x and block_id not in added_blocks:\n",
    "                        extracted_blocks.append(block)\n",
    "                        added_blocks.add(block_id)  # Mark block as added\n",
    "                    elif side == \"right\" and x0 > line_x and block_id not in added_blocks:\n",
    "                        extracted_blocks.append(block)\n",
    "                        added_blocks.add(block_id)  # Mark block as added\n",
    "\n",
    "      \n",
    "        final_list.append({\n",
    "            \"pgn\": pgn,\n",
    "            \"blocks\": extracted_blocks\n",
    "        })\n",
    "\n",
    "    doc.close()\n",
    "\n",
    "    return final_list\n",
    "  \n",
    "def get_clipped_data(input:str, bboxes:list[set], *args):\n",
    "    \n",
    "        document = fitz.open(input)\n",
    "        final_list = []\n",
    "        if args:\n",
    "            pages = list(args)\n",
    "        else:\n",
    "            pages = [i for i in document.page_count]\n",
    "        \n",
    "        for pgn in pages:\n",
    "            page = document[pgn]\n",
    "\n",
    "            blocks = []\n",
    "            for bbox in bboxes:\n",
    "                blocks.extend(page.get_text('dict', clip = bbox)['blocks']) #get all blocks\n",
    "            \n",
    "            filtered_blocks = [block for block in blocks if block['type']== 0 and 'lines' in block]\n",
    "            # sorted_blocks = sorted(filtered_blocks, key= lambda x: (x['bbox'][1], x['bbox'][0]))\n",
    "            \n",
    "             # Extract text from sorted blocks\n",
    "            extracted_text = []\n",
    "            for block in filtered_blocks:\n",
    "                block_text = []\n",
    "                for line in block['lines']:\n",
    "                    line_text = \" \".join(span['text'] for span in line['spans'])\n",
    "                    block_text.append(line_text)\n",
    "                extracted_text.append(\"\\n\".join(block_text))\n",
    "            \n",
    "            final_list.append({\n",
    "            \"pgn\": pgn,\n",
    "            \"block\": filtered_blocks,\n",
    "            \"text\": extracted_text\n",
    "            })\n",
    "            \n",
    "            \n",
    "        document.close()\n",
    "        return final_list\n",
    "    \n",
    "def extract_clipped_text_all_pages(pdf_path, clip_coords):\n",
    "    results = {}\n",
    "    doc = fitz.open(pdf_path)\n",
    "    clip_rect = fitz.Rect(*clip_coords)\n",
    "    try:\n",
    "        for page_number, page in enumerate(doc):\n",
    "            text = page.get_text(\"text\", clip=clip_rect).strip()\n",
    "            results[page_number] = text\n",
    "    finally:\n",
    "        doc.close()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sample_path = mutual_fund[\"Edelweiss Mutual Fund\"]\n",
    "#sid\n",
    "rp = r\"\\pdfs\\unifi1747199772321.pdf\"\n",
    "rp = r\"\\pdfs\\unifi1747373013796.pdf\"\n",
    "rp = r\"\\pdfs\\unifi1747199156275.pdf\"\n",
    "dsp1 = \"\\\\pdfs\\\\May SID\\\\DSP Mutual Fund\\\\8_17160_May-2025_1748246407_SID.pdf\"\n",
    "tata1 = r\"\\pdfs\\May SID\\Tata Mutual Fund\\38_17082_May-2025_1746423571_SID.pdf\"\n",
    "nip1 = \"\\\\pdfs\\\\May SID\\\\Nippon India Mutual Fund\\\\33_17142_May-2025_1747631690_SID.pdf\"\n",
    "sbi1 = \"\\\\pdfs\\\\May SID\\\\SBI Mutual Fund\\\\35_17126_May-2025_1747282059_SID.pdf\",\n",
    "samc1 = \"\\\\pdfs\\\\May SID\\\\Samco Mutual Fund\\\\58_17166_May-2025_1748246684_SID.pdf\"\n",
    "bar1 = \"\\\\pdfs\\\\May SID\\\\Baroda BNP Paribas Mutual Fund\\\\2_17136_May-2025_1747367452_SID.pdf\"\n",
    "unifi1 = \"\\\\pdfs\\\\May SID\\\\Unifi Mutual Fund\\\\1747199772321.pdf\"\n",
    "moti1 = \"\\\\pdfs\\\\May SID\\\\Motilal Oswal Mutual Fund\\\\28_17086_May-2025_1746677550_SID.pdf\"\n",
    "\n",
    "bajaj1 = \"\\\\pdfs\\\\Apr SID\\\\Bajaj Finserv Mutual Fund\\\\59_17022_Apr-2025_1744773931_SID.pdf\"\n",
    "\n",
    "#kim\n",
    "kimsbi1 = \"\\\\pdfs\\\\May KIM\\\\SBI Mutual Fund\\\\35_17127_May-2025_1747282099_KIM.pdf\"\n",
    "kimnip1 = \"\\\\pdfs\\\\May KIM\\\\Nippon India Mutual Fund\\\\33_17143_May-2025_1747631729_KIM.pdf\"\n",
    "\n",
    "\n",
    "#other\n",
    "sample_path = dir_path + bajaj1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified PDF saved to: DryRun.pdf\n"
     ]
    }
   ],
   "source": [
    "lines = [\n",
    "    ((110, 0), (110, 812)),# Vertical line\n",
    "    ((0, 350), (812, 350)),\n",
    "    ((570, 0), (570, 812))\n",
    "]\n",
    "pages = [12, 14,16]\n",
    "bboxes = [[0, 120, 180, 812],[180, 85, 360, 812]] #[(0, 85, 180, 812),(180, 85, 360, 812),(0,100,270,812),(0,100,350,812)]\n",
    "pages = [i for i in range(1,110)]\n",
    "Helper.draw_lines_on_pdf(sample_path, lines, bboxes, pages, dry_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "bajajf1 = r\"C:\\Users\\Kaustubh.keny\\Projects\\Apr 25\\Bajaj finserv Mutual Fund\\59_30-Apr-25_FS.pdf\"\n",
    "bajajf2 = r\"C:\\Users\\Kaustubh.keny\\Projects\\Jan 25\\Bajaj finserv Mutual Fund\\59_31-Jan-25_FS.pdf\"\n",
    "hdfc1 = r\"C:\\Users\\Kaustubh.keny\\Projects\\Jan 25\\HDFC Mutual Fund\\12_31-Jan-2025_FS.pdf\"\n",
    "nipp1 = r\"C:\\Users\\Kaustubh.keny\\Projects\\Jan 25\\Nippon India Mutual Fund\\33_31-Jan-25_FS.pdf\"\n",
    "dsp1 = r\"C:\\Users\\Kaustubh.keny\\Projects\\PDF\\Apr 25\\DSP Mutual Fund\\8_30-Apr-25_FS.pdf\"\n",
    "\n",
    "hdfc1 = r\"c:\\Users\\rando\\OneDrive\\Documents\\PDFDrive\\Feb25\\HDFC Mutual Fund\\12_28-Feb-2025_1_FS.pdf\"\n",
    "bajaj1 = r\"c:\\Users\\rando\\OneDrive\\Documents\\PDFDrive\\Feb25\\Bajaj finserv Mutual Fund\\59_28-Feb-25_FS.pdf\"\n",
    "nippon1 = r\"c:\\Users\\rando\\OneDrive\\Documents\\PDFDrive\\Feb25\\Nippon India Mutual Fund\\33_28-Feb-25_FS.pdf\"\n",
    "\n",
    "# tables = camelot.read_pdf(hdfc1,flavor=\"lattice\",pages=\"91-93\") #,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,\n",
    "# dfs = pd.concat([table.df for table in tables], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NIPPON\n",
    "table_parser = TableParser()\n",
    "tables = camelot.read_pdf(nippon1,flavor=\"lattice\",pages=\"129-140\")\n",
    "dfs = pd.concat([table.df for table in tables], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dfs.to_excel(\"this.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BAJAJAJ\n",
    "table_parser = TableParser()\n",
    "tables = camelot.read_pdf(bajaj1,flavor=\"lattice\",pages=\"14-17\")\n",
    "dfs = pd.concat([table.df for table in tables], ignore_index=True)\n",
    "sc1 = table_parser._get_matching_col_indices(dfs,[\"Bajaj.+?Fund\",\"SCHEME\\\\s*NAME\"],thresh=20)\n",
    "sc2 = table_parser._get_matching_col_indices(dfs,[\"Information\\\\s*ratio\",\"Portfolio\\\\s*Quants\",\"Tracking Error\",\"YTM\",\"Average\\\\s*Maturity\",\"Sharpe\"],thresh=20)\n",
    "# sc2 = table_parser._get_matching_col_indices(dfs,[\"Debt\\\\s*Quant\",\"Modified\\\\s*Duration\",\"Macaulay\",\"YTM\",\"Average\\\\s*Maturity\",\"Sharpe\"],thresh=20)\n",
    "all_cols = list(set(sc1 + sc2+ [sc2[0]+1,sc2[0]+2,sc2[0]+3]))\n",
    "fdf = dfs.iloc[:, all_cols]\n",
    "fdf.columns = [\"MUTUAL_FUND\",\"METRICS\",\"METRIC_VALS\",\"DEBT\",\"DEBT_VALS\"]\n",
    "hdfc_pattern = re.compile(\n",
    "    r\"(Baj.+?(?:FUNDS?|ETF|PATH|INDEX|SAVER)\\s*(?:OF FUNDS?|FUND|FUND OF FUNDS|FOF|.+?PLAN|.+?GROWTH)?)\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "fdf.MUTUAL_FUND = table_parser._clean_series(fdf.MUTUAL_FUND,[\"normalize_alphanumeric\"])\n",
    "fdf.MUTUAL_FUND = fdf.MUTUAL_FUND.apply(lambda x: hdfc_pattern.findall(x)[0] if isinstance(x, str) and hdfc_pattern.findall(x) else \"\")\n",
    "fdf = table_parser._clean_dataframe(fdf,[\"newline_to_space\",\"str_to_pd_NA\"])\n",
    "fdf = fdf.dropna(axis=0, how=\"all\").dropna(axis=1, how=\"all\")\n",
    "fdf =table_parser._clean_dataframe(fdf,['NA_to_str'])\n",
    "\n",
    "data = {}\n",
    "for idx, rows in fdf.iterrows():\n",
    "    values = list(rows)\n",
    "    main_scheme_name = str(values[0]).strip() if not pd.isna(values[0]) else \"\"\n",
    "    if main_scheme_name:\n",
    "        temp = main_scheme_name\n",
    "        if temp not in data:\n",
    "            data[temp] = {\"metrics\": []}\n",
    "        data[temp][\"metrics\"].append(\" \".join(map(str, values[1:])))\n",
    "    \n",
    "    if temp:\n",
    "        data[temp][\"metrics\"].append(\" \".join(map(str, values)))\n",
    "\n",
    "fdf.to_excel(\"this.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HDFC\n",
    "table_parser = TableParser()\n",
    "tables = camelot.read_pdf(hdfc1,flavor=\"lattice\",pages=\"91-93\")\n",
    "dfs = pd.concat([table.df for table in tables], ignore_index=True)\n",
    "sc1 = table_parser._get_matching_col_indices(dfs,[\"HDFC.+?Fund\"],thresh=20)\n",
    "sc2 = table_parser._get_matching_col_indices(dfs,[\"MINIMUM\\\\s*APPLICATION\\\\s*AMOUNT\",\"Additional\\\\s*Purchase\"], thresh=20)\n",
    "\n",
    "print(\"Matched columns:\", sc1,sc2)\n",
    "all_cols = list(set(sc1 + sc2))\n",
    "fdf = dfs.iloc[:, all_cols]\n",
    "fdf.columns = [\"MUTUAL_FUND\",\"MIN_ADD\"]\n",
    "hdfc_pattern = re.compile(\n",
    "    r\"(HDFC.+?(?:FUNDS?|ETF|PATH|INDEX|SAVER)\\s*(?:OF FUNDS?|FUND OF FUNDS|FOF|.+?PLAN)?)\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "fdf.MUTUAL_FUND = table_parser._clean_series(fdf.MUTUAL_FUND,[\"normalize_alphanumeric\"])\n",
    "fdf.MUTUAL_FUND = fdf.MUTUAL_FUND.apply(lambda x: hdfc_pattern.findall(x)[0] if isinstance(x, str) and hdfc_pattern.findall(x) else x)\n",
    "fdf = table_parser._clean_dataframe(fdf,[\"newline_to_space\",\"str_to_pd_NA\"])\n",
    "fdf = fdf.dropna(axis=0, how=\"all\").dropna(axis=1, how=\"all\")\n",
    "\n",
    "data = {}\n",
    "for idx, rows in fdf.iterrows():\n",
    "    values = list(rows)\n",
    "    main_scheme_name = values[0]\n",
    "    if main_scheme_name not in data:\n",
    "        data[main_scheme_name] = {\"min_add\":values[1]}\n",
    "    else:\n",
    "        data[main_scheme_name].update({\"min_add_one\":values[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DSP\n",
    "\n",
    "table_parser = TableParser()\n",
    "sc1 = table_parser._get_matching_col_indices(dfs,[\"DSP.+?Fund\"],thresh=20)\n",
    "sc2 = table_parser._get_matching_col_indices(dfs,[\"REGULAR\\\\s+PLAN\",\"DIRECT\\\\s+PLAN\"], thresh=20)\n",
    "sc3 = table_parser._get_matching_col_indices(dfs,[\"Managing this scheme\",\"total work experience\"],thresh=20)\n",
    "print(\"Matched columns:\", sc1,sc2,sc3)\n",
    "all_cols = list(set(sc1 + sc2 + sc3))\n",
    "fdf = dfs.iloc[:, all_cols]\n",
    "fdf[\"LOAD_STRUCTURE\"] = fdf.iloc[:, -1]\n",
    "fdf.columns = [\"MUTUAL_FUND\",\"FUND_MANAGER\",\"MIN_ADD\",\"LOAD_STRUCTURE\"]\n",
    "\n",
    "dsp_pattern = re.compile(\n",
    "    r\"(DSP.+?(?:FUNDS?|ETF|PATH|INDEX|SAVER)\\s*(?:OF FUNDS?|FUND OF FUNDS|FOF|.+?PLAN)?)\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "fdf.MUTUAL_FUND = table_parser._clean_series(fdf.MUTUAL_FUND,[\"normalize_alphanumeric\"])\n",
    "fdf.MUTUAL_FUND = fdf.MUTUAL_FUND.apply(lambda x: dsp_pattern.findall(x)[0] if isinstance(x, str) and dsp_pattern.findall(x) else pd.NA)\n",
    "fdf = table_parser._clean_dataframe(fdf,[\"newline_to_space\",\"str_to_pd_NA\"])\n",
    "fdf = fdf.dropna(axis=0, how=\"all\").dropna(axis=1, how=\"all\")\n",
    "\n",
    "data = {}\n",
    "for idx, rows in fdf.iterrows():\n",
    "    values = list(rows)\n",
    "    main_scheme_name = values[0]\n",
    "    if main_scheme_name not in data:\n",
    "        data[main_scheme_name] = {\"fund_manager\":values[1],\"load_structure\":values[3],\"min_add\":values[2]}\n",
    "    else:\n",
    "         data[main_scheme_name].update({\"fund_manager_one\":values[1],\"load_structure_one\":values[3],\"min_add_one\":values[2]})\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"hello.json\",\"w\") as file:\n",
    "    json.dump(data,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proper_fund_names(path: str):\n",
    "    pattern = \"(HDFC.*?(?:FUND|Fund|ETF|FO?o?F)\\\\s*(?:of Funds?|.+?Plan|Fund of Funds?|Fund)?)\"\n",
    "    title = {}   \n",
    "    with fitz.open(path) as doc:\n",
    "        for pgn, page in enumerate(doc):\n",
    "            text = \" \".join(page.get_text(\"text\", clip=(0, 0, 400, 60)).split(\"\\n\"))\n",
    "            text = re.sub(\"[^A-Za-z0-9\\\\s\\\\-\\\\(\\\\).,]+|\\u2028\", \"\", text).strip()\n",
    "            print(text)\n",
    "            if matches := re.findall(pattern, text, re.DOTALL):\n",
    "                title[pgn] = \" \".join([_ for _ in matches[0].strip().split(\" \") if _])\n",
    "                print(pgn,matches[0])\n",
    "    return title\n",
    "title = get_proper_fund_names(sample_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os ,json \n",
    "import pandas as pd\n",
    "\n",
    "full_name = set()\n",
    "split_name = set()\n",
    "MANAGER_REGEX = FundRegex().MANAGER_STOP_WORDS\n",
    "for a,b,files in os.walk(r\"C:\\Users\\Kaustubh.keny\\Projects\\office-work\\mywork-repo\\sql_learn\\json\\MAR25DATA\"):\n",
    "    for paths in files:\n",
    "        sample_path = os.path.join(os.getcwd(),\"..\",\"sql_learn\",\"json\",\"MAR25DATA\",paths)\n",
    "        # print(sample_path)\n",
    "        try:\n",
    "            with open(sample_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                data = json.load(file)\n",
    "                for k,scheme in data.items():\n",
    "                    # print(k)\n",
    "                    if \"fund_manager\" in scheme:\n",
    "                        for entry in scheme['fund_manager']:\n",
    "                            # print(entry['name'])\n",
    "                            name = entry['name']\n",
    "                            for regex_ in MANAGER_REGEX:\n",
    "                                name = re.sub(f\"\\\\b{regex_}\\\\b|[^A-Za-z\\\\s0-9]+\",\"\",name, re.IGNORECASE)\n",
    "                                name = re.sub(r\"\\s+\",\" \",name)\n",
    "                            full_name.add(name)\n",
    "                            printthis = name if name.strip() else \"EMPTY\"\n",
    "                            # print(f\"<<{printthis}>>\")\n",
    "                            if printthis == \"EMPTY\":\n",
    "                                print(k,printthis,entry['name'])\n",
    "                            # split_name.add(name.split(' '))\n",
    "                    # print(scheme.keys())\n",
    "        except Exception as e:\n",
    "            print(f\"NEVER MIND {e}\")\n",
    "    # print(files)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envPDF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
