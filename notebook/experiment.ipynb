{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Config already loaded. Skipping re-initialization.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pprint, json, math, os, sys\n",
    "# dir_path = \"C:\\\\Users\\\\rando\\\\OneDrive\\\\Documents\\\\mywork-repo\"\n",
    "# fund_path = r\"C:\\Users\\rando\\OneDrive\\Documents\\PDFDrive\\Mar25\"\n",
    "# dry_path = r\"C:\\Users\\rando\\OneDrive\\Documents\\mywork-repo\\notebook\\DryRun.pdf\"\n",
    "\n",
    "dir_path = \"C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\office-work\\\\mywork-repo\"\n",
    "fund_path = \"C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\Mar 25\"\n",
    "dry_path = r\"C:\\Users\\Kaustubh.keny\\Projects\\office-work\\mywork-repo\\notebook\\DryRun.pdf\"\n",
    "sys.path.append(os.path.abspath(dir_path))\n",
    "from app.config_loader import load_config_once\n",
    "conf = load_config_once()\n",
    "\n",
    "\n",
    "import fitz, pdfplumber, ocrmypdf,camelot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "from app.utils import Helper\n",
    "from app.parse_regex import *\n",
    "\n",
    "dry_path = r'DryRun.pdf'\n",
    "fin_path = r'\\data\\input\\financial_indices.xlsx'\n",
    "mutual_fund = Helper.get_fund_paths(fund_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_clipped_data(input:str, pages:list, bboxes:list):\n",
    "        \n",
    "        document = fitz.open(input)\n",
    "        final_list = []\n",
    "    \n",
    "        for pgn in pages:\n",
    "            page = document[pgn]\n",
    "            \n",
    "            all_blocks = [] #store every data from bboxes\n",
    "            \n",
    "            for bbox in bboxes:\n",
    "                blocks, seen_blocks = [], set()  #store unique blocks based on content and bbox\n",
    "                \n",
    "                page_blocks = page.get_text('dict', clip=bbox)['blocks']\n",
    "                for block in page_blocks:\n",
    "                    if block['type'] == 0 and 'lines' in block: #type 0 means text block\n",
    "                        #hash_key\n",
    "                        block_key = (tuple(block['bbox']), tuple(tuple(line['spans'][0]['text'] for line in block['lines'])))\n",
    "                        if block_key not in seen_blocks:\n",
    "                            seen_blocks.add(block_key)\n",
    "                            blocks.append(block)\n",
    "\n",
    "                sorted_blocks = sorted(blocks, key=lambda x: (x['bbox'][1], x['bbox'][0]))\n",
    "                all_blocks.append(sorted_blocks)\n",
    "\n",
    "            final_list.append({\n",
    "                \"pgn\": pgn,\n",
    "                \"block\": all_blocks #will be list[list,list,..]\n",
    "            })\n",
    "\n",
    "        document.close()\n",
    "        return final_list\n",
    "    \n",
    "def extract_data_relative_line(path: str, line_x: float, side: str):\n",
    "    doc = fitz.open(path)\n",
    "    pages = doc.page_count\n",
    "\n",
    "    final_list = []\n",
    "\n",
    "    for pgn in range(pages):\n",
    "        page = doc[pgn]\n",
    "\n",
    "        blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "        sorted_blocks = sorted(blocks, key=lambda x: (x[\"bbox\"][1], x[\"bbox\"][0]))\n",
    "        extracted_blocks = []\n",
    "\n",
    "        # Keep track of blocks to avoid duplicates\n",
    "        added_blocks = set()\n",
    "\n",
    "        for block in sorted_blocks:\n",
    "            block_id = id(block)  # Unique identifier for the block\n",
    "\n",
    "            for line in block.get(\"lines\", []):\n",
    "                for span in line.get(\"spans\", []):\n",
    "                    origin = span[\"origin\"]\n",
    "                    x0, _ = origin\n",
    "\n",
    "                    # Check the side condition\n",
    "                    if side == \"left\" and x0 < line_x and block_id not in added_blocks:\n",
    "                        extracted_blocks.append(block)\n",
    "                        added_blocks.add(block_id)  # Mark block as added\n",
    "                    elif side == \"right\" and x0 > line_x and block_id not in added_blocks:\n",
    "                        extracted_blocks.append(block)\n",
    "                        added_blocks.add(block_id)  # Mark block as added\n",
    "\n",
    "      \n",
    "        final_list.append({\n",
    "            \"pgn\": pgn,\n",
    "            \"blocks\": extracted_blocks\n",
    "        })\n",
    "\n",
    "    doc.close()\n",
    "\n",
    "    return final_list\n",
    "  \n",
    "def get_clipped_data(input:str, bboxes:list[set], *args):\n",
    "    \n",
    "        document = fitz.open(input)\n",
    "        final_list = []\n",
    "        if args:\n",
    "            pages = list(args)\n",
    "        else:\n",
    "            pages = [i for i in document.page_count]\n",
    "        \n",
    "        for pgn in pages:\n",
    "            page = document[pgn]\n",
    "\n",
    "            blocks = []\n",
    "            for bbox in bboxes:\n",
    "                blocks.extend(page.get_text('dict', clip = bbox)['blocks']) #get all blocks\n",
    "            \n",
    "            filtered_blocks = [block for block in blocks if block['type']== 0 and 'lines' in block]\n",
    "            # sorted_blocks = sorted(filtered_blocks, key= lambda x: (x['bbox'][1], x['bbox'][0]))\n",
    "            \n",
    "             # Extract text from sorted blocks\n",
    "            extracted_text = []\n",
    "            for block in filtered_blocks:\n",
    "                block_text = []\n",
    "                for line in block['lines']:\n",
    "                    line_text = \" \".join(span['text'] for span in line['spans'])\n",
    "                    block_text.append(line_text)\n",
    "                extracted_text.append(\"\\n\".join(block_text))\n",
    "            \n",
    "            final_list.append({\n",
    "            \"pgn\": pgn,\n",
    "            \"block\": filtered_blocks,\n",
    "            \"text\": extracted_text\n",
    "            })\n",
    "            \n",
    "            \n",
    "        document.close()\n",
    "        return final_list\n",
    "    \n",
    "def extract_clipped_text_all_pages(pdf_path, clip_coords):\n",
    "    results = {}\n",
    "    doc = fitz.open(pdf_path)\n",
    "    clip_rect = fitz.Rect(*clip_coords)\n",
    "    try:\n",
    "        for page_number, page in enumerate(doc):\n",
    "            text = page.get_text(\"text\", clip=clip_rect).strip()\n",
    "            results[page_number] = text\n",
    "    finally:\n",
    "        doc.close()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sample_path = mutual_fund[\"Edelweiss Mutual Fund\"]\n",
    "#sid\n",
    "rp = r\"\\pdfs\\unifi1747199772321.pdf\"\n",
    "rp = r\"\\pdfs\\unifi1747373013796.pdf\"\n",
    "rp = r\"\\pdfs\\unifi1747199156275.pdf\"\n",
    "dsp1 = \"\\\\pdfs\\\\May SID\\\\DSP Mutual Fund\\\\8_17160_May-2025_1748246407_SID.pdf\"\n",
    "tata1 = r\"\\pdfs\\May SID\\Tata Mutual Fund\\38_17082_May-2025_1746423571_SID.pdf\"\n",
    "nip1 = \"\\\\pdfs\\\\May SID\\\\Nippon India Mutual Fund\\\\33_17142_May-2025_1747631690_SID.pdf\"\n",
    "sbi1 = \"\\\\pdfs\\\\May SID\\\\SBI Mutual Fund\\\\35_17126_May-2025_1747282059_SID.pdf\",\n",
    "samc1 = \"\\\\pdfs\\\\May SID\\\\Samco Mutual Fund\\\\58_17166_May-2025_1748246684_SID.pdf\"\n",
    "bar1 = \"\\\\pdfs\\\\May SID\\\\Baroda BNP Paribas Mutual Fund\\\\2_17136_May-2025_1747367452_SID.pdf\"\n",
    "unifi1 = \"\\\\pdfs\\\\May SID\\\\Unifi Mutual Fund\\\\1747199772321.pdf\"\n",
    "moti1 = \"\\\\pdfs\\\\May SID\\\\Motilal Oswal Mutual Fund\\\\28_17086_May-2025_1746677550_SID.pdf\"\n",
    "\n",
    "#kim\n",
    "kimsbi1 = \"\\\\pdfs\\\\May KIM\\\\SBI Mutual Fund\\\\35_17127_May-2025_1747282099_KIM.pdf\"\n",
    "kimnip1 = \"\\\\pdfs\\\\May KIM\\\\Nippon India Mutual Fund\\\\33_17143_May-2025_1747631729_KIM.pdf\"\n",
    "\n",
    "\n",
    "#other\n",
    "sample_path = dir_path + kimnip1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified PDF saved to: DryRun.pdf\n"
     ]
    }
   ],
   "source": [
    "lines = [\n",
    "    ((110, 0), (110, 812)),# Vertical line\n",
    "    ((0, 350), (812, 350)),\n",
    "    ((570, 0), (570, 812))\n",
    "]\n",
    "pages = [12, 14,16]\n",
    "bboxes = [[0, 120, 180, 812],[180, 85, 360, 812]] #[(0, 85, 180, 812),(180, 85, 360, 812),(0,100,270,812),(0,100,350,812)]\n",
    "pages = [i for i in range(1,110)]\n",
    "Helper.draw_lines_on_pdf(sample_path, lines, bboxes, pages, dry_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clip_coords = (484, 50,600, 100)  # (x0, y0, x1, y1)\n",
    "\n",
    "clipped_texts = extract_clipped_text_all_pages(sample_path, clip_coords)\n",
    "\n",
    "for page_num, text in clipped_texts.items():\n",
    "    print(f\"Page {page_num}:\\n{text}\\n{'-'*40}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _detect_table_start_by_keywords(df: pd.DataFrame, keywords: list):\n",
    "#     normalized_keywords = [re.sub(r\"\\s+\", \" \", kw.strip().lower()) for kw in keywords]\n",
    "\n",
    "#     for i in range(df.shape[0]):\n",
    "#         for j in range(df.shape[1]):\n",
    "#             cell = str(df.iat[i, j]).strip().lower()\n",
    "#             cell = re.sub(r\"\\s+\", \" \", cell)\n",
    "#             if any(kw in cell for kw in normalized_keywords):\n",
    "#                 return i, j\n",
    "#     return 0, 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalize_alphanumeric(text: str) -> str:\n",
    "    if not isinstance(text,str):\n",
    "        return text\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]+\", \" \", str(text))\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119'"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\",\".join([str(i) for i in range(103,120)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "bajajf1 = r\"C:\\Users\\Kaustubh.keny\\Projects\\Apr 25\\Bajaj finserv Mutual Fund\\59_30-Apr-25_FS.pdf\"\n",
    "bajajf2 = r\"C:\\Users\\Kaustubh.keny\\Projects\\Jan 25\\Bajaj finserv Mutual Fund\\59_31-Jan-25_FS.pdf\"\n",
    "hdfc1 = r\"C:\\Users\\Kaustubh.keny\\Projects\\Jan 25\\HDFC Mutual Fund\\12_31-Jan-2025_FS.pdf\"\n",
    "nipp1 = r\"C:\\Users\\Kaustubh.keny\\Projects\\Jan 25\\Nippon India Mutual Fund\\33_31-Jan-25_FS.pdf\"\n",
    "dsp1 = r\"C:\\Users\\Kaustubh.keny\\Projects\\Jan 25\\DSP Mutual Fund\\8_31-Jan-25_FS.pdf\"\n",
    "# tables = camelot.read_pdf(bajajf2,flavor=\"hybrid\",pages=\"14,15,16,17\")\n",
    "# tables = camelot.read_pdf(hdfc1,flavor=\"lattice\",pages=\"100,101,102\")\n",
    "tables = camelot.read_pdf(dsp1,flavor=\"lattice\",pages=\"104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120\")\n",
    "dfs = pd.concat([table.df for table in tables], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.to_excel(\"sample.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matching_row_indices(df, pattern, thresh = 5):\n",
    "    matched_rows = []\n",
    "    for idx, row in df.iterrows():\n",
    "        row_text = _normalize_alphanumeric(\" \".join(map(str, row)))\n",
    "        if matches:=re.findall(pattern, row_text, re.IGNORECASE):\n",
    "            if len(matches)>=thresh:\n",
    "                matched_rows.append(idx)\n",
    "    return matched_rows\n",
    "\n",
    "def get_matching_column_indices(df, pattern, thresh = 5):\n",
    "    matched_cols = []\n",
    "    for col in df.columns:\n",
    "        col_text = _normalize_alphanumeric(\" \".join(map(str, df[col].fillna(\"\").astype(str))))\n",
    "        if matches:=re.findall(pattern, col_text,re.IGNORECASE):\n",
    "            if len(matches)>=thresh:\n",
    "                matched_cols.append(col)  # or df.columns.get_loc(col) if you want integer index\n",
    "    return matched_cols\n",
    "\n",
    "def get_matching_cells(df, pattern):\n",
    "    matches = []\n",
    "    for r in range(df.shape[0]):\n",
    "        for c in range(df.shape[1]):\n",
    "            val = str(df.iat[r, c])\n",
    "            text = _normalize_alphanumeric(val)\n",
    "            if text and re.search(pattern, text, re.IGNORECASE):\n",
    "                matches.append((r, c))\n",
    "    return matches\n",
    "\n",
    "def slice_rows(indices):\n",
    "    indices = sorted(indices)\n",
    "    indices.append(-1)\n",
    "    return [(indices[i], indices[i + 1]) for i in range(len(indices) - 1)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched columns: [1] [6] [2]\n"
     ]
    }
   ],
   "source": [
    "# metric_row = get_matching_column_indices(dfs, r\"PORTFOLIO\\s+QUANT\", thresh=0)\n",
    "sc1 = get_matching_column_indices(dfs,r\"DSP.+?Fund\",thresh=20)\n",
    "sc2 = get_matching_column_indices(dfs,r\"(REGULAR\\s+PLAN|DIRECT\\s+PLAN)\",thresh=20)\n",
    "sc3 = get_matching_column_indices(dfs,r\"(Managing this scheme|total work experience)\",thresh=20)#r\"DSP.+?Fund\" r\"(REGULAR\\s+PLAN|DIRECT\\s+PLAN)\" r\"(Managing this scheme|total work experience)\"\n",
    "# print(\"Matched columns:\", metric_row) \n",
    "print(\"Matched columns:\", sc1,sc2,sc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kaustubh.keny\\AppData\\Local\\Temp\\ipykernel_15792\\3181273811.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df[\"LOAD_STRUCTURE\"] = filtered_df.iloc[:, -1]\n",
      "C:\\Users\\Kaustubh.keny\\AppData\\Local\\Temp\\ipykernel_15792\\3181273811.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df.replace(r\"\\n\",\"\",regex=True,inplace=True)\n"
     ]
    }
   ],
   "source": [
    "all_cols = list(set(sc1 + sc2 + sc3))\n",
    "filtered_df = dfs.iloc[:, all_cols]\n",
    "filtered_df[\"LOAD_STRUCTURE\"] = filtered_df.iloc[:, -1]\n",
    "filtered_df.columns = [\"MUTUAL_FUND\",\"FUND_MANAGER\",\"MIN_ADD\",\"LOAD_STRUCTURE\"]\n",
    "filtered_df.replace(r\"\\n\",\"\",regex=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MUTUAL_FUND</th>\n",
       "      <th>FUND_MANAGER</th>\n",
       "      <th>MIN_ADD</th>\n",
       "      <th>LOAD_STRUCTURE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DSP Flexi Cap Fund</td>\n",
       "      <td>Bhavin GandhiTotal work experience of 20 years...</td>\n",
       "      <td>PLANS: REGULAR PLAN (RP) &amp;DIRECT PLAN (DP)• Mi...</td>\n",
       "      <td>PLANS: REGULAR PLAN (RP) &amp;DIRECT PLAN (DP)• Mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DSP EquityOpportunitiesFund</td>\n",
       "      <td>Rohit SinghaniaTotal work experience of 23 yea...</td>\n",
       "      <td>PLANS: REGULAR PLAN (RP) &amp;DIRECT PLAN (DP)• Mi...</td>\n",
       "      <td>PLANS: REGULAR PLAN (RP) &amp;DIRECT PLAN (DP)• Mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DSP Top 100 Equity Fund</td>\n",
       "      <td>Abhishek SinghTotal work experience of 17 year...</td>\n",
       "      <td>PLANS: REGULAR PLAN (RP) &amp;DIRECT PLAN (DP)• Mi...</td>\n",
       "      <td>PLANS: REGULAR PLAN (RP) &amp;DIRECT PLAN (DP)• Mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DSP Mid Cap Fund</td>\n",
       "      <td>Vinit SambreTotal work experience of 26 years....</td>\n",
       "      <td>PLANS: REGULAR PLAN (RP) &amp;DIRECT PLAN (DP)• Mi...</td>\n",
       "      <td>PLANS: REGULAR PLAN (RP) &amp;DIRECT PLAN (DP)• Mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DSP India T.I.G.E.R. Fund(The Infrastructure G...</td>\n",
       "      <td>Charanjit SinghTotal work experience of 19 yea...</td>\n",
       "      <td>PLANS: REGULAR PLAN (RP) &amp;DIRECT PLAN (DP)• Mi...</td>\n",
       "      <td>PLANS: REGULAR PLAN (RP) &amp;DIRECT PLAN (DP)• Mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DSP Small Cap Fund</td>\n",
       "      <td>Vinit SambreTotal work experience of 26 years....</td>\n",
       "      <td>PLANS: REGULAR PLAN (RP) &amp;DIRECT PLAN (DP)• Mi...</td>\n",
       "      <td>PLANS: REGULAR PLAN (RP) &amp;DIRECT PLAN (DP)• Mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DSP Focus Fund</td>\n",
       "      <td>Vinit SambreTotal work experience of 26 years....</td>\n",
       "      <td>PLANS: REGULAR PLAN (RP) &amp;DIRECT PLAN (DP)• Mi...</td>\n",
       "      <td>PLANS: REGULAR PLAN (RP) &amp;DIRECT PLAN (DP)• Mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DSP NaturalResourcesand New EnergyFund</td>\n",
       "      <td>Rohit SinghaniaTotal work experience of 23 yea...</td>\n",
       "      <td>PLANS: REGULAR PLAN (RP) &amp;DIRECT PLAN (DP)• Mi...</td>\n",
       "      <td>PLANS: REGULAR PLAN (RP) &amp;DIRECT PLAN (DP)• Mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DSP ELSS Tax Saver Fund (erstwhile known as DS...</td>\n",
       "      <td>Rohit SinghaniaTotal work experience of 23 yea...</td>\n",
       "      <td>PLANS: REGULAR PLAN (RP) &amp;DIRECT PLAN (DP)• Re...</td>\n",
       "      <td>PLANS: REGULAR PLAN (RP) &amp;DIRECT PLAN (DP)• Re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>DSP Healthcare Fund</td>\n",
       "      <td>Chirag DagliTotal work experience of 22 years....</td>\n",
       "      <td>PLANS: REGULAR PLAN (RP) &amp;DIRECT PLAN (DP)• Mi...</td>\n",
       "      <td>PLANS: REGULAR PLAN (RP) &amp;DIRECT PLAN (DP)• Mi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         MUTUAL_FUND  \\\n",
       "0                                 DSP Flexi Cap Fund   \n",
       "1                        DSP EquityOpportunitiesFund   \n",
       "2                            DSP Top 100 Equity Fund   \n",
       "3                                   DSP Mid Cap Fund   \n",
       "4  DSP India T.I.G.E.R. Fund(The Infrastructure G...   \n",
       "5                                 DSP Small Cap Fund   \n",
       "6                                     DSP Focus Fund   \n",
       "7             DSP NaturalResourcesand New EnergyFund   \n",
       "8  DSP ELSS Tax Saver Fund (erstwhile known as DS...   \n",
       "9                                DSP Healthcare Fund   \n",
       "\n",
       "                                        FUND_MANAGER  \\\n",
       "0  Bhavin GandhiTotal work experience of 20 years...   \n",
       "1  Rohit SinghaniaTotal work experience of 23 yea...   \n",
       "2  Abhishek SinghTotal work experience of 17 year...   \n",
       "3  Vinit SambreTotal work experience of 26 years....   \n",
       "4  Charanjit SinghTotal work experience of 19 yea...   \n",
       "5  Vinit SambreTotal work experience of 26 years....   \n",
       "6  Vinit SambreTotal work experience of 26 years....   \n",
       "7  Rohit SinghaniaTotal work experience of 23 yea...   \n",
       "8  Rohit SinghaniaTotal work experience of 23 yea...   \n",
       "9  Chirag DagliTotal work experience of 22 years....   \n",
       "\n",
       "                                             MIN_ADD  \\\n",
       "0  PLANS: REGULAR PLAN (RP) &DIRECT PLAN (DP)• Mi...   \n",
       "1  PLANS: REGULAR PLAN (RP) &DIRECT PLAN (DP)• Mi...   \n",
       "2  PLANS: REGULAR PLAN (RP) &DIRECT PLAN (DP)• Mi...   \n",
       "3  PLANS: REGULAR PLAN (RP) &DIRECT PLAN (DP)• Mi...   \n",
       "4  PLANS: REGULAR PLAN (RP) &DIRECT PLAN (DP)• Mi...   \n",
       "5  PLANS: REGULAR PLAN (RP) &DIRECT PLAN (DP)• Mi...   \n",
       "6  PLANS: REGULAR PLAN (RP) &DIRECT PLAN (DP)• Mi...   \n",
       "7  PLANS: REGULAR PLAN (RP) &DIRECT PLAN (DP)• Mi...   \n",
       "8  PLANS: REGULAR PLAN (RP) &DIRECT PLAN (DP)• Re...   \n",
       "9  PLANS: REGULAR PLAN (RP) &DIRECT PLAN (DP)• Mi...   \n",
       "\n",
       "                                      LOAD_STRUCTURE  \n",
       "0  PLANS: REGULAR PLAN (RP) &DIRECT PLAN (DP)• Mi...  \n",
       "1  PLANS: REGULAR PLAN (RP) &DIRECT PLAN (DP)• Mi...  \n",
       "2  PLANS: REGULAR PLAN (RP) &DIRECT PLAN (DP)• Mi...  \n",
       "3  PLANS: REGULAR PLAN (RP) &DIRECT PLAN (DP)• Mi...  \n",
       "4  PLANS: REGULAR PLAN (RP) &DIRECT PLAN (DP)• Mi...  \n",
       "5  PLANS: REGULAR PLAN (RP) &DIRECT PLAN (DP)• Mi...  \n",
       "6  PLANS: REGULAR PLAN (RP) &DIRECT PLAN (DP)• Mi...  \n",
       "7  PLANS: REGULAR PLAN (RP) &DIRECT PLAN (DP)• Mi...  \n",
       "8  PLANS: REGULAR PLAN (RP) &DIRECT PLAN (DP)• Re...  \n",
       "9  PLANS: REGULAR PLAN (RP) &DIRECT PLAN (DP)• Mi...  "
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 11), (11, 20), (20, 28), (28, 36), (36, 47), (47, 58), (58, 63), (63, 77), (77, 88), (88, 97), (97, 106), (106, 118), (118, 129), (129, 133), (133, 137), (137, -1)]\n"
     ]
    }
   ],
   "source": [
    "# new_df = pd.concat([dfs.iloc[:, scheme_row[0]],dfs.iloc[:, metric_row[0]:metric_row[0]+2]], axis=1, ignore_index=True)\\\n",
    "#     .replace(\"\",pd.NA)\\\n",
    "#     .dropna(axis=0,how=\"all\")\\\n",
    "#     .replace(pd.NA,\"\")\\\n",
    "#     .reset_index(drop=True)\n",
    "# matched_rows = get_matching_row_indices(new_df, r\"Bajaj?\\s+Finserv\")\n",
    "# bins = slice_rows(matched_rows)\n",
    "# print(bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in bins:\n",
    "    content = new_df.iloc[i:j,:]\n",
    "    content_text = \"\"\n",
    "    for idx,row in content.iterrows():\n",
    "        content_text+=\" \".join(row.to_list())\n",
    "    print(content_text,\"\\n_____\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.to_excel(\"sample.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Helper.get_all_pdf_data(sample_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proper_fund_names(path: str):\n",
    "    pattern = \"(HDFC.*?(?:FUND|Fund|ETF|FO?o?F)\\\\s*(?:of Funds?|.+?Plan|Fund of Funds?|Fund)?)\"\n",
    "    title = {}   \n",
    "    with fitz.open(path) as doc:\n",
    "        for pgn, page in enumerate(doc):\n",
    "            text = \" \".join(page.get_text(\"text\", clip=(0, 0, 400, 60)).split(\"\\n\"))\n",
    "            text = re.sub(\"[^A-Za-z0-9\\\\s\\\\-\\\\(\\\\).,]+|\\u2028\", \"\", text).strip()\n",
    "            print(text)\n",
    "            if matches := re.findall(pattern, text, re.DOTALL):\n",
    "                title[pgn] = \" \".join([_ for _ in matches[0].strip().split(\" \") if _])\n",
    "                print(pgn,matches[0])\n",
    "    return title\n",
    "title = get_proper_fund_names(sample_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os ,json \n",
    "import pandas as pd\n",
    "\n",
    "full_name = set()\n",
    "split_name = set()\n",
    "MANAGER_REGEX = FundRegex().MANAGER_STOP_WORDS\n",
    "for a,b,files in os.walk(r\"C:\\Users\\Kaustubh.keny\\Projects\\office-work\\mywork-repo\\sql_learn\\json\\MAR25DATA\"):\n",
    "    for paths in files:\n",
    "        sample_path = os.path.join(os.getcwd(),\"..\",\"sql_learn\",\"json\",\"MAR25DATA\",paths)\n",
    "        # print(sample_path)\n",
    "        try:\n",
    "            with open(sample_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                data = json.load(file)\n",
    "                for k,scheme in data.items():\n",
    "                    # print(k)\n",
    "                    if \"fund_manager\" in scheme:\n",
    "                        for entry in scheme['fund_manager']:\n",
    "                            # print(entry['name'])\n",
    "                            name = entry['name']\n",
    "                            for regex_ in MANAGER_REGEX:\n",
    "                                name = re.sub(f\"\\\\b{regex_}\\\\b|[^A-Za-z\\\\s0-9]+\",\"\",name, re.IGNORECASE)\n",
    "                                name = re.sub(r\"\\s+\",\" \",name)\n",
    "                            full_name.add(name)\n",
    "                            printthis = name if name.strip() else \"EMPTY\"\n",
    "                            # print(f\"<<{printthis}>>\")\n",
    "                            if printthis == \"EMPTY\":\n",
    "                                print(k,printthis,entry['name'])\n",
    "                            # split_name.add(name.split(' '))\n",
    "                    # print(scheme.keys())\n",
    "        except Exception as e:\n",
    "            print(f\"NEVER MIND {e}\")\n",
    "    # print(files)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envPDF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
