{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint, json, math, os, sys\n",
    "import fitz, pdfplumber, ocrmypdf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# dir_path = \"C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\office-work\\\\mywork-repo\"\n",
    "# fund_path = \"C:\\\\Users\\\\Kaustubh.keny\\\\Projects\\\\Feb 25\"\n",
    "\n",
    "dir_path = \"C:\\\\Users\\\\rando\\\\OneDrive\\\\Documents\\\\mywork-repo\"\n",
    "fund_path =  \"C:\\\\Users\\\\rando\\\\OneDrive\\\\Documents\\\\Feb25\"\n",
    "sys.path.append(os.path.abspath(dir_path))\n",
    "\n",
    "from app.utils import Helper\n",
    "from app.parse_regex import *\n",
    "\n",
    "dry_path = r'\\data\\output\\DryRun.pdf'\n",
    "fin_path = r'\\data\\input\\financial_indices.xlsx'\n",
    "mutual_fund = Helper.get_fund_paths(fund_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_clipped_data(input:str, pages:list, bboxes:list):\n",
    "        \n",
    "        document = fitz.open(input)\n",
    "        final_list = []\n",
    "    \n",
    "        for pgn in pages:\n",
    "            page = document[pgn]\n",
    "            \n",
    "            all_blocks = [] #store every data from bboxes\n",
    "            \n",
    "            for bbox in bboxes:\n",
    "                blocks, seen_blocks = [], set()  #store unique blocks based on content and bbox\n",
    "                \n",
    "                page_blocks = page.get_text('dict', clip=bbox)['blocks']\n",
    "                for block in page_blocks:\n",
    "                    if block['type'] == 0 and 'lines' in block: #type 0 means text block\n",
    "                        #hash_key\n",
    "                        block_key = (tuple(block['bbox']), tuple(tuple(line['spans'][0]['text'] for line in block['lines'])))\n",
    "                        if block_key not in seen_blocks:\n",
    "                            seen_blocks.add(block_key)\n",
    "                            blocks.append(block)\n",
    "\n",
    "                sorted_blocks = sorted(blocks, key=lambda x: (x['bbox'][1], x['bbox'][0]))\n",
    "                all_blocks.append(sorted_blocks)\n",
    "\n",
    "            final_list.append({\n",
    "                \"pgn\": pgn,\n",
    "                \"block\": all_blocks #will be list[list,list,..]\n",
    "            })\n",
    "\n",
    "        document.close()\n",
    "        return final_list\n",
    "    \n",
    "def extract_data_relative_line(path: str, line_x: float, side: str):\n",
    "    doc = fitz.open(path)\n",
    "    pages = doc.page_count\n",
    "\n",
    "    final_list = []\n",
    "\n",
    "    for pgn in range(pages):\n",
    "        page = doc[pgn]\n",
    "\n",
    "        blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "        sorted_blocks = sorted(blocks, key=lambda x: (x[\"bbox\"][1], x[\"bbox\"][0]))\n",
    "        extracted_blocks = []\n",
    "\n",
    "        # Keep track of blocks to avoid duplicates\n",
    "        added_blocks = set()\n",
    "\n",
    "        for block in sorted_blocks:\n",
    "            block_id = id(block)  # Unique identifier for the block\n",
    "\n",
    "            for line in block.get(\"lines\", []):\n",
    "                for span in line.get(\"spans\", []):\n",
    "                    origin = span[\"origin\"]\n",
    "                    x0, _ = origin\n",
    "\n",
    "                    # Check the side condition\n",
    "                    if side == \"left\" and x0 < line_x and block_id not in added_blocks:\n",
    "                        extracted_blocks.append(block)\n",
    "                        added_blocks.add(block_id)  # Mark block as added\n",
    "                    elif side == \"right\" and x0 > line_x and block_id not in added_blocks:\n",
    "                        extracted_blocks.append(block)\n",
    "                        added_blocks.add(block_id)  # Mark block as added\n",
    "\n",
    "      \n",
    "        final_list.append({\n",
    "            \"pgn\": pgn,\n",
    "            \"blocks\": extracted_blocks\n",
    "        })\n",
    "\n",
    "    doc.close()\n",
    "\n",
    "    return final_list\n",
    "  \n",
    "def get_clipped_data(input:str, bboxes:list[set], *args):\n",
    "    \n",
    "        document = fitz.open(input)\n",
    "        final_list = []\n",
    "        if args:\n",
    "            pages = list(args)\n",
    "        else:\n",
    "            pages = [i for i in document.page_count]\n",
    "        \n",
    "        for pgn in pages:\n",
    "            page = document[pgn]\n",
    "\n",
    "            blocks = []\n",
    "            for bbox in bboxes:\n",
    "                blocks.extend(page.get_text('dict', clip = bbox)['blocks']) #get all blocks\n",
    "            \n",
    "            filtered_blocks = [block for block in blocks if block['type']== 0 and 'lines' in block]\n",
    "            # sorted_blocks = sorted(filtered_blocks, key= lambda x: (x['bbox'][1], x['bbox'][0]))\n",
    "            \n",
    "             # Extract text from sorted blocks\n",
    "            extracted_text = []\n",
    "            for block in filtered_blocks:\n",
    "                block_text = []\n",
    "                for line in block['lines']:\n",
    "                    line_text = \" \".join(span['text'] for span in line['spans'])\n",
    "                    block_text.append(line_text)\n",
    "                extracted_text.append(\"\\n\".join(block_text))\n",
    "            \n",
    "            final_list.append({\n",
    "            \"pgn\": pgn,\n",
    "            \"block\": filtered_blocks,\n",
    "            \"text\": extracted_text\n",
    "            })\n",
    "            \n",
    "            \n",
    "        document.close()\n",
    "        return final_list\n",
    "    \n",
    "def get_clipped_text(input:str, bboxes:list[set],*args):\n",
    "\n",
    "    document = fitz.open(input)\n",
    "    final_list = []\n",
    "    \n",
    "    if args:\n",
    "        pages = list(args)\n",
    "    else:\n",
    "        pages = [i for i in document.page_count]\n",
    "    \n",
    "    for pgn in pages:\n",
    "        page = document[pgn]\n",
    "        blocks = []\n",
    "        for bbox in bboxes:\n",
    "            blocks = page.get_text('text', clip = bbox).split('\\n') #get all blocks\n",
    "        final_list.append({\n",
    "        \"pgn\": pgn,\n",
    "        \"block\": blocks\n",
    "        })   \n",
    "    document.close()\n",
    "    return final_list\n",
    "\n",
    "def get_proper_fund_names(path: str, pages: list):\n",
    "    doc = fitz.open(path)\n",
    "    title = {}\n",
    "\n",
    "    for pgn in pages:\n",
    "        page = doc[pgn]\n",
    "        blocks = page.get_text(\"dict\")['blocks']\n",
    "        text_all = \" \".join(\n",
    "            span[\"text\"].strip()\n",
    "            for block in blocks[:4]\n",
    "            for line in block.get(\"lines\", [])\n",
    "            for span in line.get(\"spans\", [])\n",
    "            if span[\"text\"].strip()\n",
    "        )\n",
    "\n",
    "        text_all = re.sub(r'[^A-Za-z0-9\\s]+', '', text_all).strip()\n",
    "        matches = re.findall(r\"((?:LIC\\s*MF|BLNCED|LOW)\\s+.*?(?:FUND|ETF|FTF|FOF|PLAN|SAVER))\", text_all, re.IGNORECASE)\n",
    "\n",
    "        title[pgn] = matches[0] if matches else \"\"\n",
    "\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_path = mutual_fund[\"Lic Mutual Fund\"]\n",
    "sample_path = r\"C:\\Users\\rando\\OneDrive\\Documents\\Feb25\\LIC Mutual Fund\\25_28-Feb-25_FS_ocr.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified PDF saved to: C:\\Users\\rando\\OneDrive\\Documents\\mywork-repo\\data\\output\\DryRun.pdf\n"
     ]
    }
   ],
   "source": [
    "lines = [\n",
    "    ((200, 0), (200, 812)),# Vertical line\n",
    "    ((360, 0), (360, 812)),\n",
    "    # ((0, 145), (812, 145))\n",
    "]\n",
    "pages = [12, 14,16]\n",
    "bboxes = [[435, 17, 610, 103],] #[(0, 85, 180, 812),(180, 85, 360, 812),(0,100,270,812),(0,100,350,812)]\n",
    "pages = [i for i in range(1,110)]\n",
    "Helper.draw_lines_on_pdf(sample_path, lines, bboxes, pages, dir_path +dry_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_clipped_text_all_pages(pdf_path, clip_coords):\n",
    "    results = {}\n",
    "    doc = fitz.open(pdf_path)\n",
    "    clip_rect = fitz.Rect(*clip_coords)\n",
    "    try:\n",
    "        for page_number, page in enumerate(doc):\n",
    "            text = page.get_text(\"text\", clip=clip_rect).strip()\n",
    "            results[page_number] = text\n",
    "    finally:\n",
    "        doc.close()\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clip_coords = (484, 50,600, 100)  # (x0, y0, x1, y1)\n",
    "\n",
    "clipped_texts = extract_clipped_text_all_pages(sample_path, clip_coords)\n",
    "\n",
    "for page_num, text in clipped_texts.items():\n",
    "    print(f\"Page {page_num}:\\n{text}\\n{'-'*40}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An Open Ended dynamic equity scheme investing across large cap, mid cap and small cap stocks\n",
    "# An open-ended insurance linked tax saving scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proper_fund_names(path: str):\n",
    "    pattern = \"((?:LI?i?C|BSE|BANK|SMALL|HEALTH|MNEY|An\\\\s*open).*?(?:FUND|Path|ETF|FTF|EOF|FOF|PLAN|SAVER|tax saving scheme|small cap stocks)\\\\s*(?:FUND\\\\s*OF\\\\s*FUND)?)\"\n",
    "    title = {}   \n",
    "    with fitz.open(path) as doc:\n",
    "        for pgn, page in enumerate(doc):\n",
    "            text = \" \".join(page.get_text(\"text\", clip=( 0,\n",
    "          0,\n",
    "          400,\n",
    "          100)).split(\"\\n\"))\n",
    "            text = re.sub(\"[^A-Za-z0-9\\\\s\\\\-\\\\(\\\\).,]+|\\u2028\", \"\", text).strip()\n",
    "            # print(text)\n",
    "            if matches := re.findall(pattern, text, re.DOTALL):\n",
    "                title[pgn] = \" \".join([_ for _ in matches[0].strip().split(\" \") if _])\n",
    "                print(pgn,matches[0])\n",
    "    return title\n",
    "title = get_proper_fund_names(sample_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r\"(360 ONE.*)$\" 0,0,520,50\n",
    "# \"(Aditya Birla.*(?:Plan\\\\*?\\\\#?\\\\'?|Sensex|Fund|Path|ETF|FOF\\\\*?|Scheme|EOF|Funds\\\\*?|Yojna)?)$\" 0,0,470,25\n",
    "# \"(Bajaj.*(?:Fund|Path|ETF|FOF|EOF|Growth))$\" 0,0,470,50\n",
    "# \"(Bank of India.*?(?:Plan|Funds?|ETF|FOF|FTF))\"  0,55,280,105\n",
    "# \"(Baroda BNP.*?(?:Fund|Path|ETF|FTF|FOF|Index|Fund of Fund))\" 0,0,220,120\n",
    "# \"CANARA.*?\\\\)\" 0,0,400,55\n",
    "# \"((?:DSP|Bharat).*?(?:Fund\\\\s*(?:of Fund)?|FUND|ETF|FTF|FOF))\" 0,0,500,40\n",
    "# \"(Edelweiss\\s*.+?(?:Fund|Path|ETF|FOF|Path))\" 0,0,150,100\n",
    "# \"((?:Franklin|Templeton).*?(?:Fund\\\\s*(?:of Funds)?|Plan))\" 0,0,470,80\n",
    "# \"(GROWW.*?FUND)\" 0,0,470,60\n",
    "# \"(HDFC.*?(?:FUND|Fund\\\\s*(?:of Funds?)?|ETF\\\\s*(?:Fund of Funds?)?))\" 0,0,400,60\n",
    "# \"(HSBC.*?(?:FUND|Fund\\\\s*(?:of Funds?)?|ETF\\\\s*(?:Fund of Funds?)?))\" 0,0,600,45\n",
    "# \"(Helios.*)\" 0,0,600,40\n",
    "# \"((?:ICICI|BHARAT).*)\" 0,0,490,30\n",
    "# \"(Invesco India.*?(?:Fund(?:of Funds?)?|ETF|FOF|Path))\" 180,0,590,40\n",
    "# \"(ITI.*?(?:Fund(?:of Funds?)?|ETF|FOF|Path))\" 0,30,300,100\n",
    "# \"(JM.*?(?:Fund(?:of Funds?)?|ETF|FOF|Path))\" 0,0,400,50\n",
    "# \"(KOTAK.*?(?:FUND\\\\s*(?:OF FUNDS?\\\\s*|-\\\\s*\\\\w+)?|ETF|FOF|PATH|FTF))\" 0,0,450,70\n",
    "# \"((?:LI?i?C|BSE|BANK|SMALL|HEALTH).*?(?:FUND|Path|ETF|FTF|EOF|FOF|PLAN|SAVER|FUND\\s*OF\\s*FUND))\" 0,0,400,100\n",
    "# \"\\\\b(Mahindra.*?(?:Fund|ETF|EOF|FOF|FTF|Path|FO))\\\\b\" 170,50,450,80\n",
    "# r'MIRAE ASSET .*?\\b(?:ETF|EOF|FOF|FTF|FUND|FUND OF FUND|INDEX FUND)\\b' 0,0,560,140\n",
    "# \"(NJ.*?(?:FUND\\\\s*(?:OF FUNDS?\\\\s*|-\\\\s*\\\\w+)?|ETF|FOF|PATH|FTF))\" 0,30,480,70\n",
    "# \"([A-Z0-9\\\\s\\\\-]+\\\\s*PGIM INDIA)\" 0,0,300,80\n",
    "# \"(Samco.*?Fund)\" 0,30,600,100\n",
    "# SBI PASSIVE r\"((?:SBI|i\\s*_|S35).*$)\" 0, 0, 400, 50 \n",
    "# SBI NORMAL \"(SBI.*?(?:Fund\\\\s*(?:of funds?|.*?Plan)?|ETF|FTF|FOF))\" 160, 640, 550, 812\n",
    "# r\"(Union\\s*[A-Za-z\\s]+?(?:FUND|PATH|ETF|FOF))\" 0,0,180,150 \n",
    "# \"(Sundaram.*?(?:Fund\\\\s*(?:of funds?|.*?Plan|Series)?|ETF|FTF|FOF))\" 0,0,400,40\n",
    "# \"((?:Tata|Treasury|TATA).*?(?:Funds?(?:.*?Plan)?|ETF|FOF|EOF|Plan.*?(?:days)?))\" 0,0,540,40\n",
    "# \"((?:TAURUS|Taurus).*?(?:FUND|Fund))\" 0,0,480,40\n",
    "# \"(TRUSTMF.*?(?:FUND|Fund))\" 0,0,340,45\n",
    "# \"(quant.*?(?:FUND|Fund))\" 0,0,480,80\n",
    "# \"(UTI.*?(?:FUND(?:\\\\s*OF FUND|.+?DURATION)?|PLAN|ETF|FTF|FOF|Fund|ETF(?: FUND OF FUND)?)(?:\\\\s*\\\\(\\\\s*Erst.+?\\\\))?)\" 0,0,460,35\n",
    "# \"(WhiteOak.*?(?:Fund))\" 0,0,410,60\n",
    "# \"(Zerodha.*?(?:Fund|ETF(?:\\\\s*FoF)?|FoF))\" 0,60,500,180\n",
    "# \"(Motilal.*?(?:Fund\\\\s*(?:o?O?f Funds?(?:.+?Aggressive|.+?Conservative)?)?|ETF(?:.+? of Funds?)?|FoF))\" 0,0,600,60\n",
    "\n",
    "# \"(SHRIRAM.*?(?:FUNDS?|ETF|Fo?O?F|PLANS?))\" 0,0,480,70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Helper.get_all_pdf_data(sample_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with fitz.open(sample_path) as doc:\n",
    "    indices = [\"Fund Manager\", \"Co-Fund Manager\", \"Date of Allotment\", \"Benchmark Index\", \"Minimum Application\", \"Additional Purchase\", \"Entry Load\", \"Exit Load\", \"Portfolio Turnover\", \"Net AUM\", \"Monthly Average AUM\", \"Std Dev\", \"Sharpe Ratio\", \"Portfolio Beta\", \"R Squared\", \"Treynor\", \"YTM\", \"Macaulay\", \"Residual Maturity\", \"NAV\"]\n",
    "    output_path = sample_path.replace(\".pdf\", \"_hltd.pdf\")\n",
    "    highlight_count,found_indices = 0,[]\n",
    "    for pgn, page in enumerate(doc):\n",
    "        blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "        for block in blocks:\n",
    "            if \"lines\" not in block:\n",
    "                continue\n",
    "            for line in block[\"lines\"]:\n",
    "                for span in line[\"spans\"]:\n",
    "                    text = span[\"text\"].strip().lower()\n",
    "                    text =  re.sub(\"[^\\\\w\\\\s]\", \"\", text)\n",
    "                    for indice in indices:\n",
    "                        if re.search(rf\"\\b{re.escape(indice)}\\b\", text,re.IGNORECASE):\n",
    "                            if indice not in found_indices:\n",
    "                                found_indices.append(indice)\n",
    "                                highlight_count += 1\n",
    "                            page.add_highlight_annot(fitz.Rect(span[\"bbox\"]))\n",
    "                            break\n",
    "\n",
    "    doc.save(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#canara\n",
    "# def _update_manager_data(self,main_key:str,manager_data):\n",
    "#     name = r\"(?:Mr\\.?|Mrs\\.?|Ms\\.?)\\s+([A-Z][a-z]+\\s[A-Z][a-z]+)\"\n",
    "#     exp = r\"\\b\\d+\\s*Years?\\b\"\n",
    "#     since = r\"\\bSince\\s*([0-9]+\\s*-\\s*[A-Za-z]+\\.?\\s*-\\s*[0-9]+)\\b\"\n",
    "#     nsample, msample, esample = [], [], []\n",
    "#     nlength = 0\n",
    "#     value = \" \".join(manager_data.values())\n",
    "#     nsample = re.findall(name, value, re.IGNORECASE)\n",
    "#     esample = re.findall(exp, value, re.IGNORECASE)\n",
    "#     msample = re.findall(since, value, re.IGNORECASE)\n",
    "    \n",
    "#     nlength = len(nsample)\n",
    "#     msample += [\"\"] * (nlength - len(msample))\n",
    "#     esample += [\"\"] * (nlength - len(esample))\n",
    "    \n",
    "#     final_list = [self._return_manager_data(since=m,name=n,exp=e)for n, m, e in zip(nsample, msample, esample)]\n",
    "#     return {main_key:final_list}\n",
    "\n",
    "# # hdfc\n",
    "# def _extract_manager_data(self, main_key: str, data, pattern:str):\n",
    "#     DATE_PATTERN = r\"([A-Za-z]+\\s*\\d+),\"\n",
    "#     NAME_PATTERN = r\"([A-Za-z]+\\s[A-Za-z]+)\"\n",
    "#     EXP_PATTERN = r\"over (\\d+ years)\"\n",
    "#     YEAR_PATTERN = r\"(\\d{4})\"\n",
    "\n",
    "#     manager_data = \" \".join(data)\n",
    "#     manager_data = re.sub(r\"[^A-Za-z0-9\\s\\-\\(\\).,]+\", \"\", manager_data).strip()\n",
    "\n",
    "#     experience_years = re.findall(EXP_PATTERN, manager_data, re.IGNORECASE)\n",
    "#     dates = re.findall(DATE_PATTERN, manager_data, re.IGNORECASE)[:len(experience_years)]\n",
    "#     years = re.findall(YEAR_PATTERN, manager_data, re.IGNORECASE)[:len(experience_years)]\n",
    "#     names = re.findall(NAME_PATTERN, manager_data, re.IGNORECASE)[:len(experience_years)]\n",
    "    \n",
    "#     managing_since = [f\"{date}, {year}\" for date, year in zip(dates, years)]\n",
    "#     experience_list = [f\"{exp} years\" for exp in experience_years]\n",
    "#     final_list = [\n",
    "#         self._return_manager_data(name=name,since=since,exp=exp)\n",
    "#         for since, exp, name in zip(managing_since, experience_list, names)\n",
    "#     ]\n",
    "    \n",
    "#     return {main_key:final_list}\n",
    "\n",
    "\n",
    "# #NJMF\n",
    "# def _update_manager_data(self,main_key:str,manager_data):\n",
    "#     exp =\"\\\\d{1,2} years\"\n",
    "#     name = \"(?:Mr\\\\.?|Mrs\\\\.?|Ms\\\\.?)\\\\s*([A-Za-z]+\\\\s*[A-Za-z]+)\"\n",
    "#     since = \"(?:since|from)\\\\s*([A-Za-z]+\\\\s*\\\\d{1,2},\\\\s*\\\\d{3,4}|inception)\"\n",
    "    \n",
    "#     manager_data = \" \".join(manager_data) if isinstance(manager_data,list) else manager_data\n",
    "#     manager_data =re.sub(self.REGEX[\"escape\"], \"\", manager_data).strip()\n",
    "#     nsample = re.findall(name, manager_data, re.IGNORECASE)\n",
    "#     esample = re.findall(exp, manager_data, re.IGNORECASE)\n",
    "#     msample = re.findall(since, manager_data, re.IGNORECASE)\n",
    "#     final_list = [self._return_manager_data(since=m,name=n,exp=e)for n, m, e in zip(nsample, msample, esample)]\n",
    "#     return {main_key:final_list}\n",
    "\n",
    "# #sbi\n",
    "# def _extract_manager_data(self, main_key: str, data, pattern: str):\n",
    "#     name = \"(?:Mr\\\\.?|Mrs\\\\.?|Ms\\\\.?)\\\\s*([A-Za-z.]+\\\\s*[A-Za-z]+)\"\n",
    "#     since = \"((?:\\\\(w.e.f\\\\.?)?[A-Za-z]+\\\\s*\\\\d{4}\\\\s*(?:\\\\()?)\"\n",
    "#     exp = \"([0-9]+ years)\"\n",
    "\n",
    "#     final_list = []\n",
    "#     manager_data = \" \".join(data) if isinstance(data,list) else data\n",
    "#     manager_data =re.sub(self.REGEX[\"escape\"], \"\", manager_data).strip()\n",
    "#     n = re.findall(name,manager_data, re.IGNORECASE)\n",
    "#     s = re.findall(since,manager_data, re.IGNORECASE)\n",
    "#     e = re.findall(exp,manager_data, re.IGNORECASE)\n",
    "    \n",
    "#     adjust = lambda target, lst: target[:len(lst)] + ([target[-1]] * abs(len(target) - len(lst)) if lst else [\"\"])\n",
    "#     n,s = adjust(n,e),adjust(s,e)\n",
    "#     for name,since,exp in zip(n,s,e):\n",
    "#         final_list.append(self._return_manager_data(name=name,since=since,exp=exp))\n",
    "#     return {main_key: final_list}\n",
    "\n",
    "# #sbI PASSIVE\n",
    "# def _update_manager_data(self,main_key:str,data):\n",
    "#     final_list = []\n",
    "#     manager_data = \" \".join(data) if isinstance(data, list) else data\n",
    "#     manager_data = re.sub(self.REGEX['escape'], \"\", manager_data).strip()\n",
    "#     pattern_info = self.REGEX[\"manager\"]\n",
    "#     regex_pattern = pattern_info['pattern']\n",
    "#     field_names = pattern_info['fields']\n",
    "#     if matches := re.findall(regex_pattern, manager_data, re.IGNORECASE):\n",
    "#         for match in matches:\n",
    "#             if isinstance(match, str):\n",
    "#                 match = (match,)\n",
    "#             record = {field_names[i]: match[i] if i < len(match) else \"\" for i in range(len(field_names))} #kwargs\n",
    "#             final_list.append(self._return_manager_data(**record))\n",
    "\n",
    "#     return {main_key: final_list}\n",
    "    \n",
    "# # ADITYA\n",
    "# def _update_manager_data(self,main_key:str,manager_data):\n",
    "#     nsample, msample, esample = [], [], []\n",
    "#     nlength = 0\n",
    "#     for key, value in manager_data.items():\n",
    "#         if re.search(r\"\\bfund_manager\\b\", key, re.IGNORECASE):\n",
    "#             nsample = re.findall(self.REGEX[\"manager\"][\"name\"], value, re.IGNORECASE)\n",
    "#             nlength = len(nsample)\n",
    "\n",
    "#         elif re.search(r\"^managing\", key, re.IGNORECASE):\n",
    "#             msample = re.findall(self.REGEX[\"manager\"][\"since\"], value, re.IGNORECASE)\n",
    "            \n",
    "#         elif re.search(r\"^experience\", key, re.IGNORECASE):\n",
    "#             esample = re.findall(self.REGEX[\"manager\"][\"exp\"], value, re.IGNORECASE)\n",
    "#     nlength = len(nsample)\n",
    "#     msample += [\"\"] * (nlength - len(msample))\n",
    "#     esample += [\"\"] * (nlength - len(esample))\n",
    "\n",
    "#     final_list = [\n",
    "#         self._return_manager_data(since=m,name=n,exp=e)\n",
    "#         for n, m, e in zip(nsample, msample, esample)\n",
    "#     ]\n",
    "\n",
    "#     return {main_key:final_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import camelot\n",
    "def get_something(path: str):\n",
    "    pattern = r\"^(REDEMPTION PROCEEDS|FEATURES|ASSET ALLOCATION|FUND MANAGER|SCHEME|Sr\\. No\\.)$\"\n",
    "\n",
    "    with fitz.open(path) as doc:\n",
    "        exists = defaultdict(int)\n",
    "        for pgn, page in enumerate(doc):\n",
    "            page_text = [t.strip() for t in page.get_text().split(\"\\n\")]\n",
    "            for text in page_text:\n",
    "                if re.match(pattern,text):\n",
    "                    exists[pgn]+=1\n",
    "                    \n",
    "    \n",
    "    return [str(pgn+1) for pgn, count in exists.items() if count > 4] #camelot starts pages from 1\n",
    "\n",
    "pages = get_something(sample_path)\n",
    "\n",
    "imp_pages = \",\".join(pages)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = camelot.read_pdf(sample_path,pages=imp_pages, flavor=\"stream\", table_areas= [\"30,50,612,812\"],column_tol = 4, split_text = True) #[\"30,50,612,812\"]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import camelot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def clean_column_name(x):\n",
    "    cleaned = re.sub(r\"[^A-Za-z0-9\\s]\", \"\", str(x), flags=re.IGNORECASE) #Corrected re.sub.\n",
    "    cleaned = \"_\".join(cleaned.split())\n",
    "    return cleaned\n",
    "\n",
    "with pd.ExcelWriter(\"cleaned_tables.xlsx\", engine=\"xlsxwriter\") as writer:\n",
    "    for i, table in enumerate(tables):\n",
    "        df = table.df\n",
    "        df = df.map(lambda x: \" \".join(str(x).split(\"\\n\")).strip())\n",
    "        df.replace(\"\", np.nan, inplace=True)\n",
    "        df.iloc[2:, 0] = df.iloc[2:, 0].ffill()\n",
    "        df = df.iloc[1:, :]\n",
    "        df.columns = df.iloc[0, :].apply(clean_column_name)\n",
    "        # print(df.columns)\n",
    "        df = df.iloc[1:, :]\n",
    "        sheet_name = f\"Table_{i+1}\"\n",
    "\n",
    "        # workbook = writer.book\n",
    "        # worksheet = writer.sheets[sheet_name]\n",
    "\n",
    "        # row = 1\n",
    "        # last_value = None\n",
    "        # start_row = None\n",
    "\n",
    "        # for j, value in enumerate(df.iloc[:, 0], start=1):\n",
    "        #     if pd.notna(value):  # New group found\n",
    "        #         if last_value is not None and start_row is not None:\n",
    "        #             worksheet.merge_range(start_row, 0, row - 1, 0, last_value)  # Merge previous block\n",
    "        #         last_value = value\n",
    "        #         start_row = row\n",
    "        #     row += 1\n",
    "\n",
    "        # # Merge last group\n",
    "        # if last_value is not None and start_row is not None:\n",
    "        #     worksheet.merge_range(start_row, 0, row - 1, 0, last_value)\n",
    "    \n",
    "        \n",
    "\n",
    "print(\"Cleaned tables saved with merged spanning rows!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fund_dict_cleaned = {\n",
    "#     'LIC MF LARGE CAP FUND': 'LARGE CAP',\n",
    "#     'LIC MF LARGE & MID CAP FUND': 'LARGE& MID CAP',\n",
    "#     'LIC MF MULTICAP FUND': 'MULTICAP',\n",
    "#     'LIC MF MID CAP FUND': \"V't CAP\",\n",
    "#     'LIC MF SMALLCAP FUND': 'SMALLCAP',\n",
    "#     'LIC MF DIVIDEND YIELD FUND': 'DIV_DEND YIELD',\n",
    "#     'LIC MF VALUE FUND': 'VALUE',\n",
    "#     'LIC MF FOCUSED FUND': 'FOCUSED',\n",
    "#     'LIC MF INFRASTRUCTURE FUND': 'INFRASTRUCTURE',\n",
    "#     'LIC MF MANUFACTURING FUND': 'Poin MANGFACTURING',\n",
    "#     'LIC MF BANKING & FINANCIAL SERVICES FUND': 'BANKING & FINANC-AL SERVICES',\n",
    "#     'LIC MF HEALTHCARE FUND': 'HEALTHCARE',\n",
    "#     'LIC MF ELSS TAX SAVER': 'EL_SS TAX SAVER',\n",
    "#     'LIC MF AGGRESSIVE HYBID FUND': 'AGGRESSIVE HYBRiD',\n",
    "#     'LIC MF BALANCED ADVANTAGE FUND': 'Bâ€™L*NCED ADVANTAGE',\n",
    "#     'LIC MF EQUITY SAVINGS FUND': 'EQUITY SAVINGS',\n",
    "#     'LIC MF CONSERVATIVE HYBRID FUND': 'CONSERWATIVE nip?!)',\n",
    "#     'LIC MF ARBITRAGE FUND': 'ARBITRAGE',\n",
    "#     'LIC MF OVERNIGHT FUND': 'OVERNIGHT',\n",
    "#     'LIC MF LIQUID FUND': 'LIQUID',\n",
    "#     'LIC MF ULTRA SHORT DURATION FUND': 'ULTRA SHORT DURATION',\n",
    "#     'LIC MF LOW DURATION FUND': \"LOWâ€™ DURATION\",\n",
    "#     'LIC MF MEDIUM TO LONG DURATION FUND': 'MEDIUM-F2Â°LONG DURATION',\n",
    "#     'LIC MF BANKING & PSU FUND': 'BANK&ONG & PSU',\n",
    "#     'LIC MF SHORT DURATION FUND': '_. SHORT DURATION',\n",
    "#     'LIC MF GILT FUND': 'MGI',\n",
    "#     'LIC MF CHILDRENS FUND': 'l HILDRENS',\n",
    "#     'LIC MF BSE SENSEX ETF': 'BSE SENSEX ETF',\n",
    "#     'LIC MF NIFTY 50 ETF': 'NIFTY 50ETF',\n",
    "#     'LIC MF NIFTY 100 ETF': 'NIFTY 100 ETF',\n",
    "#     'LIC MF NIFTY MIDCAP 100 ETF': 'NIFTY MIDCAP 100 ETF',\n",
    "#     'LIC MF NIFTY 8-13 YR G-SEC ETF': 'NIFTY 8-13 YR G-SECETF',\n",
    "#     'LIC MF BSE SENSEX INDEX FUND': 'BSE SENSEX INDEX',\n",
    "#     'LIC MF NIFTY 50 INDEX FUND': 'NIFTY 50 INDEX',\n",
    "#     'LIC MF NIFTY NEXT 50 INDEX FUND': 'NIFTY NEXT 50 INDEX',\n",
    "#     'LIC MF GOLD EXCHANGE TRADED FUND': 'G2LD EXCHANGE TRADED',\n",
    "#     'LIC MF GOLD ETF FUND OF FUND': 'GSLD ETF',\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fund_dict_cleaned = {\n",
    "    'LIC MF LARGE CAP FUND': 'LARGE CAP',\n",
    "    'LIC MF LARGE & MID CAP FUND': 'LARGE.*?MID CAP',\n",
    "    'LIC MF MULTICAP FUND': 'MULTICAP',\n",
    "    'LIC MF MID CAP FUND': \"V't CAP\",\n",
    "    'LIC MF SMALLCAP FUND': 'SMALLCAP',\n",
    "    'LIC MF DIVIDEND YIELD FUND': 'DI.*?END YIELD',\n",
    "    'LIC MF VALUE FUND': 'VALUE FUND',\n",
    "    'LIC MF FOCUSED FUND': 'FOCUSED FUND',\n",
    "    'LIC MF INFRASTRUCTURE FUND': 'INFRASTRUCTURE',\n",
    "    'LIC MF MANUFACTURING FUND': 'MANGFACTURING',\n",
    "    'LIC MF BANKING & FINANCIAL SERVICES FUND': 'BANKING.*?FINAN.*?SERVICES',\n",
    "    'LIC MF HEALTHCARE FUND': 'HEALTHCARE',\n",
    "    'LIC MF ELSS TAX SAVER': 'EL.*?TAX SAVER',\n",
    "    'LIC MF AGGRESSIVE HYBID FUND': 'AGGRESSIVE HYBRiD',\n",
    "    'LIC MF BALANCED ADVANTAGE FUND': 'B.*?NCED ADVANTAGE',\n",
    "    'LIC MF EQUITY SAVINGS FUND': 'EQUITY SAVINGS',\n",
    "    'LIC MF CONSERVATIVE HYBRID FUND': 'CONSERWATIVE.*nip?!)',\n",
    "    'LIC MF ARBITRAGE FUND': 'ARBITRAGE FUND',\n",
    "    'LIC MF OVERNIGHT FUND': 'OVERNIGHT',\n",
    "    '':'LIC MF MULTI.*?CATION FUND',\n",
    "    'LIC MF LIQUID FUND': 'LIQUID FUND',\n",
    "    'LIC MF ULTRA SHORT DURATION FUND': 'ULTRA SHORT DURATION',\n",
    "    'LIC MF LOW DURATION FUND': \"LO.*?DURATION\",\n",
    "    'LIC MF MEDIUM TO LONG DURATION FUND': 'MEDIUM.*?LONG DURATION',\n",
    "    'LIC MF BANKING & PSU FUND': 'BANK.*?PSU',\n",
    "    'LIC MF SHORT DURATION FUND': 'SHORT DURATION',\n",
    "    'LIC MF GILT FUND': 'MGI',\n",
    "    'LIC MF CHILDRENS FUND': '.*?HILDRE.*?FUND',\n",
    "    'LIC MF BSE SENSEX ETF': 'BSE SENSEX ETF',\n",
    "    'LIC MF NIFTY 50 ETF': 'NIFTY\\\\s*50\\\\s*ETF',\n",
    "    'LIC MF NIFTY 100 ETF': 'NIFTY 100 ETF',\n",
    "    'LIC MF NIFTY MIDCAP 100 ETF': 'NIFTY MIDCAP 100 ETF',\n",
    "    'LIC MF NIFTY 8-13 YR G-SEC ETF': 'NIFTY 8-13 YR G-SEC\\\\s*ETF',\n",
    "    'LIC MF BSE SENSEX INDEX FUND': 'BSE SENSEX INDEX',\n",
    "    'LIC MF NIFTY 50 INDEX FUND': 'NIFTY 50 INDEX',\n",
    "    'LIC MF NIFTY NEXT 50 INDEX FUND': 'NIFTY NEXT 50 INDEX',\n",
    "    'LIC MF GOLD EXCHANGE TRADED FUND': 'G2LD\\\\s*EXCHANGE\\\\s*TRADED',\n",
    "    'LIC MF GOLD ETF FUND OF FUND': 'GSLD\\s*ETF',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NIPPON DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def via_block(path:str):\n",
    "    pattern = r\"FUNDS AT A GLANCE\"\n",
    "    amc_pattern = \"^(Nippon India|CPSE).*(?=Plan|Next 50|Sensex|Fund|Path|ETF|FOF|EOF|Funds|$)\"\n",
    "    imp_pages = []\n",
    "    with fitz.open(path) as doc:\n",
    "        for pgn, page in enumerate(doc):\n",
    "                page_blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "                sorted_blocks = sorted(page_blocks, key=lambda x: (x[\"bbox\"][1], x[\"bbox\"][0]))\n",
    "                for block_count, block in enumerate(sorted_blocks[:10]):\n",
    "                    if \"lines\" not in block:\n",
    "                        continue\n",
    "                    for line in block[\"lines\"]:\n",
    "                        for span in line[\"spans\"]:\n",
    "                            text = span[\"text\"].strip()\n",
    "                            if re.match(pattern,text):\n",
    "                                imp_pages.append(pgn)\n",
    "                                \n",
    "        amc_fund = defaultdict(list)\n",
    "    \n",
    "        for pgn in imp_pages:\n",
    "            page = doc[pgn]\n",
    "            page_blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "            sorted_blocks = sorted(page_blocks, key=lambda x: (x[\"bbox\"][1], x[\"bbox\"][0]))\n",
    "            for block_count, block in enumerate(sorted_blocks):\n",
    "                if \"lines\" not in block:\n",
    "                    continue\n",
    "                for line in block[\"lines\"]:\n",
    "                    for span in line[\"spans\"]:\n",
    "                        text = span[\"text\"].strip()\n",
    "                        color = span['color']\n",
    "                        if re.match(amc_pattern,text)and color == -1:\n",
    "                            # matches = re.findall(amc_pattern,text)\n",
    "                            amc_fund[pgn].append(text)\n",
    "                            \n",
    "    return imp_pages, dict(amc_fund)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages,amc = via_block(sample_path)\n",
    "pages = list(map(str,[x+1 for x in pages]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_scheme = defaultdict(list)\n",
    "for key, value in amc.items():\n",
    "    # print(key)\n",
    "    set1 = ['Scheme Name']+value[:4]\n",
    "    set2 = ['Scheme Name']+value[4:]\n",
    "    final_scheme[key+1].append(set1)\n",
    "    final_scheme[key+1].append(set2)\n",
    "\n",
    "final_scheme = dict(final_scheme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_pages = \",\".join(pages)\n",
    "tables = camelot.read_pdf(sample_path,pages=imp_pages, flavor=\"lattice\", line_scale = 40)  #table_areas = [\"0,0,580,690\"]            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "with pd.ExcelWriter(\"merged_tables.xlsx\", engine=\"openpyxl\") as writer:\n",
    "    count = 0  # Toggle between 0 and 1\n",
    "    \n",
    "    for i, table in enumerate(tables):\n",
    "        df = table.df\n",
    "        if df.shape[1] < 3:\n",
    "            continue\n",
    "\n",
    "    \n",
    "        df = df.map(lambda x: \" \".join(x.split(\"\\n\")).strip())\n",
    "        df = df.map(lambda x: np.nan if not x.strip() else x)\n",
    "        df.set_index(df.columns[0], inplace=True)\n",
    "\n",
    "        for check in [\"Scheme Name\", \"Market Capitalization\"]:\n",
    "            if check in df.index:\n",
    "                df.drop(check, inplace=True)\n",
    "                \n",
    "        df_cleaned = df[~df.index.isna()]\n",
    "        df_cleaned = df_cleaned[df_cleaned.index != \"\"]\n",
    "        df_cleaned = df_cleaned.reset_index()\n",
    "        df_fill = df_cleaned.ffill(axis=1)\n",
    "\n",
    "\n",
    "        sch_vals = final_scheme[table.page][count]\n",
    "        count = 1 - count  # Toggle between 0 and 1\n",
    "\n",
    "        if len(sch_vals) == 5:\n",
    "            df_fill.loc[-1] = sch_vals \n",
    "            df_fill = df_fill.sort_index().reset_index(drop=True)\n",
    "\n",
    "        # Write to a new sheet\n",
    "        df_fill.to_excel(writer, sheet_name=f\"Table_{i+1}\", index=False)\n",
    "\n",
    "print(\"All tables saved in separate sheets in 'merged_tables.xlsx' ðŸš€\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hello =  {\n",
    "    \"number\": 0,\n",
    "    \"type\": 0,\n",
    "    \"bbox\": (0,0,0,0), #406.72119140625, 439.4930419921875, 565.697265625, 484.5830383300781\n",
    "    \"lines\": [\n",
    "        {\n",
    "            \"spans\": [\n",
    "                {\n",
    "                    \"size\": 30.0,\n",
    "                    \"flags\": 20,\n",
    "                    \"font\": \"Montserrat-Regular\", #set this\n",
    "                    \"color\": -1, #set this\n",
    "                    \"ascender\": 1.0429999828338623,\n",
    "                    \"descender\": -0.2619999945163727,\n",
    "                    \"text\": \"DUMMYDUMMYDUMMYDUMMY\",\n",
    "                    \"origin\": (406.72119140625, 458.26702880859375),\n",
    "                    \"bbox\": (0,0,0,0), #406.72119140625,439.4930419921875,565.697265625,462.9830322265625,\n",
    "                }\n",
    "            ],\n",
    "            \"wmode\": 0,\n",
    "            \"dir\": (1.0, 0.0),\n",
    "            \"bbox\": (0,0,0,0), #406.72119140625,439.4930419921875,565.697265625,462.9830322265625,\n",
    "        },\n",
    "        \n",
    "    ],\n",
    "},  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envPDF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
