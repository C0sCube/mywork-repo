{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from samco import Samco\n",
    "from samco import Tata\n",
    "from helper import Helper\n",
    "import pprint, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fitz\n",
    "\n",
    "dir_path = r\"C:\\Users\\Kaustubh.keny\\OneDrive - Cogencis Information Services Ltd\\Documents\\mywork-repo\"\n",
    "#dir_path = r\"C:\\Users\\rando\\OneDrive\\Documents\\mywork-repo\"\n",
    "\n",
    "dry_path = r'\\output\\DryRun.pdf'\n",
    "fin_path = r'\\files\\financial_indices.xlsx'\n",
    "fund_path = r\"C:\\Users\\Kaustubh.keny\\OneDrive - Cogencis Information Services Ltd\\Documents\\Dec 24\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutual_fund = Helper.get_fund_paths(fund_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Doc saved at: C:\\Users\\Kaustubh.keny\\OneDrive - Cogencis Information Services Ltd\\Documents\\mywork-repo\\files\\pdf_report.xlsx\n",
      "\n",
      "Pages to extract: [3, 5, 7, 9, 11, 13, 15, 17, 18]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" SAMCO PDF FILE MAIN CODE\"\"\"\n",
    "\n",
    "object = Samco(dir_path, dry_path, fin_path)\n",
    "samco_path = mutual_fund['Samco Mutual Fund']\n",
    "\n",
    "fund_data = object.fund_data\n",
    "path, imp, fund_titles = object.check_and_highlight(samco_path, fund_data)\n",
    "\n",
    "sample = input(\"Do you want to proceed (Y/N)?: \")\n",
    "\n",
    "pages =  [3, 5, 7, 9, 11, 13, 15, 17, 18]\n",
    "bbox = object.content_bbox\n",
    "data_cond = object.data_cond\n",
    "\n",
    "data = object.get_clipped_data(samco_path,pages, bbox, fund_titles)\n",
    "data = object.extract_span_data(data,[])\n",
    "clean_data = object.process_text_data(data, data_cond)\n",
    "nested_data, matrix = object.create_nested_dict(clean_data,20.0, 10.0)\n",
    "extracted_text = object.get_generated_content(nested_data, object.DRYPATH)\n",
    "\n",
    "#CREATE FINAL CONTENT    \n",
    "final_text = dict()\n",
    "for fund, items in extracted_text.items():\n",
    "    \n",
    "    content_dict = dict()\n",
    "    for header, content in items.items():\n",
    "        header_content = object.match_regex_to_content(header, content)\n",
    "        content_dict.update(header_content)\n",
    "\n",
    "    final_text[fund] = content_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Doc saved at: C:\\Users\\Kaustubh.keny\\OneDrive - Cogencis Information Services Ltd\\Documents\\mywork-repo\\files\\pdf_report.xlsx\n",
      "\n",
      "Pages to extract: [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tata' object has no attribute 'data_cond'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m pages \u001b[38;5;241m=\u001b[39m  [\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m17\u001b[39m, \u001b[38;5;241m18\u001b[39m, \u001b[38;5;241m19\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m21\u001b[39m, \u001b[38;5;241m22\u001b[39m, \u001b[38;5;241m23\u001b[39m, \u001b[38;5;241m24\u001b[39m, \u001b[38;5;241m25\u001b[39m, \u001b[38;5;241m26\u001b[39m, \u001b[38;5;241m27\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m29\u001b[39m, \u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m31\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m33\u001b[39m, \u001b[38;5;241m34\u001b[39m, \u001b[38;5;241m35\u001b[39m, \u001b[38;5;241m36\u001b[39m, \u001b[38;5;241m37\u001b[39m, \u001b[38;5;241m38\u001b[39m, \u001b[38;5;241m39\u001b[39m, \u001b[38;5;241m40\u001b[39m, \u001b[38;5;241m41\u001b[39m, \u001b[38;5;241m42\u001b[39m, \u001b[38;5;241m43\u001b[39m, \u001b[38;5;241m44\u001b[39m, \u001b[38;5;241m45\u001b[39m, \u001b[38;5;241m46\u001b[39m, \u001b[38;5;241m47\u001b[39m, \u001b[38;5;241m48\u001b[39m, \u001b[38;5;241m49\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m51\u001b[39m, \u001b[38;5;241m52\u001b[39m, \u001b[38;5;241m53\u001b[39m, \u001b[38;5;241m54\u001b[39m, \u001b[38;5;241m55\u001b[39m, \u001b[38;5;241m56\u001b[39m, \u001b[38;5;241m57\u001b[39m, \u001b[38;5;241m58\u001b[39m, \u001b[38;5;241m59\u001b[39m, \u001b[38;5;241m60\u001b[39m, \u001b[38;5;241m62\u001b[39m, \u001b[38;5;241m63\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m66\u001b[39m, \u001b[38;5;241m67\u001b[39m, \u001b[38;5;241m68\u001b[39m, \u001b[38;5;241m69\u001b[39m, \u001b[38;5;241m70\u001b[39m, \u001b[38;5;241m71\u001b[39m, \u001b[38;5;241m72\u001b[39m, \u001b[38;5;241m73\u001b[39m, \u001b[38;5;241m74\u001b[39m, \u001b[38;5;241m75\u001b[39m, \u001b[38;5;241m76\u001b[39m, \u001b[38;5;241m77\u001b[39m, \u001b[38;5;241m78\u001b[39m, \u001b[38;5;241m79\u001b[39m, \u001b[38;5;241m80\u001b[39m, \u001b[38;5;241m81\u001b[39m, \u001b[38;5;241m82\u001b[39m, \u001b[38;5;241m83\u001b[39m]\n\u001b[0;32m     12\u001b[0m bbox \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39mcontent_bbox\n\u001b[1;32m---> 13\u001b[0m data_cond \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_cond\u001b[49m\n\u001b[0;32m     15\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39mget_clipped_data(file_path,pages, bbox, fund_titles)\n\u001b[0;32m     16\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39mextract_span_data(data,[])\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tata' object has no attribute 'data_cond'"
     ]
    }
   ],
   "source": [
    "\"\"\" TATA PDF FILE MAIN CODE\"\"\"\n",
    "\n",
    "object = Tata(dir_path, dry_path, fin_path)\n",
    "file_path = mutual_fund['Tata Mutual Fund']\n",
    "\n",
    "fund_data = object.fund_data\n",
    "path, imp, fund_titles = object.check_and_highlight(file_path, fund_data)\n",
    "\n",
    "sample = input(\"Do you want to proceed (Y/N)?: \")\n",
    "\n",
    "pages =  [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 62, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83]\n",
    "bbox = object.content_bbox\n",
    "data_cond = object.data_conditions\n",
    "\n",
    "data = object.get_clipped_data(file_path,pages, bbox, fund_titles)\n",
    "data = object.extract_span_data(data,[])\n",
    "clean_data = object.process_text_data(data, data_cond)\n",
    "nested_data, matrix = object.create_nested_dict(clean_data,20.0, 10.0)\n",
    "extracted_text = object.get_generated_content(nested_data, object.DRYPATH)\n",
    "\n",
    "#CREATE FINAL CONTENT    \n",
    "final_text = dict()\n",
    "for fund, items in extracted_text.items():\n",
    "    \n",
    "    content_dict = dict()\n",
    "    for header, content in items.items():\n",
    "        header_content = object.match_regex_to_content(header, content)\n",
    "        content_dict.update(header_content)\n",
    "\n",
    "    final_text[fund] = content_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Doc saved at C:\\Users\\Kaustubh.keny\\OneDrive - Cogencis Information Services Ltd\\Documents\\mywork-repo\\output\\pdf_report.xlsx\n",
      "360 one focused equity fund\n",
      " pdf generated to: C:\\Users\\Kaustubh.keny\\OneDrive - Cogencis Information Services Ltd\\Documents\\mywork-repo\\output\\DryRun.pdf\n",
      "360 one flexicap fund\n",
      " pdf generated to: C:\\Users\\Kaustubh.keny\\OneDrive - Cogencis Information Services Ltd\\Documents\\mywork-repo\\output\\DryRun.pdf\n",
      "360 one quant fund\n",
      " pdf generated to: C:\\Users\\Kaustubh.keny\\OneDrive - Cogencis Information Services Ltd\\Documents\\mywork-repo\\output\\DryRun.pdf\n",
      "360 one elss tax saver nifty 50 index fund\n",
      " pdf generated to: C:\\Users\\Kaustubh.keny\\OneDrive - Cogencis Information Services Ltd\\Documents\\mywork-repo\\output\\DryRun.pdf\n",
      "360 one balanced hybrid fund\n",
      " pdf generated to: C:\\Users\\Kaustubh.keny\\OneDrive - Cogencis Information Services Ltd\\Documents\\mywork-repo\\output\\DryRun.pdf\n",
      "360 one dynamic bond fund\n",
      " pdf generated to: C:\\Users\\Kaustubh.keny\\OneDrive - Cogencis Information Services Ltd\\Documents\\mywork-repo\\output\\DryRun.pdf\n"
     ]
    }
   ],
   "source": [
    "\"\"\"ASSET ONE 360 FILE MAIN CODE\"\"\"\n",
    "\n",
    "from asset360 import One360\n",
    "\n",
    "\n",
    "_reader = Reader(dir_path)\n",
    "file_path = _reader.get_file_path(r'\\files\\one360.pdf')\n",
    "dry_path  = _reader.get_file_path(r'\\output\\DryRun.pdf')\n",
    "fin_path  = _reader.get_file_path(r'\\files\\financial_indices.xlsx')\n",
    "json_path = _reader.get_file_path(r'\\output\\dump360.json')\n",
    "\n",
    "_one360 = One360()\n",
    "\n",
    "financial_indices = _reader.get_financial_indices(fin_path)\n",
    "fund_data = _one360.fund_data\n",
    "bbox = [[0,50,160,900]]\n",
    "\n",
    "path,fund_names,imp_pages = _reader.check_and_highlight(file_path, financial_indices,fund_data)\n",
    "_reader.save_pdf_data(imp_pages, fund_names)\n",
    "\n",
    "#user_input = input(\"Enter the desired pages: \")\n",
    "pages = [5,6,7,8,9,10,11]\n",
    "\n",
    "data = _reader.get_clipped_data(file_path,pages,bbox, fund_names)\n",
    "extract_data = _reader.extract_span_data(data,[])\n",
    "data_conditions = [[8.0],-10791002,20.0]\n",
    "cleaned_data = _reader.process_text_data(extract_data, data_conditions)\n",
    "nested_data, matrix = _reader.create_nested_dict(cleaned_data, 20.0, 10.0)\n",
    "\n",
    "\n",
    "extracted_text = dict() #hv to write a function for this !!\n",
    "for fund, items in nested_data.items():\n",
    "    print(fund)\n",
    "    _reader.generate_pdf_from_data(items, dry_path)\n",
    "    extract_data = _reader.extract_data_from_pdf(dry_path)\n",
    "    extracted_text[fund] = extract_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" BAJAJ FINSERV FILE MAIN CODE\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = _reader.get_file_path(r'\\files\\bajaj finserv.pdf')\n",
    "doc = fitz.open(file_path)\n",
    "\n",
    "count = doc.page_count\n",
    "all_blocks = list()\n",
    "\n",
    "for pgn in range(count):\n",
    "    page = doc[pgn]\n",
    "    \n",
    "    blocks = page.get_text('dict')['blocks']\n",
    "    images = page.get_images()\n",
    "    filtered_blocks = [block for block in blocks if block['type']==0]\n",
    "    sorted_blocks = sorted(blocks, key=lambda x: (x['bbox'][1], x['bbox'][0]))\n",
    "    all_blocks.append({\n",
    "        \"pgn\":pgn,\n",
    "        \"blocks\":sorted_blocks,\n",
    "        \"images\": images\n",
    "    })\n",
    "    \n",
    "    #draw lines\n",
    "    \n",
    "    lines = fitz.Rect()\n",
    "    \n",
    "doc.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(465, 464, 824, 359, 8, 'Indexed', '', 'X140', 'FlateDecode')]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_blocks[17]['images']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for page_number, page in enumerate(doc, start=0):\n",
    "    print(f\"\\n_____________{page_number}______________\")\n",
    "    images = page.get_images(full=True)  # Get all images on the page\n",
    "    for img_index, img in enumerate(images, start=1):\n",
    "        print(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_blocks[17]['blocks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
