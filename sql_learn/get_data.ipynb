{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1818d93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "import json, os, re\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import re\n",
    "fund_names = [\"360 ONE Mutual Fund\", \"Aditya Birla Sun Life Mutual Fund\", \"Axis Mutual Fund\", \"Angel One Mutual Fund\",\n",
    "              \"BajaJ Finserv Mutual Fund\", \"Bandhan Mutual Fund\", \"Bank of India Mutual Fund\", \n",
    "              \"Baroda BNP Paribas Mutual Fund\", \"Canara Robeco Mutual Fund\", \"DSP Mutual Fund\", \n",
    "              \"Edelweiss Mutual Fund\", \"Franklin Templeton Mutual Fund\", \"Groww Mutual Fund\", \n",
    "              \"HDFC Mutual Fund\", \"Helios Mutual Fund\", \"HSBC Mutual Fund\", \"ICICI Prudential Mutual Fund\",\n",
    "              \"Invesco Mutual Fund\", \"ITI Mutual Fund\", \"JM Financial Mutual Fund\", \"Kotak Mahindra Mutual Fund\",\n",
    "              \"PGIM India Mutual Fund\", \"LIC Mutual Fund\", \"Mahindra Manulife Mutual Fund\", \"Mirae Asset Mutual Fund\", \n",
    "              \"Motilal Oswal Mutual Fund\", \"Navi Mutual Fund\", \"Nippon India Mutual Fund\", \"NJ Mutual Fund\", \n",
    "              \"Old Bridge Mutual Fund\", \"PPFAS Mutual Fund\", \"Quantum Mutual Fund\", \"Quant Mutual Fund\", \n",
    "              \"Samco Mutual Fund\", \"SBI Mutual Fund\", \"Shriram Mutual fund\", \"Sundaram Mutual Fund\", \"Tata Mutual Fund\", \n",
    "              \"Taurus Mutual Fund\", \"Trust Mutual Fund\", \"Union Mutual Fund\", \"UTI Mutual Fund\", \"WhiteOak Mutual Fund\", \"Zerodha Mutual Fund\"]\n",
    "\n",
    "column_mapping = {\n",
    "    \"id\": \"id\",\n",
    "    \"entered_time\": \"entered_date\",\n",
    "    \"amc_for_month\": \"amc_for_month\",\n",
    "    \"data_from\": \"data_from\",\n",
    "    \"amc_name\": \"amc_name\",\n",
    "    \"main_scheme_name\": \"main_scheme_name\",\n",
    "    \"fund_name\": \"fund_name\",\n",
    "    \"benchmark_index\": \"benchmark_index\",\n",
    "    \"monthly_aaum_date\": \"monthly_aaum_date\",\n",
    "    \"monthly_aaum_value\": \"monthly_aaum_value\",\n",
    "    \"scheme_launch_date\": \"scheme_launch_date\",\n",
    "    \"min_addl_amt\": \"min_addl_amt\",\n",
    "    \"min_addl_amt_multiple\": \"min_addl_amt_multiple\",\n",
    "    \"min_amt\": \"min_amt\",\n",
    "    \"min_amt_multiple\": \"min_amt_multiple\",\n",
    "    \"entry_load\": \"entry_load\",\n",
    "    \"exit_load\": \"exit_load\",\n",
    "    \"alpha\": \"alpha\",\n",
    "    \"arithmetic_mean_ratio\": \"arithmetic_mean_ratio\",\n",
    "    \"average_div_yld\": \"average_div_yield\",\n",
    "    \"average_pb\": \"average_pb\",\n",
    "    \"average_pe\": \"average_pe\",\n",
    "    \"avg_maturity\": \"avg_maturity\",\n",
    "    \"beta\": \"beta\",\n",
    "    \"correlation_ratio\": \"correlation_ratio\",\n",
    "    \"downside_deviation\": \"downside_deviation\",\n",
    "    \"information_ratio\": \"information_ratio\",\n",
    "    \"macaulay\": \"macaulay\",\n",
    "    \"mod_duration\": \"mod_duration\",\n",
    "    \"port_turnover_ratio\": \"port_turnover_ratio\",\n",
    "    \"r_squared_ratio\": \"r_squared_ratio\",\n",
    "    \"roe_ratio\": \"roe_ratio\",\n",
    "    \"sharpe\": \"sharpe\",\n",
    "    \"sortino_ratio\": \"sortino_ratio\",\n",
    "    \"std_dev\": \"std_dev\",\n",
    "    \"tracking_error\": \"tracking_error\",\n",
    "    \"treynor_ratio\": \"treynor_ratio\",\n",
    "    \"upside_deviation\": \"upside_deviation\",\n",
    "    \"ytm\": \"ytm\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89b9e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mydata\n",
    "conn = mysql.connector.connect(\n",
    "    host=\"172.22.225.155\",\n",
    "    user=\"cog_mf\",\n",
    "    password=\"bnYwFChjLAV2Z%9E\",\n",
    "    database=\"cog_mf\",\n",
    "    port=3306,\n",
    "    charset='utf8mb4'\n",
    ")\n",
    "\n",
    "# conn = mysql.connector.connect(\n",
    "#     host=\"localhost\",   \n",
    "#     user=\"root\",            \n",
    "#     password=\"1234\", \n",
    "#     database=\"cog_updated_db\",  \n",
    "#     port=3306,               \n",
    "#     charset=\"utf8mb4\"\n",
    "# )\n",
    "\n",
    "# conn = mysql.connector.connect(\n",
    "#     host=\"localhost\",   \n",
    "#     user=\"root\",            \n",
    "#     password=\"1234\", \n",
    "#     database=\"data_db\",  \n",
    "#     port=3306,               \n",
    "#     charset=\"utf8mb4\"\n",
    "# )\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "output_path = f\"feb_self_{timestamp}.xlsx\"\n",
    "data_from_value = \"my_data\"\n",
    "\n",
    "with pd.ExcelWriter(output_path, engine=\"xlsxwriter\") as writer:\n",
    "    queries_dict = {}\n",
    "    workbook = writer.book\n",
    "    header_format = workbook.add_format({\n",
    "        'bold': True,\n",
    "        'text_wrap': True,\n",
    "        'valign': 'top',\n",
    "        'fg_color': '#D7E4BC',\n",
    "        'border': 1\n",
    "    })\n",
    "    \n",
    "    for fund in fund_names:\n",
    "        search_fund = re.sub(r'mutual fund', '', fund, flags=re.IGNORECASE).strip()\n",
    "        query = \"\"\"\n",
    "        SELECT *\n",
    "        FROM mf_keny_alldata\n",
    "        WHERE amc_name LIKE %s\n",
    "        AND amc_for_month REGEXP 'FEB25'\n",
    "        AND entered_time LIKE \"%2025-05-09%\";\n",
    "        \"\"\"\n",
    "        # AND (monthly_aaum_date REGEXP \"202503|March 2025\" OR monthly_aaum_date IS NULL)\n",
    "        # AND data_from REGEXP \"mydata\"\n",
    "        \n",
    "        # query = \"\"\"\n",
    "        # SELECT *\n",
    "        # FROM mf_common_fund_manager_data\n",
    "        # WHERE amc_name LIKE %s\"\"\"\n",
    "        \n",
    "        # queries_dict[sheet_name] = (\n",
    "        #     fund,\n",
    "        #     query.strip(),\n",
    "        #     f\"LIKE: %{search_fund}%,\"\n",
    "        # )\n",
    "        \n",
    "        df = pd.read_sql(query, conn, params=(f\"%{search_fund}%\",))\n",
    "\n",
    "        if df.empty:\n",
    "            print(f\"Skipped (no data): {fund}\")\n",
    "            continue\n",
    "        \n",
    "        df.drop_duplicates(inplace=True)\n",
    "        # df.drop(columns=[\"id\",\"amc_id\",\"fund_id\",\"entered_time\",\"amc_name\",\"fund_name\"], axis=1, inplace=True)\n",
    "        \n",
    "        df.insert(0, 'data_from', data_from_value)\n",
    "        cols = df.columns.tolist()\n",
    "        \n",
    "        sheet_name = \"_\".join(search_fund.lower().split())[:31]\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "        worksheet = writer.sheets[sheet_name]\n",
    "\n",
    "        for col_num, header in enumerate(df.columns.values):\n",
    "            worksheet.write(0, col_num, header, header_format)\n",
    "            worksheet.set_column(col_num, col_num, min(len(header) + 8, 50))\n",
    "\n",
    "        #add query content to excel sheet\n",
    "        # query_rows = [\n",
    "        # {\"Fund\": fund, \"Query\": q, \"Params\": p}\n",
    "        # for sheet, (fund, q, p) in queries_dict.items()\n",
    "        # ]\n",
    "        # queries_df = pd.DataFrame(query_rows)\n",
    "        # queries_df.to_excel(writer, sheet_name=\"Queries\", index=False)\n",
    "\n",
    "        # query_ws = writer.sheets[\"Queries\"]\n",
    "        # for col_num, header in enumerate(queries_df.columns):\n",
    "        #     query_ws.write(0, col_num, header, header_format)\n",
    "        #     query_ws.set_column(col_num, col_num, 50)\n",
    "        \n",
    "print(f\"Done! Exported to: {output_path}\")\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fd6c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finklivedb\n",
    "# query = \"\"\"\n",
    "#             SELECT *\n",
    "#             FROM mf_json_common_details\n",
    "#             WHERE (monthly_aaum_date REGEXP '2025|FEBRU' OR monthly_aaum_date IS NULL)\n",
    "#               AND fund_name LIKE %s\n",
    "#         \"\"\"\n",
    "\n",
    "conn = mysql.connector.connect(\n",
    "    host=\"172.22.225.155\",\n",
    "    user=\"cog_mf\",\n",
    "    password=\"bnYwFChjLAV2Z%9E\",\n",
    "    database=\"cog_mf\",\n",
    "    port=3306,\n",
    "    charset='utf8mb4'\n",
    ")\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "output_path = f\"jan_live_{timestamp}.xlsx\"\n",
    "data_from_value = \"cog_mf\"\n",
    "aaum_date = '202501|January 2025'\n",
    "\n",
    "with pd.ExcelWriter(output_path, engine=\"xlsxwriter\") as writer:\n",
    "    queries_dict = {}\n",
    "    workbook = writer.book\n",
    "    header_format = workbook.add_format({\n",
    "        'bold': True,\n",
    "        'text_wrap': True,\n",
    "        'valign': 'top',\n",
    "        'fg_color': '#D7E4BC',\n",
    "        'border': 1\n",
    "    })\n",
    "    \n",
    "    for fund in fund_names:\n",
    "        search_fund = re.sub(r'mutual fund', '', fund, flags=re.IGNORECASE).strip()\n",
    "        query = \"\"\"\n",
    "            SELECT *\n",
    "            FROM mf_json_common_details d\n",
    "            WHERE d.fund_name LIKE %s\n",
    "            AND d.monthly_aaum_date REGEXP %s\n",
    "        \"\"\"\n",
    "        #  SELECT \n",
    "        #     d.*,\n",
    "        #     MAX(COALESCE(d.benchmark_index, bi.benchmark_index)) AS final_benchmark_index,\n",
    "        #     MAX(bi.riskometer_benchmark) AS riskometer_benchmark,\n",
    "        #     MAX(CASE WHEN l.type = 'Entry' THEN l.comment END) AS entry_load,\n",
    "        #     MAX(CASE WHEN l.type = 'Exit' THEN l.comment END) AS exit_load\n",
    "        # FROM mf_json_common_details d\n",
    "        # LEFT JOIN mf_json_loads l\n",
    "        # ON d.MainScheme_ID = l.MainScheme_ID\n",
    "        # AND d.document_detail_id = l.document_detail_id\n",
    "        # LEFT JOIN mf_json_benchmark_indices bi\n",
    "        # ON d.MainScheme_ID = bi.MainScheme_ID\n",
    "        # AND d.document_detail_id = bi.document_detail_id\n",
    "        # WHERE d.fund_name LIKE %s\n",
    "        # AND d.monthly_aaum_date REGEXP %s\n",
    "        # GROUP BY d.id;\n",
    "        \n",
    "        #  AND (d.monthly_aaum_date REGEXP '202503|MARCH' OR d.monthly_aaum_date IS NULL)\n",
    "        \n",
    "        # queries_dict[sheet_name] = (\n",
    "        #     fund,\n",
    "        #     query.strip(),\n",
    "        #     f\"LIKE: %{search_fund}%, REGEXP: {aaum_date}\"\n",
    "        # )\n",
    "        \n",
    "        \n",
    "        df = pd.read_sql(query, conn, params=(f\"%{search_fund}%\",aaum_date))\n",
    "\n",
    "        if df.empty:\n",
    "            print(f\"Skipped (no data): {fund}\")\n",
    "            continue\n",
    "        \n",
    "        #promote nonempty aaum_date\n",
    "        df = df.sort_values(by='monthly_aaum_date', key=lambda col: col.isnull())\n",
    "        \n",
    "        df.insert(0, 'data_from', data_from_value)\n",
    "        cols = df.columns.tolist()\n",
    "        \n",
    "        # Move 'main_scheme_name' to second column position\n",
    "        # if 'main_scheme_name' in cols:\n",
    "        #     cols.remove('main_scheme_name')\n",
    "        #     cols.insert(1, 'main_scheme_name')\n",
    "        #     df = df[cols]\n",
    "\n",
    "        # df['benchmark_index'] = df['final_benchmark_index']\n",
    "        # df.drop('final_benchmark_index', axis = 1, inplace=True)\n",
    "        \n",
    "        sheet_name = \"_\".join(search_fund.lower().split())[:31]\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "        worksheet = writer.sheets[sheet_name]\n",
    "\n",
    "        for col_num, header in enumerate(df.columns.values):\n",
    "            worksheet.write(0, col_num, header, header_format)\n",
    "            worksheet.set_column(col_num, col_num, min(len(header) + 8, 50))\n",
    "\n",
    "            \n",
    "print(f\"Done! Exported to: {output_path}\")\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "997f39ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved At: mar_combined20250509_2230.xlsx\n"
     ]
    }
   ],
   "source": [
    "live_data = pd.read_excel(\"mar_live_20250509_1821.xlsx\", sheet_name=None)\n",
    "self_data = pd.read_excel(\"mar_self_20250509_1820.xlsx\", sheet_name=None)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "output_path = f\"mar_combined{timestamp}.xlsx\"\n",
    "\n",
    "with pd.ExcelWriter(output_path, engine=\"xlsxwriter\") as writer:\n",
    "    for sheet_name in live_data:\n",
    "        df_live = live_data[sheet_name].copy()\n",
    "        df_live[\"data_from\"] = \"livedb\"\n",
    "        \n",
    "        df_live.drop_duplicates(inplace=True)\n",
    "        \n",
    "        df_self = self_data.get(sheet_name, pd.DataFrame()).copy()\n",
    "        if not df_self.empty:\n",
    "            df_self[\"data_from\"] = \"mydata\"\n",
    "            \n",
    "            df_self.drop_duplicates(inplace=True)\n",
    "            \n",
    "            df_combined = pd.concat([df_self, df_live], axis=1, keys=[\"self\", \"live\"])\n",
    "            df_combined.columns = [f\"{i}_{j}\" for i, j in df_combined.columns] \n",
    "            # Align columns\n",
    "            # df_self = df_self.reindex(columns=df_live.columns, fill_value=\"\")\n",
    "            # df_combined = pd.concat([df_live, df_self], ignore_index=True)\n",
    "        else:\n",
    "            df_combined = df_live\n",
    "\n",
    "        df_combined.to_excel(writer, sheet_name=sheet_name[:31], index=False)\n",
    "\n",
    "print(f\"Saved At: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86775be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved at: mar_combine20250510_2058.xlsx\n"
     ]
    }
   ],
   "source": [
    "live_data = pd.read_excel(\"mar_live_20250509_1821.xlsx\", sheet_name=None)\n",
    "self_data = pd.read_excel(\"mar_self_20250509_1820.xlsx\", sheet_name=None)\n",
    "\n",
    "keys_to_drop = [\n",
    "    \"id\",\n",
    "    \"document_detail_id\",\n",
    "    \"MainScheme_ID\",\n",
    "    \"scheme_objective\",\n",
    "    \"suitable_for_investors\",\n",
    "    \"benchmark_index_id\",\n",
    "    \"information_ratio\",\n",
    "    \"sortino_ratio\",\n",
    "    \"arithmetic_mean_ratio\",\n",
    "    \"correlation_ratio\",\n",
    "    \"roe_ratio\",\n",
    "    \"downside_deviation\",\n",
    "    \"upside_deviation\",\n",
    "    \"price_per_unit\",\n",
    "    \"creation_unit_size\",\n",
    "    \"offer_price\",\n",
    "    \"min_target_amt\",\n",
    "    \"entered_user\",\n",
    "    \"entered_date\",\n",
    "    \"modified_user\",\n",
    "    \"modified_date\",\n",
    "    \"is_migrated\",\n",
    "    \"is_valid\",\n",
    "    \"close_date\",\n",
    "    \"open_date\",\n",
    "    \"face_value\",\n",
    "    \"type_of_scheme\",\n",
    "    \"Riskometer\",\n",
    "    \"Riskometer_benchmark\",\n",
    "    \"SchemeCategory_level2_ID\",\n",
    "    \"scheme_code\",\n",
    "    \"sai_weblink\",\n",
    "    \"nfo_min_sub_amt\"\n",
    "]\n",
    "\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "output_path = f\"mar_combine{timestamp}.xlsx\"\n",
    "\n",
    "with pd.ExcelWriter(output_path, engine=\"xlsxwriter\") as writer:\n",
    "    for sheet_name in live_data:\n",
    "        df_live = live_data[sheet_name].copy()\n",
    "        df_live[\"data_from\"] = \"livedb\"\n",
    "        df_live.drop_duplicates(inplace=True)\n",
    "        df_live = df_live.sort_values(by=\"main_scheme_name\")\n",
    "        df_self.drop(['amc_for_month', 'entered_time', 'entry_load','exit_load'], axis=1, inplace=True)\n",
    "        \n",
    "\n",
    "        df_self = self_data.get(sheet_name, pd.DataFrame()).copy()\n",
    "        if not df_self.empty:\n",
    "            df_self[\"data_from\"] = \"mydata\"\n",
    "            df_self.drop_duplicates(inplace=True)\n",
    "            df_self.drop(['amc_for_month', 'entered_time', 'entry_load','exit_load'], axis=1, inplace=True)\n",
    "            df_self = df_self.sort_values(by=\"main_scheme_name\")\n",
    "\n",
    "            df_combined = pd.concat([df_self, df_live], axis=1, keys=[\"self\", \"live\"])\n",
    "\n",
    "            # interleaved_cols = []\n",
    "            # for col in df_self.columns:\n",
    "            #     interleaved_cols.append((\"self\", col))\n",
    "            #     interleaved_cols.append((\"live\", col))\n",
    "\n",
    "            # df_combined = df_combined[interleaved_cols]\n",
    "            df_combined.columns = [f\"{i}_{j}\" for i, j in df_combined.columns]\n",
    "\n",
    "        else:\n",
    "            df_combined = df_live\n",
    "\n",
    "        df_combined.to_excel(writer, sheet_name=sheet_name[:31], index=False)\n",
    "\n",
    "print(f\"Saved at: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2d3f545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved at: aligned_comparison_20250511_1306.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def normalize_names(df):\n",
    "    df['main_scheme_name'] = df['main_scheme_name'].str.strip().str.lower()\n",
    "    return df\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "output_path = f\"aligned_comparison_{timestamp}.xlsx\"\n",
    "\n",
    "keys_to_drop = [\n",
    "    \"id\",\n",
    "    \"document_detail_id\",\n",
    "    \"MainScheme_ID\",\n",
    "    \"scheme_objective\",\n",
    "    \"suitable_for_investors\",\n",
    "    \"benchmark_index_id\",\n",
    "    \"information_ratio\",\n",
    "    \"sortino_ratio\",\n",
    "    \"arithmetic_mean_ratio\",\n",
    "    \"correlation_ratio\",\n",
    "    \"roe_ratio\",\n",
    "    \"downside_deviation\",\n",
    "    \"upside_deviation\",\n",
    "    \"price_per_unit\",\n",
    "    \"creation_unit_size\",\n",
    "    \"offer_price\",\n",
    "    \"min_target_amt\",\n",
    "    \"entered_user\",\n",
    "    \"entered_date\",\n",
    "    \"modified_user\",\n",
    "    \"modified_date\",\n",
    "    \"is_migrated\",\n",
    "    \"is_valid\",\n",
    "    \"close_date\",\n",
    "    \"open_date\",\n",
    "    \"face_value\",\n",
    "    \"type_of_scheme\",\n",
    "    \"Riskometer\",\n",
    "    \"Riskometer_benchmark\",\n",
    "    \"SchemeCategory_level2_ID\",\n",
    "    \"scheme_code\",\n",
    "    \"sai_weblink\",\n",
    "    \"nfo_min_sub_amt\"\n",
    "]\n",
    "\n",
    "live_data = pd.read_excel(\"feb_live_20250509_1823.xlsx\", sheet_name=None)\n",
    "self_data = pd.read_excel(\"feb_self_20250509_1825.xlsx\", sheet_name=None)\n",
    "\n",
    "with pd.ExcelWriter(output_path, engine=\"xlsxwriter\") as writer:\n",
    "    for sheet_name in live_data:\n",
    "        df_live = live_data[sheet_name].copy()\n",
    "        df_self = self_data.get(sheet_name, pd.DataFrame()).copy()\n",
    "\n",
    "        if not df_self.empty:\n",
    "            df_self = normalize_names(df_self)\n",
    "            df_live = normalize_names(df_live)\n",
    "\n",
    "            df_self['data_from'] = 'mydata'\n",
    "            df_live['data_from'] = 'livedb'\n",
    "\n",
    "            df_self = df_self.drop_duplicates().sort_values(by=\"main_scheme_name\")\n",
    "            df_live = df_live.drop_duplicates().sort_values(by=\"main_scheme_name\")\n",
    "\n",
    "            df_self.drop(['amc_for_month', 'entered_time', 'entry_load', 'exit_load'], axis=1, inplace=True, errors='ignore')\n",
    "            df_self.drop(keys_to_drop, axis=1, inplace=True, errors='ignore')\n",
    "            df_live.drop(keys_to_drop, axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "            # Remove duplicate main_scheme_name entries\n",
    "            df_self = df_self.drop_duplicates(subset=[\"main_scheme_name\"])\n",
    "            df_live = df_live.drop_duplicates(subset=[\"main_scheme_name\"])\n",
    "\n",
    "            all_names = pd.Series(\n",
    "                pd.concat([df_self[\"main_scheme_name\"], df_live[\"main_scheme_name\"]]).unique()\n",
    "            ).sort_values().reset_index(drop=True)\n",
    "\n",
    "            df_self_aligned = df_self.set_index(\"main_scheme_name\").reindex(all_names).reset_index()\n",
    "            df_live_aligned = df_live.set_index(\"main_scheme_name\").reindex(all_names).reset_index()\n",
    "\n",
    "            # Rename columns (handling main_scheme_name correctly)\n",
    "            df_self_aligned.columns = [f\"{col}_self\" if col != \"main_scheme_name\" else col for col in df_self_aligned.columns]\n",
    "            df_live_aligned.columns = [f\"{col}_live\" if col != \"main_scheme_name\" else col for col in df_live_aligned.columns]\n",
    "\n",
    "            # Drop 'main_scheme_name' only if it exists in df_live_aligned\n",
    "            if 'main_scheme_name' in df_live_aligned.columns:\n",
    "                df_combined = pd.concat([df_self_aligned, df_live_aligned.drop(columns=[\"main_scheme_name\"])], axis=1)\n",
    "            else:\n",
    "                df_combined = pd.concat([df_self_aligned, df_live_aligned], axis=1)\n",
    "        else:\n",
    "            df_live['data_from'] = 'livedb'\n",
    "            df_combined = df_live\n",
    "\n",
    "        df_combined.to_excel(writer, sheet_name=sheet_name[:31], index=False)\n",
    "\n",
    "print(f\"Saved at: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e950acbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552d066b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6789664b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rando\\AppData\\Local\\Temp\\ipykernel_11268\\1172598389.py:10: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_df = pd.concat(combined_parts, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "live_data = pd.read_excel(\"aligned_comparison_20250511_1305.xlsx\", sheet_name=None)\n",
    "combined_parts = []\n",
    "\n",
    "for sheet_name in sorted(live_data):\n",
    "    df = live_data[sheet_name]\n",
    "    combined_parts.append(df)\n",
    "    blank_rows = pd.DataFrame(columns=df.columns, index=range(5))\n",
    "    combined_parts.append(blank_rows)\n",
    "\n",
    "final_df = pd.concat(combined_parts, ignore_index=True)\n",
    "final_df.to_excel(\"jan_all_dataset.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c4a410",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1e40ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "live_data = pd.read_excel(os.path.join(os.getcwd(),\"march_live_20250509_0939.xlsx\"), sheet_name=None)\n",
    "self_data = pd.read_excel(os.path.join(os.getcwd(),\"march_self_20250509_0938.xlsx\"), sheet_name=None)\n",
    "\n",
    "\n",
    "fund_names = list(live_data.keys())\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "output_path = f\"march_self_live_{timestamp}.xlsx\"\n",
    "\n",
    "with pd.ExcelWriter(output_path, engine=\"xlsxwriter\") as writer:\n",
    "    workbook = writer.book\n",
    "    header_format = workbook.add_format({\n",
    "        'bold': True,\n",
    "        'text_wrap': True,\n",
    "        'valign': 'top',\n",
    "        'fg_color': '#D7E4BC',\n",
    "        'border': 1\n",
    "    })\n",
    "\n",
    "    for fund in fund_names:\n",
    "        df_live = live_data[fund].copy()\n",
    "\n",
    "        df_live[\"data_from\"] = \"livedb\"\n",
    "\n",
    "        df_offline = self_data.get(fund, pd.DataFrame()).copy()\n",
    "        if df_offline.empty:\n",
    "            df_combined = df_live\n",
    "        else:\n",
    "            # df_offline[\"data_from\"] = \"mydata\"\n",
    "            df_offline_renamed = df_offline.rename(columns=column_mapping)\n",
    "\n",
    "            df_offline_aligned = df_offline_renamed.reindex(columns=df_live.columns)\n",
    "\n",
    "            # Combine live + offline\n",
    "            df_combined = pd.concat([df_live, df_offline_aligned], ignore_index=True)\n",
    "\n",
    "        df_combined.to_excel(writer, sheet_name=fund, index=False)\n",
    "        worksheet = writer.sheets[fund]\n",
    "\n",
    "        for col_num, header in enumerate(df_combined.columns):\n",
    "            worksheet.write(0, col_num, header, header_format)\n",
    "            worksheet.set_column(col_num, col_num, min(len(header) + 8, 50))\n",
    "\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23974cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SELECT \n",
    "    d.*,\n",
    "    COALESCE(d.benchmark_index, bi.benchmark_index) AS final_benchmark_index,\n",
    "    bi.riskometer_benchmark,\n",
    "    MAX(CASE WHEN l.type = 'Entry' THEN l.comment END) AS entry_load,\n",
    "    MAX(CASE WHEN l.type = 'Exit' THEN l.comment END) AS exit_load\n",
    "FROM mf_json_common_details d\n",
    "LEFT JOIN mf_json_loads l\n",
    "  ON d.MainScheme_ID = l.MainScheme_ID\n",
    " AND d.document_detail_id = l.document_detail_id\n",
    "LEFT JOIN mf_json_benchmark_indices bi\n",
    "  ON d.MainScheme_ID = bi.MainScheme_ID\n",
    " AND d.document_detail_id = bi.document_detail_id\n",
    "WHERE d.fund_name LIKE %s\n",
    "  AND (d.monthly_aaum_date REGEXP '202502|FEBRUARY' OR d.monthly_aaum_date IS NULL)\n",
    "GROUP BY d.id;\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envPDF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
